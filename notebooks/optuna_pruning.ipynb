{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9824499285817105"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean([0.9811991983356031, 0.9775999181916318, 0.9819993984396399, 0.9807997584156638, 0.9851998387837438, 0.9818009987438286, 0.9837987984635966, 0.9854000388397711, 0.9836002787437711, 0.9806006385757661, 0.9862001189197934, 0.9812001585277184])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4435933841643036"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([0.46279279558254016, 0.4274040750073275, 0.37198526681218375, 0.4565850335335094, 0.40799655195011414, 0.45617911231679237, 0.4838104407929939, 0.4703862348778743, 0.43080723591176123, 0.5305794783420267, 0.43219963450287135, 0.39239475034164834])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44199455427041845"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median([0.46279279558254016, 0.4274040750073275, 0.37198526681218375, 0.4565850335335094])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-27 09:58:41,597] A new study created in memory with name: no-name-d0421ffb-4dab-4854-9df1-515f576732b3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-27 09:58:41,788] Trial 0 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 93}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:41,887] Trial 1 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 31}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:42,013] Trial 2 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 33}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:42,061] Trial 3 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 34}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:42,155] Trial 4 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 49}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:42,281] Trial 5 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 56}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\2958380071.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = SGDClassifier(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n",
      "[I 2024-01-27 09:58:42,295] Trial 6 pruned. \n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:58:42,521] Trial 7 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 37}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:42,691] Trial 8 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 93}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:42,793] Trial 9 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 94}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:42,816] Trial 10 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 6}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:42,826] Trial 11 finished with value: 1.0 and parameters: {'classifier': 'DecisionTreeClassifier'}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\2958380071.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "[I 2024-01-27 09:58:42,847] Trial 12 pruned. \n",
      "[I 2024-01-27 09:58:42,883] Trial 13 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 10}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:42,985] Trial 14 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 74}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:43,156] Trial 15 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 43}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:43,219] Trial 16 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 12}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:43,387] Trial 17 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 70}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:43,601] Trial 18 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 78}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:43,768] Trial 19 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 54}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:43,783] Trial 20 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 1}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:43,947] Trial 21 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 28}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:44,028] Trial 22 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 23}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:44,081] Trial 23 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 44}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\2958380071.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = SGDClassifier(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n",
      "[I 2024-01-27 09:58:44,102] Trial 24 pruned. \n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:58:44,288] Trial 25 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 63}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:44,433] Trial 26 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 41}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:44,634] Trial 27 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 96}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\2958380071.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "[I 2024-01-27 09:58:44,663] Trial 28 finished with value: 1.0 and parameters: {'classifier': 'LogisticRegression', 'C': 57159.162081391274}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:44,669] Trial 29 finished with value: 1.0 and parameters: {'classifier': 'DecisionTreeClassifier'}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:44,774] Trial 30 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 22}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:44,888] Trial 31 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 33}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:44,988] Trial 32 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 31}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:45,047] Trial 33 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 19}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:45,300] Trial 34 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 86}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:45,379] Trial 35 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 41}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:45,558] Trial 36 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 100}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\2958380071.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = SGDClassifier(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n",
      "[I 2024-01-27 09:58:45,571] Trial 37 pruned. \n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:58:45,686] Trial 38 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 28}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:45,758] Trial 39 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 48}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:45,783] Trial 40 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 10}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:45,899] Trial 41 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 51}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:45,986] Trial 42 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 61}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:46,051] Trial 43 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 36}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:46,163] Trial 44 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 48}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:46,172] Trial 45 finished with value: 1.0 and parameters: {'classifier': 'DecisionTreeClassifier'}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\2958380071.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "[I 2024-01-27 09:58:46,198] Trial 46 pruned. \n",
      "[I 2024-01-27 09:58:46,291] Trial 47 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 38}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:46,338] Trial 48 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 12}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:46,389] Trial 49 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 63}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:46,451] Trial 50 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 16}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:46,497] Trial 51 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 26}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:46,571] Trial 52 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 36}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:46,719] Trial 53 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 86}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:46,876] Trial 54 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 58}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:47,050] Trial 55 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 71}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:47,071] Trial 56 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 1}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\2958380071.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = SGDClassifier(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n",
      "[I 2024-01-27 09:58:47,091] Trial 57 pruned. \n",
      "[I 2024-01-27 09:58:47,178] Trial 58 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 45}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:58:47,286] Trial 59 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 32}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:47,361] Trial 60 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 82}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:58:47,473] Trial 61 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 39}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:58:47,605] Trial 62 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 33}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:47,753] Trial 63 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 55}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:47,813] Trial 64 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 43}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\2958380071.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "[I 2024-01-27 09:58:47,839] Trial 65 finished with value: 1.0 and parameters: {'classifier': 'LogisticRegression', 'C': 50960.711631422615}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:47,846] Trial 66 finished with value: 1.0 and parameters: {'classifier': 'DecisionTreeClassifier'}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:47,932] Trial 67 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 25}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:48,006] Trial 68 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 28}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:48,176] Trial 69 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 100}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:58:48,224] Trial 70 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 14}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:48,466] Trial 71 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 92}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:48,784] Trial 72 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 95}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:48,894] Trial 73 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 76}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:48,979] Trial 74 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 68}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:49,146] Trial 75 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 93}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:49,322] Trial 76 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 100}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:49,341] Trial 77 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 10}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\2958380071.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = SGDClassifier(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n",
      "[I 2024-01-27 09:58:49,354] Trial 78 pruned. \n",
      "[I 2024-01-27 09:58:49,503] Trial 79 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 88}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:49,563] Trial 80 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 21}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:49,706] Trial 81 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 96}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:49,856] Trial 82 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 80}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:49,983] Trial 83 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 88}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:50,057] Trial 84 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 30}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:50,174] Trial 85 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 34}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:50,182] Trial 86 finished with value: 1.0 and parameters: {'classifier': 'DecisionTreeClassifier'}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:50,234] Trial 87 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 46}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:58:50,365] Trial 88 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 51}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\2958380071.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "[I 2024-01-27 09:58:50,386] Trial 89 finished with value: 1.0 and parameters: {'classifier': 'LogisticRegression', 'C': 0.8881616404828881}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:50,495] Trial 90 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 60}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:50,511] Trial 91 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 6}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:50,530] Trial 92 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 6}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:50,552] Trial 93 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 4}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:50,610] Trial 94 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 38}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:50,763] Trial 95 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 41}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:51,004] Trial 96 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 96}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:51,162] Trial 97 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 91}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:51,279] Trial 98 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 30}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:58:51,423] Trial 99 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 83}. Best is trial 0 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "Classifier: RandomForest\n",
      "Parameters: {'classifier': 'RandomForest', 'n_estimators': 93}\n",
      "Accuracy: 1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_C</th>\n",
       "      <th>params_alpha</th>\n",
       "      <th>params_classifier</th>\n",
       "      <th>params_max_iter</th>\n",
       "      <th>params_n_estimators</th>\n",
       "      <th>params_n_neighbors</th>\n",
       "      <th>system_attrs_completed_rung_0</th>\n",
       "      <th>system_attrs_completed_rung_1</th>\n",
       "      <th>system_attrs_completed_rung_2</th>\n",
       "      <th>system_attrs_completed_rung_3</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024-01-27 09:58:41.599115</td>\n",
       "      <td>2024-01-27 09:58:41.787290</td>\n",
       "      <td>0 days 00:00:00.188175</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024-01-27 09:58:41.790297</td>\n",
       "      <td>2024-01-27 09:58:41.887513</td>\n",
       "      <td>0 days 00:00:00.097216</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024-01-27 09:58:41.889842</td>\n",
       "      <td>2024-01-27 09:58:42.013166</td>\n",
       "      <td>0 days 00:00:00.123324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024-01-27 09:58:42.014188</td>\n",
       "      <td>2024-01-27 09:58:42.061273</td>\n",
       "      <td>0 days 00:00:00.047085</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024-01-27 09:58:42.063248</td>\n",
       "      <td>2024-01-27 09:58:42.155646</td>\n",
       "      <td>0 days 00:00:00.092398</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024-01-27 09:58:50.612026</td>\n",
       "      <td>2024-01-27 09:58:50.763600</td>\n",
       "      <td>0 days 00:00:00.151574</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GradientBoost</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024-01-27 09:58:50.765597</td>\n",
       "      <td>2024-01-27 09:58:51.004163</td>\n",
       "      <td>0 days 00:00:00.238566</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024-01-27 09:58:51.005211</td>\n",
       "      <td>2024-01-27 09:58:51.162212</td>\n",
       "      <td>0 days 00:00:00.157001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024-01-27 09:58:51.163211</td>\n",
       "      <td>2024-01-27 09:58:51.279006</td>\n",
       "      <td>0 days 00:00:00.115795</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024-01-27 09:58:51.280950</td>\n",
       "      <td>2024-01-27 09:58:51.423543</td>\n",
       "      <td>0 days 00:00:00.142593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    number  value             datetime_start          datetime_complete  \\\n",
       "0        0    1.0 2024-01-27 09:58:41.599115 2024-01-27 09:58:41.787290   \n",
       "1        1    1.0 2024-01-27 09:58:41.790297 2024-01-27 09:58:41.887513   \n",
       "2        2    1.0 2024-01-27 09:58:41.889842 2024-01-27 09:58:42.013166   \n",
       "3        3    1.0 2024-01-27 09:58:42.014188 2024-01-27 09:58:42.061273   \n",
       "4        4    1.0 2024-01-27 09:58:42.063248 2024-01-27 09:58:42.155646   \n",
       "..     ...    ...                        ...                        ...   \n",
       "95      95    1.0 2024-01-27 09:58:50.612026 2024-01-27 09:58:50.763600   \n",
       "96      96    1.0 2024-01-27 09:58:50.765597 2024-01-27 09:58:51.004163   \n",
       "97      97    1.0 2024-01-27 09:58:51.005211 2024-01-27 09:58:51.162212   \n",
       "98      98    1.0 2024-01-27 09:58:51.163211 2024-01-27 09:58:51.279006   \n",
       "99      99    1.0 2024-01-27 09:58:51.280950 2024-01-27 09:58:51.423543   \n",
       "\n",
       "                 duration  params_C  params_alpha     params_classifier  \\\n",
       "0  0 days 00:00:00.188175       NaN           NaN          RandomForest   \n",
       "1  0 days 00:00:00.097216       NaN           NaN          RandomForest   \n",
       "2  0 days 00:00:00.123324       NaN           NaN              AdaBoost   \n",
       "3  0 days 00:00:00.047085       NaN           NaN  ExtraTreesClassifier   \n",
       "4  0 days 00:00:00.092398       NaN           NaN          RandomForest   \n",
       "..                    ...       ...           ...                   ...   \n",
       "95 0 days 00:00:00.151574       NaN           NaN         GradientBoost   \n",
       "96 0 days 00:00:00.238566       NaN           NaN          RandomForest   \n",
       "97 0 days 00:00:00.157001       NaN           NaN          RandomForest   \n",
       "98 0 days 00:00:00.115795       NaN           NaN              AdaBoost   \n",
       "99 0 days 00:00:00.142593       NaN           NaN  ExtraTreesClassifier   \n",
       "\n",
       "    params_max_iter  params_n_estimators  params_n_neighbors  \\\n",
       "0               NaN                 93.0                 NaN   \n",
       "1               NaN                 31.0                 NaN   \n",
       "2               NaN                 33.0                 NaN   \n",
       "3               NaN                 34.0                 NaN   \n",
       "4               NaN                 49.0                 NaN   \n",
       "..              ...                  ...                 ...   \n",
       "95              NaN                 41.0                 NaN   \n",
       "96              NaN                 96.0                 NaN   \n",
       "97              NaN                 91.0                 NaN   \n",
       "98              NaN                 30.0                 NaN   \n",
       "99              NaN                 83.0                 NaN   \n",
       "\n",
       "    system_attrs_completed_rung_0  system_attrs_completed_rung_1  \\\n",
       "0                             NaN                            NaN   \n",
       "1                             1.0                            NaN   \n",
       "2                             1.0                            NaN   \n",
       "3                             1.0                            NaN   \n",
       "4                             1.0                            1.0   \n",
       "..                            ...                            ...   \n",
       "95                            1.0                            1.0   \n",
       "96                            1.0                            1.0   \n",
       "97                            1.0                            1.0   \n",
       "98                            1.0                            1.0   \n",
       "99                            1.0                            1.0   \n",
       "\n",
       "    system_attrs_completed_rung_2  system_attrs_completed_rung_3     state  \n",
       "0                             NaN                            NaN  COMPLETE  \n",
       "1                             NaN                            NaN  COMPLETE  \n",
       "2                             NaN                            NaN  COMPLETE  \n",
       "3                             NaN                            NaN  COMPLETE  \n",
       "4                             NaN                            NaN  COMPLETE  \n",
       "..                            ...                            ...       ...  \n",
       "95                            1.0                            1.0  COMPLETE  \n",
       "96                            1.0                            1.0  COMPLETE  \n",
       "97                            1.0                            1.0  COMPLETE  \n",
       "98                            1.0                            1.0  COMPLETE  \n",
       "99                            1.0                            1.0  COMPLETE  \n",
       "\n",
       "[93 rows x 16 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier, ExtraTreesClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load a sample dataset (in this case, the Iris dataset)\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def objective(trial):\n",
    "    # Initialize the classifier variable\n",
    "    classifier = None\n",
    "\n",
    "    # Choose a classifier from the available options\n",
    "    classifier_name = trial.suggest_categorical('classifier', ['LogisticRegression', 'SGDClassifier', 'RandomForest', \n",
    "                                                               'AdaBoost', 'GradientBoost', 'BaggingClassifier', \n",
    "                                                               'ExtraTreesClassifier', 'HistGradientBoostingClassifier', \n",
    "                                                               'DecisionTreeClassifier', 'XGBClassifier', 'KNeighborsClassifier'])\n",
    "\n",
    "    # Instantiate the selected classifier with hyperparameters suggested by Optuna\n",
    "    if classifier_name == 'LogisticRegression':\n",
    "        classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
    "    elif classifier_name == 'SGDClassifier':\n",
    "        classifier = SGDClassifier(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n",
    "    elif classifier_name == 'RandomForest':\n",
    "        classifier = RandomForestClassifier(n_estimators=trial.suggest_int('n_estimators', 10, 100))\n",
    "    elif classifier_name == 'AdaBoost':\n",
    "        classifier = AdaBoostClassifier(n_estimators=trial.suggest_int('n_estimators', 10, 100))\n",
    "    elif classifier_name == 'GradientBoost':\n",
    "        classifier = GradientBoostingClassifier(n_estimators=trial.suggest_int('n_estimators', 10, 100))\n",
    "    elif classifier_name == 'BaggingClassifier':\n",
    "        classifier = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
    "                                       n_estimators=trial.suggest_int('n_estimators', 10, 100))\n",
    "    elif classifier_name == 'ExtraTreesClassifier':\n",
    "        classifier = ExtraTreesClassifier(n_estimators=trial.suggest_int('n_estimators', 10, 100))\n",
    "    elif classifier_name == 'HistGradientBoostingClassifier':\n",
    "        classifier = HistGradientBoostingClassifier(max_iter=trial.suggest_int('max_iter', 10, 100))\n",
    "    elif classifier_name == 'DecisionTreeClassifier':\n",
    "        classifier = DecisionTreeClassifier()\n",
    "    elif classifier_name == 'XGBClassifier':\n",
    "        classifier = XGBClassifier(n_estimators=trial.suggest_int('n_estimators', 10, 100))\n",
    "    elif classifier_name == 'KNeighborsClassifier':\n",
    "        classifier = KNeighborsClassifier(n_neighbors=trial.suggest_int('n_neighbors', 1, 10))\n",
    "\n",
    "    if classifier is None:\n",
    "        raise ValueError(f\"Invalid classifier name: {classifier_name}\")\n",
    "\n",
    "    # Training the classifier\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Pruning callback to stop the trial if it's unpromising\n",
    "    trial.report(accuracy_score(y_val, classifier.predict(X_val)), step=trial.number)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "\n",
    "    return accuracy_score(y_val, classifier.predict(X_val))\n",
    "\n",
    "# Set up the optimization study\n",
    "# Set up the optimization study with SuccessiveHalvingPruner\n",
    "study = optuna.create_study(direction='maximize', pruner=optuna.pruners.SuccessiveHalvingPruner())\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "\n",
    "# Print the best parameters and their corresponding accuracy\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"Classifier:\", trial.params['classifier'])\n",
    "print(\"Parameters:\", trial.params)\n",
    "print(\"Accuracy:\", trial.value)\n",
    "\n",
    "study.trials_dataframe()[study.trials_dataframe()['state']!='PRUNED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-27 09:59:05,912] A new study created in memory with name: no-name-de6337b4-b08d-4211-b8cf-6e9e7c507129\n",
      "[I 2024-01-27 09:59:05,914] A new study created in memory with name: no-name-503b9f9d-d982-40ee-826f-fde865a510e9\n",
      "[I 2024-01-27 09:59:05,918] A new study created in memory with name: no-name-5a9abe3a-b079-4370-80c8-e26acf4246a9\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = SGDClassifier(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n",
      "[I 2024-01-27 09:59:05,923] Trial 0 finished with value: 0.9333333333333333 and parameters: {'classifier': 'SGDClassifier', 'alpha': 0.0001046372968062504}. Best is trial 0 with value: 0.9333333333333333.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = SGDClassifier(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n",
      "[I 2024-01-27 09:59:05,929] Trial 1 finished with value: 1.0 and parameters: {'classifier': 'SGDClassifier', 'alpha': 0.006237915040117193}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:05,953] Trial 2 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 10}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:06,000] Trial 3 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 100}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:06,085] Trial 4 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 56}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:06,348] Trial 5 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 89}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:06,416] Trial 6 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 25}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:06,478] Trial 7 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 31}. Best is trial 1 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:06,748] Trial 8 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 96}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:06,781] Trial 9 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 12}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = SGDClassifier(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n",
      "[I 2024-01-27 09:59:06,797] Trial 10 finished with value: 0.8666666666666667 and parameters: {'classifier': 'SGDClassifier', 'alpha': 0.04990386185916866}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2024-01-27 09:59:06,839] Trial 11 finished with value: 1.0 and parameters: {'classifier': 'LogisticRegression', 'C': 106.05947389453026}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:06,925] Trial 12 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 45}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:07,058] Trial 13 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 61}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:07,086] Trial 14 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 9}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:07,221] Trial 15 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 41}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:07,226] Trial 16 finished with value: 1.0 and parameters: {'classifier': 'DecisionTreeClassifier'}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = SGDClassifier(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n",
      "[I 2024-01-27 09:59:07,247] Trial 17 finished with value: 0.9666666666666667 and parameters: {'classifier': 'SGDClassifier', 'alpha': 0.011269727116247335}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:07,291] Trial 18 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 69}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:07,335] Trial 19 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 14}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:07,352] Trial 20 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 2}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:07,416] Trial 21 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 79}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:07,448] Trial 22 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 37}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:07,508] Trial 23 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 78}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:07,598] Trial 24 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 53}. Best is trial 1 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:07,660] Trial 25 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 27}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:07,666] Trial 26 finished with value: 1.0 and parameters: {'classifier': 'DecisionTreeClassifier'}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "[I 2024-01-27 09:59:07,681] Trial 27 finished with value: 0.3 and parameters: {'classifier': 'LogisticRegression', 'C': 1.749115645221173e-05}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = SGDClassifier(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n",
      "[I 2024-01-27 09:59:07,698] Trial 28 finished with value: 0.8666666666666667 and parameters: {'classifier': 'SGDClassifier', 'alpha': 0.0009027281531368497}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:07,864] Trial 29 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 60}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = SGDClassifier(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n",
      "[I 2024-01-27 09:59:07,874] Trial 30 finished with value: 0.9333333333333333 and parameters: {'classifier': 'SGDClassifier', 'alpha': 0.002433428582985915}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:08,023] Trial 31 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 85}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:08,128] Trial 32 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 62}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:08,380] Trial 33 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 95}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:08,429] Trial 34 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 14}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:08,476] Trial 35 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 27}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:08,569] Trial 36 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 54}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:08,802] Trial 37 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 82}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:08,878] Trial 38 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 50}. Best is trial 1 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:09,091] Trial 39 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 66}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = SGDClassifier(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n",
      "[I 2024-01-27 09:59:09,112] Trial 40 finished with value: 0.9333333333333333 and parameters: {'classifier': 'SGDClassifier', 'alpha': 1.1115226033004523e-05}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:09,422] Trial 41 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 100}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:09,735] Trial 42 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 89}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:09,990] Trial 43 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 86}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2024-01-27 09:59:10,019] Trial 44 finished with value: 1.0 and parameters: {'classifier': 'LogisticRegression', 'C': 68830.17859381097}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:10,117] Trial 45 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 70}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:10,264] Trial 46 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 92}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:10,329] Trial 47 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 75}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:10,345] Trial 48 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 10}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:10,351] Trial 49 finished with value: 1.0 and parameters: {'classifier': 'DecisionTreeClassifier'}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = SGDClassifier(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n",
      "[I 2024-01-27 09:59:10,374] Trial 50 finished with value: 0.7 and parameters: {'classifier': 'SGDClassifier', 'alpha': 0.06214591471590879}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:10,468] Trial 51 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 22}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:10,570] Trial 52 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 19}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:10,649] Trial 53 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 10}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:10,726] Trial 54 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 36}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:10,890] Trial 55 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 97}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:10,957] Trial 56 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 100}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:11,006] Trial 57 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 16}. Best is trial 1 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:11,084] Trial 58 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 26}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:11,104] Trial 59 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 1}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = SGDClassifier(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n",
      "[I 2024-01-27 09:59:11,126] Trial 60 finished with value: 1.0 and parameters: {'classifier': 'SGDClassifier', 'alpha': 0.0008122330221367166}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:11,231] Trial 61 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 32}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:11,305] Trial 62 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 44}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:11,355] Trial 63 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 32}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:11,412] Trial 64 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 26}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "[I 2024-01-27 09:59:11,425] Trial 65 finished with value: 0.3 and parameters: {'classifier': 'LogisticRegression', 'C': 3.8708131838838243e-05}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:11,684] Trial 66 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 86}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:11,744] Trial 67 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 44}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:11,749] Trial 68 finished with value: 1.0 and parameters: {'classifier': 'DecisionTreeClassifier'}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:11,865] Trial 69 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 72}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:12,026] Trial 70 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 94}. Best is trial 1 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:12,235] Trial 71 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 99}. Best is trial 1 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:12,414] Trial 72 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 91}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = SGDClassifier(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n",
      "[I 2024-01-27 09:59:12,429] Trial 73 finished with value: 0.8666666666666667 and parameters: {'classifier': 'SGDClassifier', 'alpha': 0.005345846120885628}. Best is trial 1 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:12,669] Trial 74 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 84}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:12,723] Trial 75 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 15}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:12,813] Trial 76 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 41}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:12,998] Trial 77 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 60}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:13,112] Trial 78 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 96}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = SGDClassifier(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n",
      "[I 2024-01-27 09:59:13,127] Trial 79 finished with value: 0.9333333333333333 and parameters: {'classifier': 'SGDClassifier', 'alpha': 0.00012596985480901654}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:13,327] Trial 80 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 73}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:13,371] Trial 81 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 11}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:13,442] Trial 82 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 24}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:13,510] Trial 83 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 19}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:13,544] Trial 84 finished with value: 0.9666666666666667 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 12}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "[I 2024-01-27 09:59:13,581] Trial 85 finished with value: 1.0 and parameters: {'classifier': 'LogisticRegression', 'C': 0.1582188772025031}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:13,692] Trial 86 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 50}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:13,709] Trial 87 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 6}. Best is trial 1 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:13,814] Trial 88 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 31}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:13,853] Trial 89 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 19}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:14,102] Trial 90 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 79}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2024-01-27 09:59:14,135] Trial 91 finished with value: 1.0 and parameters: {'classifier': 'LogisticRegression', 'C': 683.3397022203034}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2024-01-27 09:59:14,175] Trial 92 finished with value: 1.0 and parameters: {'classifier': 'LogisticRegression', 'C': 3.4232682019049396}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:14,231] Trial 93 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 10}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:14,245] Trial 94 finished with value: 1.0 and parameters: {'classifier': 'DecisionTreeClassifier'}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = SGDClassifier(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n",
      "[I 2024-01-27 09:59:14,263] Trial 95 finished with value: 0.9333333333333333 and parameters: {'classifier': 'SGDClassifier', 'alpha': 0.017557042851200282}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:14,517] Trial 96 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 88}. Best is trial 1 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2024-01-27 09:59:14,563] Trial 97 finished with value: 1.0 and parameters: {'classifier': 'LogisticRegression', 'C': 2.3391636933366193}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:14,631] Trial 98 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 97}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:14,734] Trial 99 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 59}. Best is trial 1 with value: 1.0.\n",
      "[I 2024-01-27 09:59:14,744] Trial 0 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 5}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:14,919] Trial 1 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 70}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:14,932] Trial 2 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 9}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2024-01-27 09:59:14,961] Trial 3 finished with value: 1.0 and parameters: {'classifier': 'LogisticRegression', 'C': 3.2959947071535702}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:15,060] Trial 4 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 58}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = SGDClassifier(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n",
      "[I 2024-01-27 09:59:15,068] Trial 5 finished with value: 0.8 and parameters: {'classifier': 'SGDClassifier', 'alpha': 0.01147101789564508}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:15,325] Trial 6 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 78}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:15,485] Trial 7 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 82}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:15,524] Trial 8 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 16}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:15,606] Trial 9 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 35}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:15,618] Trial 10 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 2}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:15,711] Trial 11 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 36}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:15,779] Trial 12 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 93}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:15,910] Trial 13 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 58}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:15,914] Trial 14 finished with value: 1.0 and parameters: {'classifier': 'DecisionTreeClassifier'}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:16,062] Trial 15 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 41}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:16,076] Trial 16 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 5}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:16,130] Trial 17 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 71}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "[I 2024-01-27 09:59:16,144] Trial 18 finished with value: 0.3 and parameters: {'classifier': 'LogisticRegression', 'C': 1.497166899838202e-05}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:16,286] Trial 19 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 99}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = SGDClassifier(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n",
      "[I 2024-01-27 09:59:16,299] Trial 20 finished with value: 0.6333333333333333 and parameters: {'classifier': 'SGDClassifier', 'alpha': 1.1645872766431516e-05}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:16,316] Trial 21 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 9}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:16,332] Trial 22 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 10}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:16,348] Trial 23 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 6}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:16,539] Trial 24 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 98}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:16,630] Trial 25 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 34}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:16,666] Trial 26 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 11}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:16,673] Trial 27 finished with value: 1.0 and parameters: {'classifier': 'DecisionTreeClassifier'}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:16,756] Trial 28 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 68}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "[I 2024-01-27 09:59:16,777] Trial 29 finished with value: 1.0 and parameters: {'classifier': 'LogisticRegression', 'C': 86180.1841254619}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:16,930] Trial 30 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 50}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2024-01-27 09:59:16,960] Trial 31 finished with value: 1.0 and parameters: {'classifier': 'LogisticRegression', 'C': 3.3663576030835327}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:17,008] Trial 32 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 23}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2024-01-27 09:59:17,041] Trial 33 finished with value: 1.0 and parameters: {'classifier': 'LogisticRegression', 'C': 4.319060612346044}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:17,056] Trial 34 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 6}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = SGDClassifier(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n",
      "[I 2024-01-27 09:59:17,070] Trial 35 finished with value: 0.7 and parameters: {'classifier': 'SGDClassifier', 'alpha': 0.08278940051531992}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:17,317] Trial 36 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 87}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:17,496] Trial 37 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 68}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:17,521] Trial 38 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 3}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:17,598] Trial 39 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 49}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:17,643] Trial 40 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 14}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:17,789] Trial 41 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 62}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:17,998] Trial 42 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 76}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:18,138] Trial 43 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 52}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:18,199] Trial 44 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 62}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:18,307] Trial 45 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 44}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:18,324] Trial 46 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 8}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "[I 2024-01-27 09:59:18,341] Trial 47 finished with value: 0.9 and parameters: {'classifier': 'LogisticRegression', 'C': 0.002318237961170534}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:18,465] Trial 48 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 29}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:18,478] Trial 49 finished with value: 1.0 and parameters: {'classifier': 'DecisionTreeClassifier'}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:18,729] Trial 50 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 87}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:18,939] Trial 51 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 79}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:19,118] Trial 52 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 73}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:19,132] Trial 53 finished with value: 0.9666666666666667 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 7}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:19,328] Trial 54 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 62}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = SGDClassifier(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n",
      "[I 2024-01-27 09:59:19,340] Trial 55 finished with value: 1.0 and parameters: {'classifier': 'SGDClassifier', 'alpha': 8.180807031078093e-05}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:19,474] Trial 56 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 82}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:19,528] Trial 57 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 87}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:19,546] Trial 58 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 4}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2024-01-27 09:59:19,572] Trial 59 finished with value: 1.0 and parameters: {'classifier': 'LogisticRegression', 'C': 2410.6653079200546}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:19,682] Trial 60 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 80}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:19,928] Trial 61 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 81}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:20,135] Trial 62 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 68}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:20,361] Trial 63 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 100}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:20,382] Trial 64 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 8}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:20,589] Trial 65 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 92}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:20,595] Trial 66 finished with value: 1.0 and parameters: {'classifier': 'DecisionTreeClassifier'}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:20,715] Trial 67 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 57}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:21,057] Trial 68 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 84}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2024-01-27 09:59:21,105] Trial 69 finished with value: 1.0 and parameters: {'classifier': 'LogisticRegression', 'C': 119.02892498698009}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:21,335] Trial 70 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 94}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:21,437] Trial 71 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 76}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:21,459] Trial 72 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 12}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:21,518] Trial 73 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 41}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:21,531] Trial 74 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 1}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = SGDClassifier(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n",
      "[I 2024-01-27 09:59:21,547] Trial 75 finished with value: 1.0 and parameters: {'classifier': 'SGDClassifier', 'alpha': 0.0009875227102582305}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:21,611] Trial 76 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 19}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:21,724] Trial 77 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 62}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:21,926] Trial 78 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 73}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "[I 2024-01-27 09:59:21,954] Trial 79 finished with value: 0.9333333333333333 and parameters: {'classifier': 'LogisticRegression', 'C': 0.02616215921796423}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:21,986] Trial 80 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 4}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:22,067] Trial 81 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 26}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:22,157] Trial 82 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 44}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:22,208] Trial 83 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 63}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:22,373] Trial 84 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 54}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:22,503] Trial 85 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 67}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:22,583] Trial 86 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 43}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:22,668] Trial 87 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 34}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:22,686] Trial 88 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 10}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:22,868] Trial 89 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 47}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:22,964] Trial 90 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 71}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:22,979] Trial 91 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 1}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:22,998] Trial 92 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 2}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:23,008] Trial 93 finished with value: 1.0 and parameters: {'classifier': 'DecisionTreeClassifier'}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:23,027] Trial 94 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 3}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "[I 2024-01-27 09:59:23,047] Trial 95 finished with value: 0.9333333333333333 and parameters: {'classifier': 'LogisticRegression', 'C': 0.014782808142161754}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:23,280] Trial 96 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 76}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:23,445] Trial 97 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 65}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:23,460] Trial 98 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 5}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = SGDClassifier(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n",
      "[I 2024-01-27 09:59:23,477] Trial 99 finished with value: 0.6666666666666666 and parameters: {'classifier': 'SGDClassifier', 'alpha': 0.000686623822684084}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:23,534] Trial 0 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 21}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:23,642] Trial 1 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 43}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:23,734] Trial 2 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 75}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:24,003] Trial 3 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 96}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:24,086] Trial 4 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 53}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:24,142] Trial 5 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 26}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:24,396] Trial 6 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 85}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:24,460] Trial 7 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 32}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "[I 2024-01-27 09:59:24,472] Trial 8 finished with value: 0.6333333333333333 and parameters: {'classifier': 'LogisticRegression', 'C': 0.0001529994272131308}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2024-01-27 09:59:24,501] Trial 9 finished with value: 1.0 and parameters: {'classifier': 'LogisticRegression', 'C': 4.948141672191648}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:24,566] Trial 10 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 14}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:24,717] Trial 11 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 47}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = SGDClassifier(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n",
      "[I 2024-01-27 09:59:24,725] Trial 12 finished with value: 0.7 and parameters: {'classifier': 'SGDClassifier', 'alpha': 1.1514801695621428e-05}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:24,876] Trial 13 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 86}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:24,882] Trial 14 finished with value: 1.0 and parameters: {'classifier': 'DecisionTreeClassifier'}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:24,986] Trial 15 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 39}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:25,005] Trial 16 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 10}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:25,041] Trial 17 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 11}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:25,177] Trial 18 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 59}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:25,227] Trial 19 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 24}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:25,356] Trial 20 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 63}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:25,475] Trial 21 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 73}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:25,559] Trial 22 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 72}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:25,565] Trial 23 finished with value: 1.0 and parameters: {'classifier': 'DecisionTreeClassifier'}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:25,604] Trial 24 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 44}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:25,655] Trial 25 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 19}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:25,838] Trial 26 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 72}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:25,854] Trial 27 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 1}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = SGDClassifier(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n",
      "[I 2024-01-27 09:59:25,870] Trial 28 finished with value: 0.8333333333333334 and parameters: {'classifier': 'SGDClassifier', 'alpha': 0.060145898661123846}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:25,923] Trial 29 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 34}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:26,086] Trial 30 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 84}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:26,340] Trial 31 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 99}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:26,492] Trial 32 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 99}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:26,678] Trial 33 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 86}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:26,836] Trial 34 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 92}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:26,906] Trial 35 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 20}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:27,088] Trial 36 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 76}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2024-01-27 09:59:27,121] Trial 37 finished with value: 1.0 and parameters: {'classifier': 'LogisticRegression', 'C': 10332.13046522942}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:27,190] Trial 38 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 64}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:27,297] Trial 39 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 55}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:27,588] Trial 40 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 79}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:27,773] Trial 41 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 52}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:27,896] Trial 42 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 47}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = SGDClassifier(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n",
      "[I 2024-01-27 09:59:27,908] Trial 43 finished with value: 0.8333333333333334 and parameters: {'classifier': 'SGDClassifier', 'alpha': 0.0007329800950479223}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "[I 2024-01-27 09:59:27,934] Trial 44 finished with value: 0.3 and parameters: {'classifier': 'LogisticRegression', 'C': 1.8340329698868952e-05}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:28,119] Trial 45 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 100}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:28,123] Trial 46 finished with value: 1.0 and parameters: {'classifier': 'DecisionTreeClassifier'}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:28,346] Trial 47 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 66}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:28,478] Trial 48 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 31}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:28,510] Trial 49 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 6}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:28,634] Trial 50 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 40}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:28,723] Trial 51 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 20}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:28,844] Trial 52 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 27}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:28,902] Trial 53 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 19}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:28,963] Trial 54 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 14}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:29,064] Trial 55 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 51}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:29,184] Trial 56 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 34}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:29,221] Trial 57 finished with value: 0.9666666666666667 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 27}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:29,258] Trial 58 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 10}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:29,360] Trial 59 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 59}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:29,367] Trial 60 finished with value: 1.0 and parameters: {'classifier': 'DecisionTreeClassifier'}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:29,561] Trial 61 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 93}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:29,766] Trial 62 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 91}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:29,961] Trial 63 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 81}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:30,003] Trial 64 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 10}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = SGDClassifier(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n",
      "[I 2024-01-27 09:59:30,020] Trial 65 finished with value: 1.0 and parameters: {'classifier': 'SGDClassifier', 'alpha': 0.09639631670402539}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:30,085] Trial 66 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 38}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:30,384] Trial 67 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 95}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:30,487] Trial 68 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 86}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:30,502] Trial 69 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 1}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:30,652] Trial 70 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 68}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:30,713] Trial 71 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 43}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:30,770] Trial 72 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 30}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "[I 2024-01-27 09:59:30,845] Trial 73 finished with value: 1.0 and parameters: {'classifier': 'LogisticRegression', 'C': 97711.69177947607}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:30,937] Trial 74 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 23}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:30,990] Trial 75 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 16}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:31,038] Trial 76 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 34}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:31,176] Trial 77 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 58}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:31,332] Trial 78 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 47}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:31,397] Trial 79 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 24}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:31,528] Trial 80 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 89}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2024-01-27 09:59:31,571] Trial 81 finished with value: 1.0 and parameters: {'classifier': 'LogisticRegression', 'C': 2.167323997856195}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2024-01-27 09:59:31,622] Trial 82 finished with value: 1.0 and parameters: {'classifier': 'LogisticRegression', 'C': 1.3449203797460485}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2024-01-27 09:59:31,687] Trial 83 finished with value: 1.0 and parameters: {'classifier': 'LogisticRegression', 'C': 127.687392791062}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:31,937] Trial 84 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 100}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:31,945] Trial 85 finished with value: 1.0 and parameters: {'classifier': 'DecisionTreeClassifier'}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:32,061] Trial 86 finished with value: 1.0 and parameters: {'classifier': 'HistGradientBoostingClassifier', 'max_iter': 52}. Best is trial 0 with value: 1.0.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_6428\\1851355380.py:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  classifier = SGDClassifier(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n",
      "[I 2024-01-27 09:59:32,071] Trial 87 finished with value: 0.9666666666666667 and parameters: {'classifier': 'SGDClassifier', 'alpha': 1.01565539506827e-05}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:32,151] Trial 88 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 38}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:32,374] Trial 89 finished with value: 1.0 and parameters: {'classifier': 'GradientBoost', 'n_estimators': 82}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:32,545] Trial 90 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 77}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:32,608] Trial 91 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 14}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:32,666] Trial 92 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 23}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:32,732] Trial 93 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 16}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:32,757] Trial 94 finished with value: 1.0 and parameters: {'classifier': 'KNeighborsClassifier', 'n_neighbors': 10}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:32,863] Trial 95 finished with value: 1.0 and parameters: {'classifier': 'XGBClassifier', 'n_estimators': 70}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:32,997] Trial 96 finished with value: 1.0 and parameters: {'classifier': 'AdaBoost', 'n_estimators': 54}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:33,174] Trial 97 finished with value: 1.0 and parameters: {'classifier': 'ExtraTreesClassifier', 'n_estimators': 97}. Best is trial 0 with value: 1.0.\n",
      "[I 2024-01-27 09:59:33,309] Trial 98 finished with value: 1.0 and parameters: {'classifier': 'RandomForest', 'n_estimators': 50}. Best is trial 0 with value: 1.0.\n",
      "f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "[I 2024-01-27 09:59:33,379] Trial 99 finished with value: 1.0 and parameters: {'classifier': 'BaggingClassifier', 'n_estimators': 28}. Best is trial 0 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial for Logistic Regression:\n",
      "{'classifier': 'SGDClassifier', 'alpha': 0.006237915040117193}\n",
      "Accuracy: 1.0\n",
      "\n",
      "Best trial for SGD Classifier:\n",
      "{'classifier': 'KNeighborsClassifier', 'n_neighbors': 5}\n",
      "Accuracy: 1.0\n",
      "\n",
      "Best trial for Random Forest:\n",
      "{'classifier': 'AdaBoost', 'n_estimators': 21}\n",
      "Accuracy: 1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_C</th>\n",
       "      <th>params_alpha</th>\n",
       "      <th>params_classifier</th>\n",
       "      <th>params_max_iter</th>\n",
       "      <th>params_n_estimators</th>\n",
       "      <th>params_n_neighbors</th>\n",
       "      <th>system_attrs_completed_rung_0</th>\n",
       "      <th>system_attrs_completed_rung_1</th>\n",
       "      <th>system_attrs_completed_rung_2</th>\n",
       "      <th>system_attrs_completed_rung_3</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>2024-01-27 09:58:42.284169</td>\n",
       "      <td>2024-01-27 09:58:42.295514</td>\n",
       "      <td>0 days 00:00:00.011345</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>2024-01-27 09:58:42.827190</td>\n",
       "      <td>2024-01-27 09:58:42.847765</td>\n",
       "      <td>0 days 00:00:00.020575</td>\n",
       "      <td>0.049516</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>2024-01-27 09:58:44.083941</td>\n",
       "      <td>2024-01-27 09:58:44.101490</td>\n",
       "      <td>0 days 00:00:00.017549</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.086437</td>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>2024-01-27 09:58:45.559953</td>\n",
       "      <td>2024-01-27 09:58:45.570953</td>\n",
       "      <td>0 days 00:00:00.011000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>2024-01-27 09:58:46.174582</td>\n",
       "      <td>2024-01-27 09:58:46.198597</td>\n",
       "      <td>0 days 00:00:00.024015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>57</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>2024-01-27 09:58:47.073593</td>\n",
       "      <td>2024-01-27 09:58:47.091676</td>\n",
       "      <td>0 days 00:00:00.018083</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.093272</td>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>78</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>2024-01-27 09:58:49.342817</td>\n",
       "      <td>2024-01-27 09:58:49.354818</td>\n",
       "      <td>0 days 00:00:00.012001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    number     value             datetime_start          datetime_complete  \\\n",
       "6        6  0.800000 2024-01-27 09:58:42.284169 2024-01-27 09:58:42.295514   \n",
       "12      12  0.966667 2024-01-27 09:58:42.827190 2024-01-27 09:58:42.847765   \n",
       "24      24  0.900000 2024-01-27 09:58:44.083941 2024-01-27 09:58:44.101490   \n",
       "37      37  0.733333 2024-01-27 09:58:45.559953 2024-01-27 09:58:45.570953   \n",
       "46      46  0.300000 2024-01-27 09:58:46.174582 2024-01-27 09:58:46.198597   \n",
       "57      57  0.966667 2024-01-27 09:58:47.073593 2024-01-27 09:58:47.091676   \n",
       "78      78  0.966667 2024-01-27 09:58:49.342817 2024-01-27 09:58:49.354818   \n",
       "\n",
       "                 duration  params_C  params_alpha   params_classifier  \\\n",
       "6  0 days 00:00:00.011345       NaN      0.000492       SGDClassifier   \n",
       "12 0 days 00:00:00.020575  0.049516           NaN  LogisticRegression   \n",
       "24 0 days 00:00:00.017549       NaN      0.086437       SGDClassifier   \n",
       "37 0 days 00:00:00.011000       NaN      0.000012       SGDClassifier   \n",
       "46 0 days 00:00:00.024015  0.000015           NaN  LogisticRegression   \n",
       "57 0 days 00:00:00.018083       NaN      0.093272       SGDClassifier   \n",
       "78 0 days 00:00:00.012001       NaN      0.001109       SGDClassifier   \n",
       "\n",
       "    params_max_iter  params_n_estimators  params_n_neighbors  \\\n",
       "6               NaN                  NaN                 NaN   \n",
       "12              NaN                  NaN                 NaN   \n",
       "24              NaN                  NaN                 NaN   \n",
       "37              NaN                  NaN                 NaN   \n",
       "46              NaN                  NaN                 NaN   \n",
       "57              NaN                  NaN                 NaN   \n",
       "78              NaN                  NaN                 NaN   \n",
       "\n",
       "    system_attrs_completed_rung_0  system_attrs_completed_rung_1  \\\n",
       "6                        0.800000                            NaN   \n",
       "12                       0.966667                            NaN   \n",
       "24                       0.900000                            NaN   \n",
       "37                       0.733333                            NaN   \n",
       "46                       0.300000                            NaN   \n",
       "57                       0.966667                            NaN   \n",
       "78                       0.966667                            NaN   \n",
       "\n",
       "    system_attrs_completed_rung_2  system_attrs_completed_rung_3   state  \n",
       "6                             NaN                            NaN  PRUNED  \n",
       "12                            NaN                            NaN  PRUNED  \n",
       "24                            NaN                            NaN  PRUNED  \n",
       "37                            NaN                            NaN  PRUNED  \n",
       "46                            NaN                            NaN  PRUNED  \n",
       "57                            NaN                            NaN  PRUNED  \n",
       "78                            NaN                            NaN  PRUNED  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier, ExtraTreesClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load a sample dataset (in this case, the Iris dataset)\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def objective(trial):\n",
    "    # Choose a classifier from the available options\n",
    "    classifier_name = trial.suggest_categorical('classifier', ['LogisticRegression', 'SGDClassifier', 'RandomForest', \n",
    "                                                               'AdaBoost', 'GradientBoost', 'BaggingClassifier', \n",
    "                                                               'ExtraTreesClassifier', 'HistGradientBoostingClassifier', \n",
    "                                                               'DecisionTreeClassifier', 'XGBClassifier', 'KNeighborsClassifier'])\n",
    "\n",
    "    # Initialize the classifier variable\n",
    "    classifier = None\n",
    "\n",
    "    # Instantiate the selected classifier with hyperparameters suggested by Optuna\n",
    "    if classifier_name == 'LogisticRegression':\n",
    "        classifier = LogisticRegression(C=trial.suggest_loguniform('C', 1e-5, 1e5))\n",
    "    elif classifier_name == 'SGDClassifier':\n",
    "        classifier = SGDClassifier(alpha=trial.suggest_loguniform('alpha', 1e-5, 1e-1))\n",
    "    elif classifier_name == 'RandomForest':\n",
    "        classifier = RandomForestClassifier(n_estimators=trial.suggest_int('n_estimators', 10, 100))\n",
    "    elif classifier_name == 'AdaBoost':\n",
    "        classifier = AdaBoostClassifier(n_estimators=trial.suggest_int('n_estimators', 10, 100))\n",
    "    elif classifier_name == 'GradientBoost':\n",
    "        classifier = GradientBoostingClassifier(n_estimators=trial.suggest_int('n_estimators', 10, 100))\n",
    "    elif classifier_name == 'BaggingClassifier':\n",
    "        classifier = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
    "                                       n_estimators=trial.suggest_int('n_estimators', 10, 100))\n",
    "    elif classifier_name == 'ExtraTreesClassifier':\n",
    "        classifier = ExtraTreesClassifier(n_estimators=trial.suggest_int('n_estimators', 10, 100))\n",
    "    elif classifier_name == 'HistGradientBoostingClassifier':\n",
    "        classifier = HistGradientBoostingClassifier(max_iter=trial.suggest_int('max_iter', 10, 100))\n",
    "    elif classifier_name == 'DecisionTreeClassifier':\n",
    "        classifier = DecisionTreeClassifier()\n",
    "    elif classifier_name == 'XGBClassifier':\n",
    "        classifier = XGBClassifier(n_estimators=trial.suggest_int('n_estimators', 10, 100))\n",
    "    elif classifier_name == 'KNeighborsClassifier':\n",
    "        classifier = KNeighborsClassifier(n_neighbors=trial.suggest_int('n_neighbors', 1, 10))\n",
    "\n",
    "    if classifier is None:\n",
    "        raise ValueError(f\"Invalid classifier name: {classifier_name}\")\n",
    "\n",
    "    # Training the classifier\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Pruning callback to stop the trial if it's unpromising\n",
    "    trial.report(accuracy_score(y_val, classifier.predict(X_val)), step=trial.number)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "\n",
    "    return accuracy_score(y_val, classifier.predict(X_val))\n",
    "\n",
    "# Set up the optimization study for each classifier\n",
    "study_lr = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n",
    "study_sgd = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n",
    "study_rf = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n",
    "# ... Create separate studies for other classifiers ...\n",
    "\n",
    "# Optimize each study independently\n",
    "study_lr.optimize(objective, n_trials=100)\n",
    "study_sgd.optimize(objective, n_trials=100)\n",
    "study_rf.optimize(objective, n_trials=100)\n",
    "# ... Optimize other studies for other classifiers ...\n",
    "\n",
    "# Print the best parameters and their corresponding accuracy for each classifier\n",
    "print(\"Best trial for Logistic Regression:\")\n",
    "print(study_lr.best_trial.params)\n",
    "print(\"Accuracy:\", study_lr.best_trial.value)\n",
    "\n",
    "print(\"\\nBest trial for SGD Classifier:\")\n",
    "print(study_sgd.best_trial.params)\n",
    "print(\"Accuracy:\", study_sgd.best_trial.value)\n",
    "\n",
    "print(\"\\nBest trial for Random Forest:\")\n",
    "print(study_rf.best_trial.params)\n",
    "print(\"Accuracy:\", study_rf.best_trial.value)\n",
    "# ... Print results for other classifiers ...\n",
    "\n",
    "study.trials_dataframe()[study.trials_dataframe()['state']=='PRUNED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\iNeuron\\\\Projects\\\\scania_failures_2\\\\notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"f:\\\\iNeuron\\\\Projects\\\\scania_failures_2\")\n",
    "\n",
    "from src.utils import (load_yaml,save_yaml,save_binary,\n",
    "                       eval_metrics, parameter_tuning, best_model_finder, \n",
    "                       stacking_clf_trainer, voting_clf_trainer, model_trainer, mlflow_logger)\n",
    "from src.constants import *\n",
    "from src.components.stage_3_data_split import data_splitting_component\n",
    "from src.components.stage_4_final_preprocessing import stage_4_final_processing_component\n",
    "from src.config.configuration_manager import ConfigurationManager\n",
    "from src.entity.entity_config import (Stage2ProcessingConf,\n",
    "                                      ModelMetricsConf, \n",
    "                                      ModelTrainerConf, \n",
    "                                      PreprocessorConf, \n",
    "                                      DataSplitConf,\n",
    "                                      Stage1ProcessingConf)\n",
    "from src import logger\n",
    "\n",
    "import optuna\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "from src.utils import eval_metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from imblearn.combine import SMOTETomek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-27 14:04:05,103: INFO: utils: config.yaml yaml_file is loaded]\n",
      "[2024-01-27 14:04:05,113: INFO: utils: params.yaml yaml_file is loaded]\n",
      "[2024-01-27 14:04:05,117: INFO: utils: schema.yaml yaml_file is loaded]\n",
      "Size:  2000\n",
      "Pre_train_data shape:  (1500, 171) \n",
      "Pre_test_data shape:  (500, 171)\n",
      "[2024-01-27 14:04:06,291: INFO: utils: config.yaml yaml_file is loaded]\n",
      "[2024-01-27 14:04:06,293: INFO: utils: params.yaml yaml_file is loaded]\n",
      "[2024-01-27 14:04:06,295: INFO: utils: schema.yaml yaml_file is loaded]\n",
      "[2024-01-27 14:04:06,297: INFO: utils: schema.yaml yaml_file is loaded]\n",
      "[2024-01-27 14:04:06,299: INFO: utils: Stage 2 Processing Commencing]\n",
      "[2024-01-27 14:04:06,306: INFO: utils: Pipeline created with KnnImputer, RobustScaler]\n",
      "[2024-01-27 14:04:06,307: INFO: utils: SmoteTomek obj created]\n",
      "[2024-01-27 14:04:06,315: INFO: utils: Commencing pipeline transformation]\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.7s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "[2024-01-27 14:04:07,084: INFO: utils: Pipeline transformation complete]\n",
      "[2024-01-27 14:04:07,085: INFO: utils: Commencing SmoteTomek]\n",
      "[2024-01-27 14:04:07,205: INFO: utils: SmoteTomek Complete]\n",
      "[2024-01-27 14:04:07,206: INFO: utils: Returning the transformed dataframe]\n",
      "[2024-01-27 14:04:07,211: INFO: utils: Saving the pipeline object]\n",
      "[2024-01-27 14:04:07,224: INFO: utils: object: artifacts/preprocessor/preprocessor.joblib pickled]\n",
      "[2024-01-27 14:04:07,225: INFO: utils: Pipeline saved at: artifacts/preprocessor/preprocessor.joblib]\n",
      "[2024-01-27 14:04:07,226: INFO: utils: Stage 2 Processing Complete]\n",
      "[2024-01-27 14:04:07,229: INFO: utils: config.yaml yaml_file is loaded]\n",
      "[2024-01-27 14:04:07,232: INFO: utils: params.yaml yaml_file is loaded]\n",
      "[2024-01-27 14:04:07,243: INFO: utils: schema.yaml yaml_file is loaded]\n",
      "[2024-01-27 14:04:07,246: INFO: utils: schema.yaml yaml_file is loaded]\n",
      "[2024-01-27 14:04:07,248: INFO: utils: Stage 2 Processing Commencing]\n",
      "[2024-01-27 14:04:07,274: INFO: utils: pickled_object: artifacts/preprocessor/preprocessor.joblib loaded]\n",
      "[2024-01-27 14:04:07,277: INFO: utils: Pipeline loaded & SmoteTomek created]\n",
      "[2024-01-27 14:04:07,278: INFO: utils: Commencing pipeline transformation]\n",
      "[2024-01-27 14:04:07,514: INFO: utils: Pipeline transformation complete]\n",
      "[2024-01-27 14:04:07,515: INFO: utils: Commencing SmoteTomek]\n",
      "[2024-01-27 14:04:07,531: INFO: utils: SmoteTomek Complete]\n",
      "[2024-01-27 14:04:07,531: INFO: utils: Returning the transformed dataframe]\n",
      "[2024-01-27 14:04:07,532: INFO: utils: Stage 2 Processing Complete]\n",
      "Train data's shape:  (2934, 171)\n",
      "Test data's shape:  (976, 171)\n",
      "\n",
      "x_train shape: (2934, 170), y_train shape: (2934,)\n",
      "x_test shape: (976, 170), y_test shape: (976,)\n",
      "\n",
      "NA values in x_train: [0]\n",
      "NA values in x_test: [0]\n",
      "\n",
      "Target value counts in y_train: class\n",
      "0    1467\n",
      "1    1467\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Target value counts in y_test: class\n",
      "0    488\n",
      "1    488\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "conf_obj = ConfigurationManager()\n",
    "stage_2_obj = conf_obj.get_stage2_processing_config()\n",
    "model_metrics_obj = conf_obj.get_metric_config()\n",
    "model_config_obj = conf_obj.get_model_config()\n",
    "data_split_obj = conf_obj.get_data_split_config()\n",
    "preprocessor_obj = conf_obj.get_preprocessor_config()\n",
    "stage_1_obj = conf_obj.get_stage1_processing_config()\n",
    "\n",
    "size = 2000\n",
    "stage_3_data_split_obj = data_splitting_component(data_split_conf = data_split_obj,\n",
    "                                                          stage1_processor_conf = stage_1_obj)\n",
    "pre_train_df, pre_test_df = stage_3_data_split_obj.data_splitting(size)\n",
    "\n",
    "stage_4_final_processing_obj = stage_4_final_processing_component(data_split_conf = data_split_obj,\n",
    "                                                                    stage_2_processor_conf = stage_2_obj,\n",
    "                                                                    preprocessor_conf = preprocessor_obj)\n",
    "train_df, test_df = stage_4_final_processing_obj.final_processing(pre_train_df, pre_test_df)\n",
    "\n",
    "\n",
    "print (\"Train data's shape: \", train_df.shape)\n",
    "print (\"Test data's shape: \", test_df.shape)\n",
    "\n",
    "x_train = train_df.drop(columns = 'class')\n",
    "y_train = train_df['class']\n",
    "\n",
    "x_test = test_df.drop(columns = 'class')\n",
    "y_test = test_df['class']\n",
    "\n",
    "print(f\"\\nx_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}\")\n",
    "print(f\"\\nNA values in x_train: {x_train.isna().sum().unique()}\")\n",
    "print(f\"NA values in x_test: {x_test.isna().sum().unique()}\")\n",
    "print(f\"\\nTarget value counts in y_train: {y_train.value_counts()}\")\n",
    "print(f\"\\nTarget value counts in y_test: {y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-27 13:14:59,193] A new study created in memory with name: no-name-eb4071f9-4ec7-421e-9524-1cadc2659d00\n",
      "[I 2024-01-27 13:16:01,763] Trial 0 finished with value: 0.8616340389904122 and parameters: {'learning_rate': 'invscaling', 'hidden_layer_sizes': (500, 300, 200, 150, 50)}. Best is trial 0 with value: 0.8616340389904122.\n",
      "[I 2024-01-27 13:16:01,904] Trial 6 finished with value: 0.8313144292433906 and parameters: {'learning_rate': 'adaptive', 'hidden_layer_sizes': (500, 300, 200, 150, 50)}. Best is trial 0 with value: 0.8616340389904122.\n",
      "[I 2024-01-27 13:16:01,965] Trial 2 finished with value: 0.8660621776720875 and parameters: {'learning_rate': 'constant', 'hidden_layer_sizes': (500, 300, 200, 150, 50)}. Best is trial 2 with value: 0.8660621776720875.\n",
      "[I 2024-01-27 13:16:02,091] Trial 5 finished with value: 0.8234663441691715 and parameters: {'learning_rate': 'adaptive', 'hidden_layer_sizes': (500, 300, 200, 150, 50)}. Best is trial 2 with value: 0.8660621776720875.\n",
      "[I 2024-01-27 13:16:02,356] Trial 4 finished with value: 0.865026658371659 and parameters: {'learning_rate': 'constant', 'hidden_layer_sizes': (500, 300, 200, 150, 50)}. Best is trial 2 with value: 0.8660621776720875.\n",
      "[I 2024-01-27 13:16:10,153] Trial 8 pruned. trial was pruned at iteration 0.\n",
      "[I 2024-01-27 13:16:10,405] Trial 9 pruned. trial was pruned at iteration 0.\n",
      "[I 2024-01-27 13:16:15,715] Trial 1 pruned. trial was pruned at iteration 2.\n",
      "[I 2024-01-27 13:16:16,006] Trial 7 pruned. trial was pruned at iteration 2.\n",
      "[I 2024-01-27 13:16:36,955] Trial 3 finished with value: 0.8640178846567554 and parameters: {'learning_rate': 'adaptive', 'hidden_layer_sizes': (1500, 800, 400, 200)}. Best is trial 2 with value: 0.8660621776720875.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OptunaSearchCV(cv=5, enable_pruning=True, estimator=MLPClassifier(), max_iter=5,\n",
       "               n_jobs=-1,\n",
       "               param_distributions={&#x27;hidden_layer_sizes&#x27;: CategoricalDistribution(choices=((500, 300, 200, 150, 50), (700, 500, 300, 100), (1500, 800, 400, 200))),\n",
       "                                    &#x27;learning_rate&#x27;: CategoricalDistribution(choices=(&#x27;constant&#x27;, &#x27;invscaling&#x27;, &#x27;adaptive&#x27;))},\n",
       "               scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OptunaSearchCV</label><div class=\"sk-toggleable__content\"><pre>OptunaSearchCV(cv=5, enable_pruning=True, estimator=MLPClassifier(), max_iter=5,\n",
       "               n_jobs=-1,\n",
       "               param_distributions={&#x27;hidden_layer_sizes&#x27;: CategoricalDistribution(choices=((500, 300, 200, 150, 50), (700, 500, 300, 100), (1500, 800, 400, 200))),\n",
       "                                    &#x27;learning_rate&#x27;: CategoricalDistribution(choices=(&#x27;constant&#x27;, &#x27;invscaling&#x27;, &#x27;adaptive&#x27;))},\n",
       "               scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "OptunaSearchCV(cv=5, enable_pruning=True, estimator=MLPClassifier(), max_iter=5,\n",
       "               n_jobs=-1,\n",
       "               param_distributions={'hidden_layer_sizes': CategoricalDistribution(choices=((500, 300, 200, 150, 50), (700, 500, 300, 100), (1500, 800, 400, 200))),\n",
       "                                    'learning_rate': CategoricalDistribution(choices=('constant', 'invscaling', 'adaptive'))},\n",
       "               scoring='accuracy')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\"learning_rate\":optuna.distributions.CategoricalDistribution(['constant', 'invscaling', 'adaptive']),\n",
    "          'hidden_layer_sizes': optuna.distributions.CategoricalDistribution([(500, 300, 200, 150, 50), (700, 500, 300, 100, ), (1500, 800, 400, 200, )])}\n",
    "optuna_search = optuna.integration.OptunaSearchCV(estimator = MLPClassifier(),\n",
    "                                  param_distributions = params,\n",
    "                                  cv = 5,\n",
    "                                  enable_pruning = True,\n",
    "                                  n_jobs = -1,\n",
    "                                  max_iter = 5,\n",
    "                                  n_trials = 10,\n",
    "                                  refit = True,\n",
    "                                  scoring = 'accuracy')\n",
    "optuna_search.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_path = \"F:\\iNeuron\\Projects\\scania_failures_2\\\\artifacts\\data\\processed\\stage_1_processing\\preprocessed_train_data.csv\"\n",
    "data = pd.read_csv(data_path).iloc[:2000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "0    1962\n",
       "1      38\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 171)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0, 1544,  115,  483,   78,   23,   21,   18,  147,   22,   17,\n",
       "         85,   24,  750,  891, 1297, 1461, 1557, 1608, 1638, 1657,   26,\n",
       "          4,  103,   11,  145,   25,   13,  303,  319,  447,  135,  321],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost in fold 3: 79060.0\n",
      "Fold:  4\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.9s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3118, 170) (3118,) (774, 170) (774,)\n",
      "class\n",
      "0    1559\n",
      "1    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    387\n",
      "0    387\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-27 14:20:24,447] Trial 1 finished with value: 48446.0 and parameters: {'n_estimators': 660, 'criterion': 'log_loss', 'max_features': None, 'class_weight': 'balanced_subsample'}. Best is trial 0 with value: 34360.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost in fold 4: 31030.0\n",
      "Fold:  0\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3118, 170) (3118,) (778, 170) (778,)\n",
      "class\n",
      "1    1559\n",
      "0    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "0    389\n",
      "1    389\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "Cost in fold 0: 32550.0\n",
      "Fold:  1\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3116, 170) (3116,) (786, 170) (786,)\n",
      "class\n",
      "0    1558\n",
      "1    1558\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    393\n",
      "0    393\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "Cost in fold 1: 100.0\n",
      "Fold:  2\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.9s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3126, 170) (3126,) (770, 170) (770,)\n",
      "class\n",
      "0    1563\n",
      "1    1563\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    385\n",
      "0    385\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "Cost in fold 2: 38020.0\n",
      "Fold:  3\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3124, 170) (3124,) (780, 170) (780,)\n",
      "class\n",
      "0    1562\n",
      "1    1562\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    390\n",
      "0    390\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "Cost in fold 3: 61560.0\n",
      "Fold:  4\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.9s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3118, 170) (3118,) (774, 170) (774,)\n",
      "class\n",
      "0    1559\n",
      "1    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    387\n",
      "0    387\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-27 14:20:34,076] Trial 2 finished with value: 33158.0 and parameters: {'n_estimators': 278, 'criterion': 'log_loss', 'max_features': 'log2', 'class_weight': 'balanced'}. Best is trial 2 with value: 33158.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost in fold 4: 33560.0\n",
      "Fold:  0\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.9s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3118, 170) (3118,) (778, 170) (778,)\n",
      "class\n",
      "1    1559\n",
      "0    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "0    389\n",
      "1    389\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "Cost in fold 0: 34050.0\n",
      "Fold:  1\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.9s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3116, 170) (3116,) (786, 170) (786,)\n",
      "class\n",
      "0    1558\n",
      "1    1558\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    393\n",
      "0    393\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "Cost in fold 1: 100.0\n",
      "Fold:  2\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.9s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3126, 170) (3126,) (770, 170) (770,)\n",
      "class\n",
      "0    1563\n",
      "1    1563\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    385\n",
      "0    385\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "Cost in fold 2: 39530.0\n",
      "Fold:  3\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.9s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3124, 170) (3124,) (780, 170) (780,)\n",
      "class\n",
      "0    1562\n",
      "1    1562\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    390\n",
      "0    390\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "Cost in fold 3: 66060.0\n",
      "Fold:  4\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3118, 170) (3118,) (774, 170) (774,)\n",
      "class\n",
      "0    1559\n",
      "1    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    387\n",
      "0    387\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-27 14:20:46,191] Trial 3 finished with value: 34656.0 and parameters: {'n_estimators': 428, 'criterion': 'gini', 'max_features': 'log2', 'class_weight': 'balanced'}. Best is trial 2 with value: 33158.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost in fold 4: 33540.0\n",
      "Fold:  0\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3118, 170) (3118,) (778, 170) (778,)\n",
      "class\n",
      "1    1559\n",
      "0    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "0    389\n",
      "1    389\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "Cost in fold 0: 33540.0\n",
      "Fold:  1\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3116, 170) (3116,) (786, 170) (786,)\n",
      "class\n",
      "0    1558\n",
      "1    1558\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    393\n",
      "0    393\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "Cost in fold 1: 600.0\n",
      "Fold:  2\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3126, 170) (3126,) (770, 170) (770,)\n",
      "class\n",
      "0    1563\n",
      "1    1563\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    385\n",
      "0    385\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "Cost in fold 2: 50520.0\n",
      "Fold:  3\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.9s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3124, 170) (3124,) (780, 170) (780,)\n",
      "class\n",
      "0    1562\n",
      "1    1562\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    390\n",
      "0    390\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "Cost in fold 3: 71560.0\n",
      "Fold:  4\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3118, 170) (3118,) (774, 170) (774,)\n",
      "class\n",
      "0    1559\n",
      "1    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    387\n",
      "0    387\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-27 14:21:00,858] Trial 4 finished with value: 38350.0 and parameters: {'n_estimators': 510, 'criterion': 'gini', 'max_features': 'sqrt', 'class_weight': 'balanced_subsample'}. Best is trial 2 with value: 33158.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost in fold 4: 35530.0\n",
      "Fold:  0\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   1.0s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3118, 170) (3118,) (778, 170) (778,)\n",
      "class\n",
      "1    1559\n",
      "0    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "0    389\n",
      "1    389\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "Cost in fold 0: 32050.0\n",
      "Fold:  1\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.9s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3116, 170) (3116,) (786, 170) (786,)\n",
      "class\n",
      "0    1558\n",
      "1    1558\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    393\n",
      "0    393\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "Cost in fold 1: 100.0\n",
      "Fold:  2\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.9s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3126, 170) (3126,) (770, 170) (770,)\n",
      "class\n",
      "0    1563\n",
      "1    1563\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    385\n",
      "0    385\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "Cost in fold 2: 39030.0\n",
      "Fold:  3\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3124, 170) (3124,) (780, 170) (780,)\n",
      "class\n",
      "0    1562\n",
      "1    1562\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    390\n",
      "0    390\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "Cost in fold 3: 61560.0\n",
      "Fold:  4\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3118, 170) (3118,) (774, 170) (774,)\n",
      "class\n",
      "0    1559\n",
      "1    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    387\n",
      "0    387\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-27 14:21:10,766] Trial 5 finished with value: 33060.0 and parameters: {'n_estimators': 254, 'criterion': 'log_loss', 'max_features': 'log2', 'class_weight': 'balanced_subsample'}. Best is trial 5 with value: 33060.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost in fold 4: 32560.0\n",
      "Fold:  0\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3118, 170) (3118,) (778, 170) (778,)\n",
      "class\n",
      "1    1559\n",
      "0    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "0    389\n",
      "1    389\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-27 14:21:12,684] Trial 6 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost in fold 0: 37550.0\n",
      "Fold:  0\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3118, 170) (3118,) (778, 170) (778,)\n",
      "class\n",
      "1    1559\n",
      "0    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "0    389\n",
      "1    389\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "Cost in fold 0: 33550.0\n",
      "Fold:  1\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3116, 170) (3116,) (786, 170) (786,)\n",
      "class\n",
      "0    1558\n",
      "1    1558\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    393\n",
      "0    393\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "Cost in fold 1: 100.0\n",
      "Fold:  2\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3126, 170) (3126,) (770, 170) (770,)\n",
      "class\n",
      "0    1563\n",
      "1    1563\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    385\n",
      "0    385\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "Cost in fold 2: 38020.0\n",
      "Fold:  3\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3124, 170) (3124,) (780, 170) (780,)\n",
      "class\n",
      "0    1562\n",
      "1    1562\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    390\n",
      "0    390\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "Cost in fold 3: 60560.0\n",
      "Fold:  4\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3118, 170) (3118,) (774, 170) (774,)\n",
      "class\n",
      "0    1559\n",
      "1    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    387\n",
      "0    387\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-27 14:21:25,792] Trial 7 finished with value: 33458.0 and parameters: {'n_estimators': 583, 'criterion': 'entropy', 'max_features': 'log2', 'class_weight': 'balanced_subsample'}. Best is trial 5 with value: 33060.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost in fold 4: 35060.0\n",
      "Fold:  0\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3118, 170) (3118,) (778, 170) (778,)\n",
      "class\n",
      "1    1559\n",
      "0    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "0    389\n",
      "1    389\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-27 14:21:29,007] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost in fold 0: 34550.0\n",
      "Fold:  0\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3118, 170) (3118,) (778, 170) (778,)\n",
      "class\n",
      "1    1559\n",
      "0    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "0    389\n",
      "1    389\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "Cost in fold 0: 32050.0\n",
      "Fold:  1\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.9s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3116, 170) (3116,) (786, 170) (786,)\n",
      "class\n",
      "0    1558\n",
      "1    1558\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    393\n",
      "0    393\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "Cost in fold 1: 100.0\n",
      "Fold:  2\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3126, 170) (3126,) (770, 170) (770,)\n",
      "class\n",
      "0    1563\n",
      "1    1563\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    385\n",
      "0    385\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "Cost in fold 2: 38010.0\n",
      "Fold:  3\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.9s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3124, 170) (3124,) (780, 170) (780,)\n",
      "class\n",
      "0    1562\n",
      "1    1562\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    390\n",
      "0    390\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "Cost in fold 3: 65560.0\n",
      "Fold:  4\n",
      "(1600, 170) (1600,) (400, 170) (400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(3118, 170) (3118,) (774, 170) (774,)\n",
      "class\n",
      "0    1559\n",
      "1    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    387\n",
      "0    387\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-27 14:21:44,615] Trial 9 finished with value: 33756.0 and parameters: {'n_estimators': 770, 'criterion': 'log_loss', 'max_features': 'log2', 'class_weight': 'balanced'}. Best is trial 5 with value: 33060.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost in fold 4: 33060.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,shuffle= False, random_state=None)\n",
    "\n",
    "def objective(trial, data = data):\n",
    "  # train_x,test_x,y_train,y_test=train_test_split(data,target,test_size=0.20,random_state=25)\n",
    "  # param= {list(config['optuna']['ExtraTreesClassifier'].keys())[0] : eval(config['optuna']['ExtraTreesClassifier']['penalty'])}\n",
    "\n",
    "  # preprocessor_config = obj.get_preprocessor_config()\n",
    "  # schema = load_yaml(obj.schema)\n",
    "  # target = list(schema.Target.keys())[0]\n",
    "  pipeline = Pipeline(steps=[('Knn_imputer',KNNImputer()),\n",
    "                             ('Robust_Scaler',RobustScaler())],\n",
    "                            verbose=True)\n",
    "  smote = SMOTETomek(n_jobs=-1,sampling_strategy='minority',random_state=8)\n",
    "  X = data.drop(columns='class')\n",
    "  y = data['class']\n",
    "  # space = {}\n",
    "  score = []\n",
    "  space = {\n",
    "    'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "    'criterion': trial.suggest_categorical('criterion', ['log_loss', 'entropy', 'gini']),\n",
    "    'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "    'class_weight': trial.suggest_categorical('class_weight', ['balanced', 'balanced_subsample'])}\n",
    "  # obj = ConfigurationManager()\n",
    "  # for key,value in obj.config_path['optuna']['ExtraTreesClassifier'].items():\n",
    "  #   space[key] = eval(value)\n",
    "  for fold, (train_indices, test_indices) in enumerate(skf.split(X, y)):\n",
    "    print (\"Fold: \",fold)\n",
    "    x_train_ = data.drop(columns = 'class').iloc[train_indices]\n",
    "    y_train_ = data['class'].iloc[train_indices]\n",
    "    x_test_  = data.drop(columns = 'class').iloc[test_indices]\n",
    "    y_test_  = data['class'].iloc[test_indices]\n",
    "\n",
    "    print(x_train_.shape, y_train_.shape, x_test_.shape, y_test_.shape)\n",
    "    print(\"Starting pipeline transformation of Xtrain\")\n",
    "    X_train_transformed = pipeline.fit_transform(X = x_train_, y = y_train_)\n",
    "    print(\"Starting SMOTE transformation of Xtrain,Ytrain\")\n",
    "    X_train_smote,y_train_smote = smote.fit_resample(X = X_train_transformed,y = y_train_)\n",
    "\n",
    "    print(\"Starting pipeline transformation of Xtest\")\n",
    "    X_test_transformed = pipeline.transform(X = x_test_)\n",
    "    print(\"Starting SMOTE transformation of Xtest,Ytest\")\n",
    "    X_test_smote,y_test_smote = smote.fit_resample(X = X_test_transformed,y = y_test_)\n",
    "\n",
    "\n",
    "    print(X_train_smote.shape, y_train_smote.shape, X_test_smote.shape, y_test_smote.shape)\n",
    "\n",
    "    print (y_train_smote.value_counts())\n",
    "    print (y_test_smote.value_counts())\n",
    "    log_reg=ExtraTreesClassifier(**space)\n",
    "    print(\"Fitting model\")\n",
    "    log_reg.fit(X_train_smote,y_train_smote)\n",
    "    y_predict=log_reg.predict(X_test_smote)\n",
    "    cost = eval_metrics(y_true = y_test_smote , y_pred = y_predict)['Cost']\n",
    "    print (f\"Cost in fold {fold}: {cost}\")\n",
    "    trial.report(cost, fold)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "    else:\n",
    "       score.append(cost)\n",
    "  return np.mean(score)\n",
    "\n",
    "pruner=optuna.pruners.MedianPruner()\n",
    "find_param=optuna.create_study(storage='mysql://root:qwerty12345@localhost/example',\n",
    "                               load_if_exists=True,direction = \"minimize\",\n",
    "                               pruner=pruner)\n",
    "find_param.optimize(objective,n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_class_weight</th>\n",
       "      <th>params_criterion</th>\n",
       "      <th>params_max_features</th>\n",
       "      <th>params_n_estimators</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>34360.0</td>\n",
       "      <td>2024-01-27 14:19:19</td>\n",
       "      <td>2024-01-27 14:19:32</td>\n",
       "      <td>0 days 00:00:13</td>\n",
       "      <td>balanced</td>\n",
       "      <td>entropy</td>\n",
       "      <td>log2</td>\n",
       "      <td>713</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>48446.0</td>\n",
       "      <td>2024-01-27 14:19:32</td>\n",
       "      <td>2024-01-27 14:20:24</td>\n",
       "      <td>0 days 00:00:52</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>None</td>\n",
       "      <td>660</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>33158.0</td>\n",
       "      <td>2024-01-27 14:20:24</td>\n",
       "      <td>2024-01-27 14:20:34</td>\n",
       "      <td>0 days 00:00:10</td>\n",
       "      <td>balanced</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>log2</td>\n",
       "      <td>278</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>34656.0</td>\n",
       "      <td>2024-01-27 14:20:34</td>\n",
       "      <td>2024-01-27 14:20:46</td>\n",
       "      <td>0 days 00:00:12</td>\n",
       "      <td>balanced</td>\n",
       "      <td>gini</td>\n",
       "      <td>log2</td>\n",
       "      <td>428</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>38350.0</td>\n",
       "      <td>2024-01-27 14:20:46</td>\n",
       "      <td>2024-01-27 14:21:01</td>\n",
       "      <td>0 days 00:00:15</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>gini</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>510</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>33060.0</td>\n",
       "      <td>2024-01-27 14:21:01</td>\n",
       "      <td>2024-01-27 14:21:11</td>\n",
       "      <td>0 days 00:00:10</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>log2</td>\n",
       "      <td>254</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>33458.0</td>\n",
       "      <td>2024-01-27 14:21:13</td>\n",
       "      <td>2024-01-27 14:21:26</td>\n",
       "      <td>0 days 00:00:13</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>entropy</td>\n",
       "      <td>log2</td>\n",
       "      <td>583</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>33756.0</td>\n",
       "      <td>2024-01-27 14:21:29</td>\n",
       "      <td>2024-01-27 14:21:45</td>\n",
       "      <td>0 days 00:00:16</td>\n",
       "      <td>balanced</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>log2</td>\n",
       "      <td>770</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number    value      datetime_start   datetime_complete        duration  \\\n",
       "0       0  34360.0 2024-01-27 14:19:19 2024-01-27 14:19:32 0 days 00:00:13   \n",
       "1       1  48446.0 2024-01-27 14:19:32 2024-01-27 14:20:24 0 days 00:00:52   \n",
       "2       2  33158.0 2024-01-27 14:20:24 2024-01-27 14:20:34 0 days 00:00:10   \n",
       "3       3  34656.0 2024-01-27 14:20:34 2024-01-27 14:20:46 0 days 00:00:12   \n",
       "4       4  38350.0 2024-01-27 14:20:46 2024-01-27 14:21:01 0 days 00:00:15   \n",
       "5       5  33060.0 2024-01-27 14:21:01 2024-01-27 14:21:11 0 days 00:00:10   \n",
       "7       7  33458.0 2024-01-27 14:21:13 2024-01-27 14:21:26 0 days 00:00:13   \n",
       "9       9  33756.0 2024-01-27 14:21:29 2024-01-27 14:21:45 0 days 00:00:16   \n",
       "\n",
       "  params_class_weight params_criterion params_max_features  \\\n",
       "0            balanced          entropy                log2   \n",
       "1  balanced_subsample         log_loss                None   \n",
       "2            balanced         log_loss                log2   \n",
       "3            balanced             gini                log2   \n",
       "4  balanced_subsample             gini                sqrt   \n",
       "5  balanced_subsample         log_loss                log2   \n",
       "7  balanced_subsample          entropy                log2   \n",
       "9            balanced         log_loss                log2   \n",
       "\n",
       "   params_n_estimators     state  \n",
       "0                  713  COMPLETE  \n",
       "1                  660  COMPLETE  \n",
       "2                  278  COMPLETE  \n",
       "3                  428  COMPLETE  \n",
       "4                  510  COMPLETE  \n",
       "5                  254  COMPLETE  \n",
       "7                  583  COMPLETE  \n",
       "9                  770  COMPLETE  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_param.trials_dataframe()[find_param.trials_dataframe()['state']=='COMPLETE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33060.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_param.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-27 15:10:05,271] A new study created in RDB with name: no-name-b2e005f9-b0c3-4de7-b0e0-468fafe85a05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Fold:  0\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3118, 170), y_train_smote.shape: (3118,),\n",
      "x_test_smote.shape: (778, 170), y_test_smote.shape:(778,)\n",
      "class\n",
      "1    1559\n",
      "0    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "0    389\n",
      "1    389\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 0: 2220.0\n",
      "\n",
      "\n",
      "Fold:  1\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3116, 170), y_train_smote.shape: (3116,),\n",
      "x_test_smote.shape: (786, 170), y_test_smote.shape:(786,)\n",
      "class\n",
      "0    1558\n",
      "1    1558\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    393\n",
      "0    393\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 1: 38350.0\n",
      "\n",
      "\n",
      "Fold:  2\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3126, 170), y_train_smote.shape: (3126,),\n",
      "x_test_smote.shape: (770, 170), y_test_smote.shape:(770,)\n",
      "class\n",
      "0    1563\n",
      "1    1563\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    385\n",
      "0    385\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 2: 34470.0\n",
      "\n",
      "\n",
      "Fold:  3\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.7s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3124, 170), y_train_smote.shape: (3124,),\n",
      "x_test_smote.shape: (780, 170), y_test_smote.shape:(780,)\n",
      "class\n",
      "0    1562\n",
      "1    1562\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    390\n",
      "0    390\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 3: 1450.0\n",
      "\n",
      "\n",
      "Fold:  4\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.7s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-27 15:10:11,186] Trial 0 finished with value: 23910.0 and parameters: {'penalty': 'l2'}. Best is trial 0 with value: 23910.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3118, 170), y_train_smote.shape: (3118,),\n",
      "x_test_smote.shape: (774, 170), y_test_smote.shape:(774,)\n",
      "class\n",
      "0    1559\n",
      "1    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    387\n",
      "0    387\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 4: 43060.0\n",
      "\n",
      "\n",
      "Fold:  0\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.7s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3118, 170), y_train_smote.shape: (3118,),\n",
      "x_test_smote.shape: (778, 170), y_test_smote.shape:(778,)\n",
      "class\n",
      "1    1559\n",
      "0    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "0    389\n",
      "1    389\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 0: 2220.0\n",
      "\n",
      "\n",
      "Fold:  1\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3116, 170), y_train_smote.shape: (3116,),\n",
      "x_test_smote.shape: (786, 170), y_test_smote.shape:(786,)\n",
      "class\n",
      "0    1558\n",
      "1    1558\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    393\n",
      "0    393\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 1: 38350.0\n",
      "\n",
      "\n",
      "Fold:  2\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.7s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3126, 170), y_train_smote.shape: (3126,),\n",
      "x_test_smote.shape: (770, 170), y_test_smote.shape:(770,)\n",
      "class\n",
      "0    1563\n",
      "1    1563\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    385\n",
      "0    385\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 2: 34470.0\n",
      "\n",
      "\n",
      "Fold:  3\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.7s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3124, 170), y_train_smote.shape: (3124,),\n",
      "x_test_smote.shape: (780, 170), y_test_smote.shape:(780,)\n",
      "class\n",
      "0    1562\n",
      "1    1562\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    390\n",
      "0    390\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 3: 1450.0\n",
      "\n",
      "\n",
      "Fold:  4\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-27 15:10:16,914] Trial 1 finished with value: 23910.0 and parameters: {'penalty': 'l2'}. Best is trial 0 with value: 23910.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3118, 170), y_train_smote.shape: (3118,),\n",
      "x_test_smote.shape: (774, 170), y_test_smote.shape:(774,)\n",
      "class\n",
      "0    1559\n",
      "1    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    387\n",
      "0    387\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 4: 43060.0\n",
      "\n",
      "\n",
      "Fold:  0\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.7s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3118, 170), y_train_smote.shape: (3118,),\n",
      "x_test_smote.shape: (778, 170), y_test_smote.shape:(778,)\n",
      "class\n",
      "1    1559\n",
      "0    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "0    389\n",
      "1    389\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 0: 2200.0\n",
      "\n",
      "\n",
      "Fold:  1\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3116, 170), y_train_smote.shape: (3116,),\n",
      "x_test_smote.shape: (786, 170), y_test_smote.shape:(786,)\n",
      "class\n",
      "0    1558\n",
      "1    1558\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    393\n",
      "0    393\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 1: 38340.0\n",
      "\n",
      "\n",
      "Fold:  2\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3126, 170), y_train_smote.shape: (3126,),\n",
      "x_test_smote.shape: (770, 170), y_test_smote.shape:(770,)\n",
      "class\n",
      "0    1563\n",
      "1    1563\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    385\n",
      "0    385\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 2: 31290.0\n",
      "\n",
      "\n",
      "Fold:  3\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3124, 170), y_train_smote.shape: (3124,),\n",
      "x_test_smote.shape: (780, 170), y_test_smote.shape:(780,)\n",
      "class\n",
      "0    1562\n",
      "1    1562\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    390\n",
      "0    390\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 3: 17270.0\n",
      "\n",
      "\n",
      "Fold:  4\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.7s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-27 15:10:22,647] Trial 2 finished with value: 26046.0 and parameters: {'penalty': None}. Best is trial 0 with value: 23910.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3118, 170), y_train_smote.shape: (3118,),\n",
      "x_test_smote.shape: (774, 170), y_test_smote.shape:(774,)\n",
      "class\n",
      "0    1559\n",
      "1    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    387\n",
      "0    387\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 4: 41130.0\n",
      "\n",
      "\n",
      "Fold:  0\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3118, 170), y_train_smote.shape: (3118,),\n",
      "x_test_smote.shape: (778, 170), y_test_smote.shape:(778,)\n",
      "class\n",
      "1    1559\n",
      "0    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "0    389\n",
      "1    389\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 0: 2220.0\n",
      "\n",
      "\n",
      "Fold:  1\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3116, 170), y_train_smote.shape: (3116,),\n",
      "x_test_smote.shape: (786, 170), y_test_smote.shape:(786,)\n",
      "class\n",
      "0    1558\n",
      "1    1558\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    393\n",
      "0    393\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 1: 38350.0\n",
      "\n",
      "\n",
      "Fold:  2\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3126, 170), y_train_smote.shape: (3126,),\n",
      "x_test_smote.shape: (770, 170), y_test_smote.shape:(770,)\n",
      "class\n",
      "0    1563\n",
      "1    1563\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    385\n",
      "0    385\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 2: 34470.0\n",
      "\n",
      "\n",
      "Fold:  3\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.7s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3124, 170), y_train_smote.shape: (3124,),\n",
      "x_test_smote.shape: (780, 170), y_test_smote.shape:(780,)\n",
      "class\n",
      "0    1562\n",
      "1    1562\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    390\n",
      "0    390\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 3: 1450.0\n",
      "\n",
      "\n",
      "Fold:  4\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-27 15:10:28,339] Trial 3 finished with value: 23910.0 and parameters: {'penalty': 'l2'}. Best is trial 0 with value: 23910.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3118, 170), y_train_smote.shape: (3118,),\n",
      "x_test_smote.shape: (774, 170), y_test_smote.shape:(774,)\n",
      "class\n",
      "0    1559\n",
      "1    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    387\n",
      "0    387\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 4: 43060.0\n",
      "\n",
      "\n",
      "Fold:  0\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3118, 170), y_train_smote.shape: (3118,),\n",
      "x_test_smote.shape: (778, 170), y_test_smote.shape:(778,)\n",
      "class\n",
      "1    1559\n",
      "0    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "0    389\n",
      "1    389\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 0: 2220.0\n",
      "\n",
      "\n",
      "Fold:  1\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3116, 170), y_train_smote.shape: (3116,),\n",
      "x_test_smote.shape: (786, 170), y_test_smote.shape:(786,)\n",
      "class\n",
      "0    1558\n",
      "1    1558\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    393\n",
      "0    393\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 1: 38350.0\n",
      "\n",
      "\n",
      "Fold:  2\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.9s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3126, 170), y_train_smote.shape: (3126,),\n",
      "x_test_smote.shape: (770, 170), y_test_smote.shape:(770,)\n",
      "class\n",
      "0    1563\n",
      "1    1563\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    385\n",
      "0    385\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 2: 34470.0\n",
      "\n",
      "\n",
      "Fold:  3\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3124, 170), y_train_smote.shape: (3124,),\n",
      "x_test_smote.shape: (780, 170), y_test_smote.shape:(780,)\n",
      "class\n",
      "0    1562\n",
      "1    1562\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    390\n",
      "0    390\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 3: 1450.0\n",
      "\n",
      "\n",
      "Fold:  4\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-27 15:10:34,444] Trial 4 finished with value: 23910.0 and parameters: {'penalty': 'l2'}. Best is trial 0 with value: 23910.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3118, 170), y_train_smote.shape: (3118,),\n",
      "x_test_smote.shape: (774, 170), y_test_smote.shape:(774,)\n",
      "class\n",
      "0    1559\n",
      "1    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    387\n",
      "0    387\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 4: 43060.0\n",
      "\n",
      "\n",
      "Fold:  0\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3118, 170), y_train_smote.shape: (3118,),\n",
      "x_test_smote.shape: (778, 170), y_test_smote.shape:(778,)\n",
      "class\n",
      "1    1559\n",
      "0    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "0    389\n",
      "1    389\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 0: 2220.0\n",
      "\n",
      "\n",
      "Fold:  1\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3116, 170), y_train_smote.shape: (3116,),\n",
      "x_test_smote.shape: (786, 170), y_test_smote.shape:(786,)\n",
      "class\n",
      "0    1558\n",
      "1    1558\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    393\n",
      "0    393\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 1: 38350.0\n",
      "\n",
      "\n",
      "Fold:  2\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3126, 170), y_train_smote.shape: (3126,),\n",
      "x_test_smote.shape: (770, 170), y_test_smote.shape:(770,)\n",
      "class\n",
      "0    1563\n",
      "1    1563\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    385\n",
      "0    385\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 2: 34470.0\n",
      "\n",
      "\n",
      "Fold:  3\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3124, 170), y_train_smote.shape: (3124,),\n",
      "x_test_smote.shape: (780, 170), y_test_smote.shape:(780,)\n",
      "class\n",
      "0    1562\n",
      "1    1562\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    390\n",
      "0    390\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 3: 1450.0\n",
      "\n",
      "\n",
      "Fold:  4\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-27 15:10:40,575] Trial 5 finished with value: 23910.0 and parameters: {'penalty': 'l2'}. Best is trial 0 with value: 23910.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3118, 170), y_train_smote.shape: (3118,),\n",
      "x_test_smote.shape: (774, 170), y_test_smote.shape:(774,)\n",
      "class\n",
      "0    1559\n",
      "1    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    387\n",
      "0    387\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 4: 43060.0\n",
      "\n",
      "\n",
      "Fold:  0\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3118, 170), y_train_smote.shape: (3118,),\n",
      "x_test_smote.shape: (778, 170), y_test_smote.shape:(778,)\n",
      "class\n",
      "1    1559\n",
      "0    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "0    389\n",
      "1    389\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 0: 2220.0\n",
      "\n",
      "\n",
      "Fold:  1\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3116, 170), y_train_smote.shape: (3116,),\n",
      "x_test_smote.shape: (786, 170), y_test_smote.shape:(786,)\n",
      "class\n",
      "0    1558\n",
      "1    1558\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    393\n",
      "0    393\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 1: 38350.0\n",
      "\n",
      "\n",
      "Fold:  2\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3126, 170), y_train_smote.shape: (3126,),\n",
      "x_test_smote.shape: (770, 170), y_test_smote.shape:(770,)\n",
      "class\n",
      "0    1563\n",
      "1    1563\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    385\n",
      "0    385\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 2: 34470.0\n",
      "\n",
      "\n",
      "Fold:  3\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3124, 170), y_train_smote.shape: (3124,),\n",
      "x_test_smote.shape: (780, 170), y_test_smote.shape:(780,)\n",
      "class\n",
      "0    1562\n",
      "1    1562\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    390\n",
      "0    390\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 3: 1450.0\n",
      "\n",
      "\n",
      "Fold:  4\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-27 15:10:46,658] Trial 6 finished with value: 23910.0 and parameters: {'penalty': 'l2'}. Best is trial 0 with value: 23910.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3118, 170), y_train_smote.shape: (3118,),\n",
      "x_test_smote.shape: (774, 170), y_test_smote.shape:(774,)\n",
      "class\n",
      "0    1559\n",
      "1    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    387\n",
      "0    387\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 4: 43060.0\n",
      "\n",
      "\n",
      "Fold:  0\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3118, 170), y_train_smote.shape: (3118,),\n",
      "x_test_smote.shape: (778, 170), y_test_smote.shape:(778,)\n",
      "class\n",
      "1    1559\n",
      "0    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "0    389\n",
      "1    389\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 0: 2220.0\n",
      "\n",
      "\n",
      "Fold:  1\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.9s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3116, 170), y_train_smote.shape: (3116,),\n",
      "x_test_smote.shape: (786, 170), y_test_smote.shape:(786,)\n",
      "class\n",
      "0    1558\n",
      "1    1558\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    393\n",
      "0    393\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 1: 38350.0\n",
      "\n",
      "\n",
      "Fold:  2\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3126, 170), y_train_smote.shape: (3126,),\n",
      "x_test_smote.shape: (770, 170), y_test_smote.shape:(770,)\n",
      "class\n",
      "0    1563\n",
      "1    1563\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    385\n",
      "0    385\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 2: 34470.0\n",
      "\n",
      "\n",
      "Fold:  3\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3124, 170), y_train_smote.shape: (3124,),\n",
      "x_test_smote.shape: (780, 170), y_test_smote.shape:(780,)\n",
      "class\n",
      "0    1562\n",
      "1    1562\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    390\n",
      "0    390\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 3: 1450.0\n",
      "\n",
      "\n",
      "Fold:  4\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-27 15:10:52,863] Trial 7 finished with value: 23910.0 and parameters: {'penalty': 'l2'}. Best is trial 0 with value: 23910.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3118, 170), y_train_smote.shape: (3118,),\n",
      "x_test_smote.shape: (774, 170), y_test_smote.shape:(774,)\n",
      "class\n",
      "0    1559\n",
      "1    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    387\n",
      "0    387\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 4: 43060.0\n",
      "\n",
      "\n",
      "Fold:  0\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3118, 170), y_train_smote.shape: (3118,),\n",
      "x_test_smote.shape: (778, 170), y_test_smote.shape:(778,)\n",
      "class\n",
      "1    1559\n",
      "0    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "0    389\n",
      "1    389\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 0: 2200.0\n",
      "\n",
      "\n",
      "Fold:  1\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3116, 170), y_train_smote.shape: (3116,),\n",
      "x_test_smote.shape: (786, 170), y_test_smote.shape:(786,)\n",
      "class\n",
      "0    1558\n",
      "1    1558\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    393\n",
      "0    393\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 1: 38340.0\n",
      "\n",
      "\n",
      "Fold:  2\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3126, 170), y_train_smote.shape: (3126,),\n",
      "x_test_smote.shape: (770, 170), y_test_smote.shape:(770,)\n",
      "class\n",
      "0    1563\n",
      "1    1563\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    385\n",
      "0    385\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 2: 31290.0\n",
      "\n",
      "\n",
      "Fold:  3\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-27 15:10:57,734] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3124, 170), y_train_smote.shape: (3124,),\n",
      "x_test_smote.shape: (780, 170), y_test_smote.shape:(780,)\n",
      "class\n",
      "0    1562\n",
      "1    1562\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    390\n",
      "0    390\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 3: 17270.0\n",
      "\n",
      "\n",
      "Fold:  0\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3118, 170), y_train_smote.shape: (3118,),\n",
      "x_test_smote.shape: (778, 170), y_test_smote.shape:(778,)\n",
      "class\n",
      "1    1559\n",
      "0    1559\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "0    389\n",
      "1    389\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 0: 2200.0\n",
      "\n",
      "\n",
      "Fold:  1\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3116, 170), y_train_smote.shape: (3116,),\n",
      "x_test_smote.shape: (786, 170), y_test_smote.shape:(786,)\n",
      "class\n",
      "0    1558\n",
      "1    1558\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    393\n",
      "0    393\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 1: 38340.0\n",
      "\n",
      "\n",
      "Fold:  2\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3126, 170), y_train_smote.shape: (3126,),\n",
      "x_test_smote.shape: (770, 170), y_test_smote.shape:(770,)\n",
      "class\n",
      "0    1563\n",
      "1    1563\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    385\n",
      "0    385\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 2: 31290.0\n",
      "\n",
      "\n",
      "Fold:  3\n",
      "x_train_.shape: (1600, 170), y_train_.shape: (1600,),\n",
      "x_test_.shape: (400, 170), y_test_.shape:(400,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=   0.8s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.0s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-27 15:11:02,636] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "x_train_smote.shape: (3124, 170), y_train_smote.shape: (3124,),\n",
      "x_test_smote.shape: (780, 170), y_test_smote.shape:(780,)\n",
      "class\n",
      "0    1562\n",
      "1    1562\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    390\n",
      "0    390\n",
      "Name: count, dtype: int64\n",
      "Fitting model\n",
      "\n",
      "Cost in fold 3: 17270.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,shuffle= False, random_state=None)\n",
    "\n",
    "def objective(trial, data = data):\n",
    "  # train_x,test_x,y_train,y_test=train_test_split(data,target,test_size=0.20,random_state=25)\n",
    "  # param= {list(config['optuna']['ExtraTreesClassifier'].keys())[0] : eval(config['optuna']['ExtraTreesClassifier']['penalty'])}\n",
    "\n",
    "  # preprocessor_config = obj.get_preprocessor_config()\n",
    "  # schema = load_yaml(obj.schema)\n",
    "  # target = list(schema.Target.keys())[0]\n",
    "  pipeline = Pipeline(steps=[('Knn_imputer',KNNImputer()),\n",
    "                             ('Robust_Scaler',RobustScaler())],\n",
    "                            verbose=True)\n",
    "  smote = SMOTETomek(n_jobs=-1,sampling_strategy='minority',random_state=8)\n",
    "  X = data.drop(columns='class').iloc[:2000,:]\n",
    "  y = data['class'].iloc[:2000]\n",
    "  # space = {}\n",
    "  score = []\n",
    "  # space = {\n",
    "  #   'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "  #   'criterion': trial.suggest_categorical('criterion', ['log_loss', 'entropy', 'gini']),\n",
    "  #   'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "  #   'class_weight': trial.suggest_categorical('class_weight', ['balanced', 'balanced_subsample'])}\n",
    "  space = {\n",
    "         'penalty': trial.suggest_categorical('penalty', ['l2', None])\n",
    "  }\n",
    "  # obj = ConfigurationManager()\n",
    "  # for key,value in obj.config_path['optuna']['ExtraTreesClassifier'].items():\n",
    "  #   space[key] = eval(value)\n",
    "  for fold, (train_indices, test_indices) in enumerate(skf.split(X, y)):\n",
    "    print (\"\\n\\nFold: \",fold)\n",
    "    x_train_ = data.drop(columns = 'class').iloc[train_indices]\n",
    "    y_train_ = data['class'].iloc[train_indices]\n",
    "    x_test_  = data.drop(columns = 'class').iloc[test_indices]\n",
    "    y_test_  = data['class'].iloc[test_indices]\n",
    "\n",
    "    print(f\"x_train_.shape: {x_train_.shape}, y_train_.shape: {y_train_.shape},\\nx_test_.shape: {x_test_.shape}, y_test_.shape:{y_test_.shape}\")\n",
    "    print(\"Starting pipeline transformation of Xtrain\")\n",
    "    X_train_transformed = pipeline.fit_transform(X = x_train_, y = y_train_)\n",
    "    print(\"Starting SMOTE transformation of Xtrain,Ytrain\")\n",
    "    X_train_smote,y_train_smote = smote.fit_resample(X = X_train_transformed,y = y_train_)\n",
    "\n",
    "    print(\"Starting pipeline transformation of Xtest\")\n",
    "    X_test_transformed = pipeline.transform(X = x_test_)\n",
    "    print(\"Starting SMOTE transformation of Xtest,Ytest\")\n",
    "    X_test_smote,y_test_smote = smote.fit_resample(X = X_test_transformed,y = y_test_)\n",
    "\n",
    "    print(f\"x_train_smote.shape: {X_train_smote.shape}, y_train_smote.shape: {y_train_smote.shape},\\nx_test_smote.shape: {X_test_smote.shape}, y_test_smote.shape:{y_test_smote.shape}\")\n",
    "    # print(X_train_smote.shape, y_train_smote.shape, X_test_smote.shape, y_test_smote.shape)\n",
    "\n",
    "    print (y_train_smote.value_counts())\n",
    "    print (y_test_smote.value_counts())\n",
    "    log_reg=LogisticRegression(**space)\n",
    "    print(\"Fitting model\")\n",
    "    log_reg.fit(X_train_smote,y_train_smote)\n",
    "    y_predict=log_reg.predict(X_test_smote)\n",
    "    cost = eval_metrics(y_true = y_test_smote , y_pred = y_predict)['Cost']\n",
    "    print (f\"\\nCost in fold {fold}: {cost}\")\n",
    "    trial.report(cost, fold)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "    else:\n",
    "       score.append(cost)\n",
    "  return np.mean(score)\n",
    "\n",
    "pruner=optuna.pruners.MedianPruner()\n",
    "find_param=optuna.create_study(storage='mysql://root:qwerty12345@localhost/example',\n",
    "                               load_if_exists=True,direction = \"minimize\",\n",
    "                               pruner=pruner)\n",
    "find_param.optimize(objective,n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23910.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_param.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-27 17:19:37,414] A new study created in memory with name: no-name-8d0d4a65-bfd8-4076-a102-6efd1cd92995\n",
      "[I 2024-01-27 17:19:37,431] Trial 0 finished with value: 0.0 and parameters: {'C': 0.31529490967331053, 'max_iter': 973}. Best is trial 0 with value: 0.0.\n",
      "[I 2024-01-27 17:19:37,456] Trial 1 finished with value: 0.0 and parameters: {'C': 4128.610575565414, 'max_iter': 221}. Best is trial 0 with value: 0.0.\n",
      "[I 2024-01-27 17:19:37,473] Trial 2 finished with value: 0.0 and parameters: {'C': 12.232739556326395, 'max_iter': 905}. Best is trial 0 with value: 0.0.\n",
      "[I 2024-01-27 17:19:37,504] Trial 3 finished with value: 0.0 and parameters: {'C': 466.77354273570484, 'max_iter': 788}. Best is trial 0 with value: 0.0.\n",
      "[I 2024-01-27 17:19:37,519] Trial 4 finished with value: 0.0 and parameters: {'C': 20043.405694105564, 'max_iter': 537}. Best is trial 0 with value: 0.0.\n",
      "[I 2024-01-27 17:19:37,564] Trial 5 finished with value: 0.0 and parameters: {'C': 461.2370687050697, 'max_iter': 301}. Best is trial 0 with value: 0.0.\n",
      "[I 2024-01-27 17:19:37,586] Trial 6 finished with value: 0.0 and parameters: {'C': 0.6138228601217929, 'max_iter': 664}. Best is trial 0 with value: 0.0.\n",
      "[I 2024-01-27 17:19:37,593] Trial 7 finished with value: 0.7 and parameters: {'C': 4.977965509022495e-05, 'max_iter': 696}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:37,601] Trial 8 finished with value: 0.30000000000000004 and parameters: {'C': 0.000261074357210244, 'max_iter': 795}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:37,621] Trial 9 finished with value: 0.0 and parameters: {'C': 31712.978905443106, 'max_iter': 761}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:37,653] Trial 10 finished with value: 0.7 and parameters: {'C': 5.693939893381303e-05, 'max_iter': 446}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:37,674] Trial 11 finished with value: 0.7 and parameters: {'C': 1.717659644387504e-05, 'max_iter': 427}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:37,693] Trial 12 finished with value: 0.09999999999999998 and parameters: {'C': 0.0026636304847735145, 'max_iter': 496}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:37,711] Trial 13 finished with value: 0.06666666666666665 and parameters: {'C': 0.010053508472011629, 'max_iter': 364}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:37,727] Trial 14 finished with value: 0.7 and parameters: {'C': 1.140410382764817e-05, 'max_iter': 162}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:37,742] Trial 15 finished with value: 0.09999999999999998 and parameters: {'C': 0.0004581423268884766, 'max_iter': 622}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:37,758] Trial 16 finished with value: 0.06666666666666665 and parameters: {'C': 0.039912277208780225, 'max_iter': 637}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:37,772] Trial 17 finished with value: 0.3666666666666667 and parameters: {'C': 0.00018305031022857694, 'max_iter': 449}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:37,794] Trial 18 finished with value: 0.0 and parameters: {'C': 8.606831119709684, 'max_iter': 723}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:37,815] Trial 19 finished with value: 0.033333333333333326 and parameters: {'C': 0.04209436347385321, 'max_iter': 320}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:37,828] Trial 20 finished with value: 0.7 and parameters: {'C': 6.799536975902168e-05, 'max_iter': 104}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:37,844] Trial 21 finished with value: 0.7 and parameters: {'C': 1.0330990038904165e-05, 'max_iter': 441}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:37,865] Trial 22 finished with value: 0.06666666666666665 and parameters: {'C': 0.001671307569426865, 'max_iter': 397}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:37,892] Trial 23 finished with value: 0.7 and parameters: {'C': 3.7991266372373184e-05, 'max_iter': 580}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:37,918] Trial 24 finished with value: 0.06666666666666665 and parameters: {'C': 0.0013338429166895442, 'max_iter': 536}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:37,939] Trial 25 finished with value: 0.7 and parameters: {'C': 6.622343530404542e-05, 'max_iter': 268}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:37,953] Trial 26 finished with value: 0.06666666666666665 and parameters: {'C': 0.009397856797860541, 'max_iter': 473}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:37,969] Trial 27 finished with value: 0.7 and parameters: {'C': 1.221104725858393e-05, 'max_iter': 884}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:37,992] Trial 28 finished with value: 0.19999999999999996 and parameters: {'C': 0.00041490636600097127, 'max_iter': 384}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,017] Trial 29 finished with value: 0.0 and parameters: {'C': 0.11948643996173258, 'max_iter': 700}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,059] Trial 30 finished with value: 0.0 and parameters: {'C': 4.366578850503585, 'max_iter': 592}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,076] Trial 31 finished with value: 0.7 and parameters: {'C': 1.0861780694907055e-05, 'max_iter': 166}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,092] Trial 32 finished with value: 0.3666666666666667 and parameters: {'C': 9.72753344526064e-05, 'max_iter': 241}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,108] Trial 33 finished with value: 0.7 and parameters: {'C': 3.0660825149623266e-05, 'max_iter': 161}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,133] Trial 34 finished with value: 0.23333333333333328 and parameters: {'C': 0.00039084968147661697, 'max_iter': 986}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,170] Trial 35 finished with value: 0.06666666666666665 and parameters: {'C': 0.005245164320808906, 'max_iter': 843}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,218] Trial 36 finished with value: 0.0 and parameters: {'C': 106.59602711535017, 'max_iter': 339}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,231] Trial 37 finished with value: 0.7 and parameters: {'C': 3.3464245590753724e-05, 'max_iter': 518}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,247] Trial 38 finished with value: 0.06666666666666665 and parameters: {'C': 0.0010032458231541066, 'max_iter': 175}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,270] Trial 39 finished with value: 0.3666666666666667 and parameters: {'C': 0.0001377889905058018, 'max_iter': 429}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,293] Trial 40 finished with value: 0.0 and parameters: {'C': 1.44813991413786, 'max_iter': 939}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,312] Trial 41 finished with value: 0.7 and parameters: {'C': 4.737760079970134e-05, 'max_iter': 227}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,339] Trial 42 finished with value: 0.3666666666666667 and parameters: {'C': 0.0001501741368830944, 'max_iter': 135}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,357] Trial 43 finished with value: 0.7 and parameters: {'C': 2.6271269629549187e-05, 'max_iter': 133}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,374] Trial 44 finished with value: 0.033333333333333326 and parameters: {'C': 0.0005635155088900475, 'max_iter': 287}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,395] Trial 45 finished with value: 0.3666666666666667 and parameters: {'C': 0.0001227240124868871, 'max_iter': 198}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,412] Trial 46 finished with value: 0.09999999999999998 and parameters: {'C': 0.0033831849643805017, 'max_iter': 691}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,441] Trial 47 finished with value: 0.0 and parameters: {'C': 2362.803290929571, 'max_iter': 115}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,462] Trial 48 finished with value: 0.7 and parameters: {'C': 2.1596638782033993e-05, 'max_iter': 787}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,479] Trial 49 finished with value: 0.7 and parameters: {'C': 6.418990752527314e-05, 'max_iter': 569}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,503] Trial 50 finished with value: 0.06666666666666665 and parameters: {'C': 0.02329384651179506, 'max_iter': 106}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,522] Trial 51 finished with value: 0.7 and parameters: {'C': 1.2553751019787482e-05, 'max_iter': 427}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,540] Trial 52 finished with value: 0.33333333333333337 and parameters: {'C': 0.0002483521852410493, 'max_iter': 492}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,559] Trial 53 finished with value: 0.7 and parameters: {'C': 1.4668534116954542e-05, 'max_iter': 633}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,577] Trial 54 finished with value: 0.7 and parameters: {'C': 6.128195942148502e-05, 'max_iter': 347}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,595] Trial 55 finished with value: 0.033333333333333326 and parameters: {'C': 0.0006918519485433609, 'max_iter': 467}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,619] Trial 56 finished with value: 0.0 and parameters: {'C': 77571.60539025952, 'max_iter': 389}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,638] Trial 57 finished with value: 0.7 and parameters: {'C': 2.7563070080959343e-05, 'max_iter': 310}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,655] Trial 58 finished with value: 0.7 and parameters: {'C': 1.063103789371453e-05, 'max_iter': 254}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,674] Trial 59 finished with value: 0.33333333333333337 and parameters: {'C': 0.00024887764924484923, 'max_iter': 434}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,694] Trial 60 finished with value: 0.09999999999999998 and parameters: {'C': 0.002065024861568779, 'max_iter': 524}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,712] Trial 61 finished with value: 0.7 and parameters: {'C': 6.907285364858199e-05, 'max_iter': 595}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,730] Trial 62 finished with value: 0.7 and parameters: {'C': 3.887521960947704e-05, 'max_iter': 737}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,747] Trial 63 finished with value: 0.3666666666666667 and parameters: {'C': 0.00010691195658743873, 'max_iter': 665}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,764] Trial 64 finished with value: 0.7 and parameters: {'C': 2.348233751181892e-05, 'max_iter': 544}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,784] Trial 65 finished with value: 0.33333333333333337 and parameters: {'C': 0.0002531933746783008, 'max_iter': 580}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,806] Trial 66 finished with value: 0.7 and parameters: {'C': 1.0407689541397682e-05, 'max_iter': 503}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,847] Trial 67 finished with value: 0.0 and parameters: {'C': 132.9790035832768, 'max_iter': 206}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,863] Trial 68 finished with value: 0.7 and parameters: {'C': 5.3740271413114535e-05, 'max_iter': 402}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,882] Trial 69 finished with value: 0.06666666666666665 and parameters: {'C': 0.0010629100467716046, 'max_iter': 841}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,904] Trial 70 finished with value: 0.7 and parameters: {'C': 2.315717122391573e-05, 'max_iter': 472}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,923] Trial 71 finished with value: 0.3666666666666667 and parameters: {'C': 9.076147322738953e-05, 'max_iter': 248}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,943] Trial 72 finished with value: 0.19999999999999996 and parameters: {'C': 0.00042182321792032045, 'max_iter': 361}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,962] Trial 73 finished with value: 0.7 and parameters: {'C': 4.080918451628902e-05, 'max_iter': 148}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:38,981] Trial 74 finished with value: 0.3666666666666667 and parameters: {'C': 0.00015166492815554976, 'max_iter': 284}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:39,007] Trial 75 finished with value: 0.7 and parameters: {'C': 1.8455544524183246e-05, 'max_iter': 269}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:39,026] Trial 76 finished with value: 0.7 and parameters: {'C': 7.086845626869683e-05, 'max_iter': 194}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:39,046] Trial 77 finished with value: 0.7 and parameters: {'C': 3.908202254535717e-05, 'max_iter': 616}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:39,067] Trial 78 finished with value: 0.3666666666666667 and parameters: {'C': 0.00017830279622522968, 'max_iter': 411}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:39,087] Trial 79 finished with value: 0.7 and parameters: {'C': 1.01255330799743e-05, 'max_iter': 326}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:39,112] Trial 80 finished with value: 0.0 and parameters: {'C': 0.5018490292611758, 'max_iter': 671}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:39,132] Trial 81 finished with value: 0.7 and parameters: {'C': 1.9202706945849405e-05, 'max_iter': 841}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:39,155] Trial 82 finished with value: 0.3666666666666667 and parameters: {'C': 9.464651203940469e-05, 'max_iter': 894}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:39,177] Trial 83 finished with value: 0.7 and parameters: {'C': 3.7009070405149515e-05, 'max_iter': 954}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:39,199] Trial 84 finished with value: 0.7 and parameters: {'C': 1.5384571565146368e-05, 'max_iter': 169}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:39,226] Trial 85 finished with value: 0.0 and parameters: {'C': 0.17301686551361647, 'max_iter': 102}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:39,246] Trial 86 finished with value: 0.06666666666666665 and parameters: {'C': 0.0005236575633500236, 'max_iter': 452}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:39,267] Trial 87 finished with value: 0.30000000000000004 and parameters: {'C': 0.000284892732817628, 'max_iter': 1000}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:39,288] Trial 88 finished with value: 0.7 and parameters: {'C': 5.4612437867874275e-05, 'max_iter': 129}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:39,309] Trial 89 finished with value: 0.7 and parameters: {'C': 1.8539527625770907e-05, 'max_iter': 563}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:39,329] Trial 90 finished with value: 0.3666666666666667 and parameters: {'C': 0.00011170409626887093, 'max_iter': 757}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:39,356] Trial 91 finished with value: 0.7 and parameters: {'C': 1.0808717415368903e-05, 'max_iter': 225}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:39,374] Trial 92 finished with value: 0.7 and parameters: {'C': 2.5696805345288073e-05, 'max_iter': 156}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:39,401] Trial 93 finished with value: 0.7 and parameters: {'C': 3.7074459895405115e-05, 'max_iter': 189}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:39,421] Trial 94 finished with value: 0.7 and parameters: {'C': 1.702226839284772e-05, 'max_iter': 381}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:39,462] Trial 95 finished with value: 0.5 and parameters: {'C': 7.992566339837774e-05, 'max_iter': 818}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:39,490] Trial 96 finished with value: 0.0 and parameters: {'C': 2.7345078232717985, 'max_iter': 122}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:39,512] Trial 97 finished with value: 0.3666666666666667 and parameters: {'C': 0.00020122849646080876, 'max_iter': 490}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:39,535] Trial 98 finished with value: 0.7 and parameters: {'C': 2.7861132308883335e-05, 'max_iter': 180}. Best is trial 7 with value: 0.7.\n",
      "[I 2024-01-27 17:19:39,559] Trial 99 finished with value: 0.7 and parameters: {'C': 1.0055605778060084e-05, 'max_iter': 884}. Best is trial 7 with value: 0.7.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'C': 4.977965509022495e-05, 'max_iter': 696}\n",
      "Accuracy of the best model: 0.3000\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameter search space\n",
    "    C = trial.suggest_loguniform('C', 1e-5, 1e5)\n",
    "    max_iter = trial.suggest_int('max_iter', 100, 1000)\n",
    "    \n",
    "    # Create and train Logistic Regression model\n",
    "    model = LogisticRegression(C=C, max_iter=max_iter, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy as the metric to optimize\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Report intermediate result for pruning\n",
    "    trial.report(accuracy, step=trial.number)\n",
    "    \n",
    "    # Handle pruning based on the intermediate result\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "\n",
    "    return 1.0 - accuracy  # Optuna minimizes the objective function\n",
    "\n",
    "    # Create Optuna study with MedianPruner and optimize hyperparameters\n",
    "study = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = LogisticRegression(**study.best_params, random_state=42)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "\n",
    "print(f\"Accuracy of the best model: {accuracy_best:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_C</th>\n",
       "      <th>params_max_iter</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [number, value, datetime_start, datetime_complete, duration, params_C, params_max_iter, state]\n",
       "Index: []"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.trials_dataframe()[study.trials_dataframe()['state']=='PRUNED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
