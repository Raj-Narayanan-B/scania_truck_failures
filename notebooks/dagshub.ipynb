{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Client created. Use the name of the repo <span style=\"font-weight: bold\">(</span>scania_truck_failures<span style=\"font-weight: bold\">)</span> as the name of the bucket\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Client created. Use the name of the repo \u001b[1m(\u001b[0mscania_truck_failures\u001b[1m)\u001b[0m as the name of the bucket\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-27 20:21:45,744: INFO: helpers: Client created. Use the name of the repo (scania_truck_failures) as the name of the bucket]\n",
      "[2024-02-27 20:21:45,826: INFO: utils: config.yaml yaml_file is loaded]\n",
      "[2024-02-27 20:21:45,837: INFO: utils: params.yaml yaml_file is loaded]\n",
      "[2024-02-27 20:21:45,847: INFO: utils: schema.yaml yaml_file is loaded]\n",
      "[2024-02-27 20:21:45,854: INFO: utils: mlflow_config.yaml yaml_file is loaded]\n",
      "[2024-02-27 20:21:45,884: INFO: utils: mlflow_config.yaml yaml_file is loaded]\n",
      "[2024-02-27 20:21:45,889: INFO: utils: mlflow_config.yaml yaml_file is loaded]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Client created. Use the name of the repo <span style=\"font-weight: bold\">(</span>scania_truck_failures<span style=\"font-weight: bold\">)</span> as the name of the bucket\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Client created. Use the name of the repo \u001b[1m(\u001b[0mscania_truck_failures\u001b[1m)\u001b[0m as the name of the bucket\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-27 20:21:45,903: INFO: helpers: Client created. Use the name of the repo (scania_truck_failures) as the name of the bucket]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import json\n",
    "from src.utils import load_yaml\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "from src.config.configuration_manager import ConfigurationManager\n",
    "from dagshub import get_repo_bucket_client\n",
    "s3 = get_repo_bucket_client(\"Raj-Narayanan-B/scania_truck_failures\")\n",
    "import os\n",
    "os.chdir(r\"f:\\\\iNeuron\\\\Projects\\\\Scania_Truck_Failures\")\n",
    "import shutil\n",
    "import glob\n",
    "from src.components.stage_0_data_DB_upload import file_tracker_component\n",
    "file_tracker_obj = file_tracker_component()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_tracker_obj.file_lineage_tracker(#add_list = ['test_file10.csv'],\n",
    "                                       update_list = [#'aps_failure_test_set_testing3.csv',#'aps_failure_test_set_testing3.csv',\n",
    "                                                       #'aps_failure_test_set_testing2.csv','aps_failure_test_set_testing4.csv',\n",
    "                                                       'test_file2.csv','test_file4.csv','test_file6.csv','test_file8.csv'],\n",
    "                                      reverse_update_list = [#'test_file2.csv','test_file4.csv', 'test_file6.csv','test_file8.csv', 'test_file10.csv',\n",
    "                                                              'aps_failure_test_set_testing1.csv','aps_failure_test_set_testing2.csv',\n",
    "                                                              'aps_failure_test_set_testing3.csv','aps_failure_test_set_testing4.csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint({'S3_file_10': 'aps_failure_test_set_testing8.csv', 'S3_file_11': 'aps_failure_test_set_testing9.csv', 'S3_file_3': 'aps_failure_test_set_testing10.csv', 'S3_file_4': 'aps_failure_test_set_testing2.csv', 'S3_file_5': 'aps_failure_test_set_testing3.csv', 'S3_file_6': 'aps_failure_test_set_testing4.csv', 'S3_file_7': 'aps_failure_test_set_testing5.csv', 'S3_file_8': 'aps_failure_test_set_testing6.csv', 'S3_file_9': 'aps_failure_test_set_testing7.csv'},compact=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['1'] not in ['test'] + ['aps_failure_test_set_testing2.csv', \n",
    "            'aps_failure_test_set_testing4.csv',\n",
    "            'aps_failure_test_set_testing5.csv',\n",
    "            'aps_failure_test_set_testing6.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = ['test', 'aps_failure_test_set_testing2.csv', 'aps_failure_test_set_testing4.csv', 'aps_failure_test_set_testing5.csv', 'aps_failure_test_set_testing6.csv']\n",
    "list2 = ['aps_failure_test_set_testing4.csv', 'aps_failure_test_set_testing5.csv', 'aps_failure_test_set_testing6.csv']\n",
    "\n",
    "result = list(set(list1) - set(list2))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(list1) - set(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file(\n",
    "    Bucket=\"scania_truck_failures\",  # name of the repo\n",
    "    Filename=r\"F:\\iNeuron\\Projects\\Data\\Scania Truck Failures\\aps_failure_test_set.csv\",  # local path of file to upload\n",
    "    Key=\"remote_aps_failure_testing_set.csv\",  # remote path where to upload the file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file(\n",
    "    Bucket=\"scania_truck_failures\",  # name of the repo\n",
    "    Filename=r\"F:\\iNeuron\\Projects\\Data\\Scania Truck Failures\\aps_failure_training_set.csv\",  # local path of file to upload\n",
    "    Key=\"remote_aps_failure_training_set.csv\",  # remote path where to upload the file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4,5):\n",
    "    s3.upload_file(\n",
    "        Bucket=\"scania_truck_failures\",  # name of the repo\n",
    "        Filename=f\"F:\\iNeuron\\Projects\\Data\\Scania Truck Failures\\\\aps_failure_test_set_testing{i}.csv\",  # local path of file to upload\n",
    "        Key=f\"aps_failure_test_set_testing{i}.csv\",  # remote path where to upload the file\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3,6):\n",
    "    s3.delete_object(Bucket = \"scania_truck_failures\",\n",
    "                    Key=f\"aps_failure_test_set_testing{i}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file(\n",
    "    Bucket=\"scania_truck\",  # name of the repo\n",
    "    Filename=r\"F:\\iNeuron\\Projects\\Data\\Scania Truck Failures\\aps_failure_training_set.csv\",  # local path of file to upload\n",
    "    Key=\"remote_aps_failure_training_set.csv\",  # remote path where to upload the file\n",
    ")\n",
    "\n",
    "s3.upload_file(\n",
    "    Bucket=\"scania_truck\",  # name of the repo\n",
    "    Filename=r\"F:\\iNeuron\\Projects\\Data\\Scania Truck Failures\\aps_failure_test_set.csv\",  # local path of file to upload\n",
    "    Key=\"remote_aps_failure_testing_set.csv\",  # remote path where to upload the file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.download_file(\n",
    "    Bucket=\"scania_truck\",  # name of the repo\n",
    "    Key=\"remote_aps_failure_training_set.csv\",  #  remote path from where to download the file\n",
    "    Filename=r\"F:\\iNeuron\\Projects\\scania_failures_2\\notebooks\\sample_train.csv\",  # local path where to download the file\n",
    ")\n",
    "\n",
    "s3.download_file(\n",
    "    Bucket=\"scania_truck\",  # name of the repo\n",
    "    Key=\"remote_aps_failure_testing_set.csv\",  #  remote path from where to download the file\n",
    "    Filename=r\"F:\\iNeuron\\Projects\\scania_failures_2\\notebooks\\sample_test.csv\",  # local path where to download the file\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dagshub import get_repo_bucket_client\n",
    "s3 = get_repo_bucket_client(\"Raj-Narayanan-B/scania_truck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_files = load_yaml(r\"artifacts\\data\\file_lineage.yaml\")\n",
    "pprint(dict(old_files),compact = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_s3 = s3.list_objects_v2(Bucket='scania_truck')['Contents']\n",
    "files_in_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_from_s3_dict = {}\n",
    "for i in range(len(files_in_s3)):\n",
    "    files_from_s3_dict[f\"file_{i+1}\"] = files_in_s3[i]['Key']\n",
    "files_from_s3_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_from_s3_list = list(files_from_s3_dict.values())\n",
    "files_from_s3_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_files_for_model_training = old_files['files_for_model_training']\n",
    "old_files_predicted = old_files['files_predicted']\n",
    "old_files_to_predict = old_files['files_to_predict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_files_for_model_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import s3_counter\n",
    "\n",
    "\n",
    "for key,value in old_files_for_model_training.items():\n",
    "    if value in files_from_s3_list:\n",
    "        files_from_s3_list.remove(value)\n",
    "    else:\n",
    "        old_files_for_model_training.pop(key)\n",
    "        raise ValueError(f\"{value} is missing from S3\")\n",
    "\n",
    "for key,value in old_files_predicted.items():\n",
    "    if value in files_from_s3_list:\n",
    "        files_from_s3_list.remove(value)\n",
    "    else:\n",
    "        old_files_predicted.pop(key)\n",
    "\n",
    "for key,value in old_files_to_predict.items():\n",
    "    if value in files_from_s3_list:\n",
    "        files_from_s3_list.remove(value)\n",
    "    else:\n",
    "        old_files_to_predict.pop(key)\n",
    "\n",
    "if len(files_from_s3_list) > 0:\n",
    "    s3_count = s3_counter(old_files_to_predict)\n",
    "    for i in files_from_s3_list:\n",
    "        old_files_to_predict[f'S3_file_{s3_count+1}'] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_from_s3_list.remove('remote_aps_failure_testing_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_from_s3_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_s3 = files_in_s3_.copy()\n",
    "# files_in_s3.append({'Key':\"test_1\"})\n",
    "# files_in_s3.append({\"Key\":\"test_2\"})\n",
    "# files_in_s3.append({\"Key\":\"test_3\"})\n",
    "# files_in_s3.append({\"Key\":\"test_4\"})\n",
    "# files_in_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_from_s3_dict = {}\n",
    "for i in range(len(files_in_s3)):\n",
    "    files_from_s3_dict[f\"file_{i+1}\"] = files_in_s3[i]['Key']\n",
    "files_from_s3_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in files_from_s3_dict.items():\n",
    "    try:\n",
    "        if files_from_s3_dict[key] == dict(config)[key]:\n",
    "            pass\n",
    "    except:\n",
    "        print(f\"{key}: {files_from_s3_dict[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_files = load_yaml(r'artifacts\\data\\file_lineage.yaml')\n",
    "files_from_s3_dict = {}\n",
    "files_from_s3_dict['files_for_model_training'] = {}  # old_files['files_for_model_training']\n",
    "files_from_s3_dict['files_to_predict'] = {}  # old_files['files_to_predict']\n",
    "files_from_s3_dict['files_predicted'] = {}  # old_files['files_predicted']\n",
    "predicted_files_counter = 1\n",
    "files_to_predict_counter = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_files.files_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_files_predicted = {}\n",
    "for key,value in old_files.files_predicted.items():\n",
    "    old_files_predicted[key] = old_files.files_predicted[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_files_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_files_to_predict = {}\n",
    "for key,value in old_files.files_to_predict.items():\n",
    "    old_files_to_predict[key] = old_files.files_to_predict[key]\n",
    "old_files_to_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_old_files = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for main_key in ['files_to_predict','files_predicted']:\n",
    "    for key,value in old_files[main_key].items():\n",
    "        all_old_files[key] = old_files[main_key][key]\n",
    "all_old_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Batch_file_3'.startswith('Batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_s3 = s3.list_objects_v2(Bucket='scania_truck')['Contents']\n",
    "files_in_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_ = {}\n",
    "# files_['S3_file_'] = {}\n",
    "for i in range(len(files_in_s3)):\n",
    "    files_[f'S3_file_{i+1}'] = files_in_s3[i]['Key']\n",
    "files_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previously_added_files = dict(set(all_old_files.items()) - set(files_.items()))\n",
    "previously_added_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_old_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_file_counter = 0\n",
    "for key in all_old_files.keys():\n",
    "    if key.startswith('Batch'):\n",
    "        batch_file_counter+=1\n",
    "batch_file_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_predicted_files = ['test_4','test_5','test_6']\n",
    "for i, file_name in enumerate(add_predicted_files):\n",
    "    if file_name in list(old_files_to_predict.values()) or file_name in list(all_old_files.values()):\n",
    "        raise ValueError(f\"Duplicate file entry: {file_name}\")\n",
    "    else:\n",
    "        old_files_to_predict[f'Batch_file_{batch_file_counter+(i+1)}'] = file_name\n",
    "        files_[f'Batch_file_{batch_file_counter+(i+1)}'] = file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_files_to_predict.pop('Batch_file_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_files_to_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_tracker_obj.file_lineage_tracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_files = load_yaml(r'artifacts\\data\\file_lineage.yaml')\n",
    "old_files_for_model_training = old_files['files_for_model_training']\n",
    "old_files_predicted = old_files['files_predicted']\n",
    "old_files_to_predict = old_files['files_to_predict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_counter(dict_:dict):\n",
    "    counter = 0\n",
    "    for key in dict_.keys():\n",
    "        if key.startswith('Batch'):\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "def s3_counter(dict_:dict):\n",
    "    counter = 0\n",
    "    for key in dict_.keys():\n",
    "        if key.startswith('S3'):\n",
    "            counter += 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_counter(old_files_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_counter(old_files_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_counter(old_files_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch_file_1: test_1.csv\n",
    "# Batch_file_2: test_2.csv\n",
    "# Batch_file_3: test_3.csv\n",
    "# Batch_file_4: test_7\n",
    "# Batch_file_5: test_4.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import save_yaml\n",
    "add_predicted_files = ['test_7']\n",
    "batch_count = batch_counter(old_files_predicted)\n",
    "for i,file in enumerate(add_predicted_files,start=1):\n",
    "    if file in list(old_files_predicted.values()):\n",
    "        raise ValueError(f\"Duplicate file entry: {file}\")\n",
    "    else:\n",
    "        old_files_predicted[f\"Batch_file_{batch_count+(i)}\"] = file\n",
    "\n",
    "files_from_s3_dict = {}\n",
    "files_from_s3_dict['files_for_model_training'] = old_files_for_model_training\n",
    "files_from_s3_dict['files_to_predict'] = old_files_to_predict\n",
    "files_from_s3_dict['files_predicted'] = old_files_predicted\n",
    "save_yaml(files_from_s3_dict,\n",
    "          filepath=r'F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\file_lineage.yaml',\n",
    "          mode = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_predicted_files = ['test_4.csv','aps_failure_test_set_testing.csv','aps_failure_test_set_testing1.csv','aps_failure_test_set_testing3.csv']\n",
    "update_predicted_files = ['aps_failure_test_set_testing8.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import save_yaml\n",
    "key_list = []\n",
    "for i in update_predicted_files:\n",
    "    for key,value in old_files_to_predict.items():\n",
    "        if i == value:\n",
    "            if key.startswith(\"S3\"):\n",
    "                key_list.append(key)\n",
    "                s3_count = s3_counter(old_files_predicted)\n",
    "                old_files_predicted[f\"S3_file_{s3_count+1}\"] = i\n",
    "            elif key.startswith('Batch'):\n",
    "                key_list.append(key)\n",
    "                batch_count = batch_counter(old_files_predicted)\n",
    "                old_files_predicted[f\"Batch_file_{batch_count+1}\"] = i\n",
    "for i in key_list:\n",
    "    old_files_to_predict.pop(i)\n",
    "\n",
    "s3_count = s3_counter(old_files_to_predict)\n",
    "batch_count = batch_counter(old_files_to_predict)\n",
    "remaining_s3_files_to_predict = []\n",
    "remaining_batch_files_to_predict = []\n",
    "for key in old_files_to_predict.keys():\n",
    "    if key.startswith(\"S3\"):\n",
    "        remaining_s3_files_to_predict.append(old_files_to_predict[key])\n",
    "    elif key.startswith(\"Batch\"):\n",
    "        remaining_batch_files_to_predict.append(old_files_to_predict[key])\n",
    "old_files_to_predict = {}\n",
    "try:\n",
    "    for i in range(s3_count):\n",
    "        old_files_to_predict[f\"S3_file_{i+1}\"] = remaining_s3_files_to_predict[i]\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    for i in range(batch_count):\n",
    "        old_files_to_predict[f\"Batch_file{i+1}\"] = remaining_batch_files_to_predict[i]\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "files_from_s3_dict = {}\n",
    "files_from_s3_dict['files_for_model_training'] = old_files_for_model_training\n",
    "files_from_s3_dict['files_to_predict'] = old_files_to_predict\n",
    "files_from_s3_dict['files_predicted'] = old_files_predicted\n",
    "save_yaml(files_from_s3_dict,\n",
    "          filepath=r'F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\file_lineage.yaml',\n",
    "          mode = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_update_predicted_files = ['test_7','aps_failure_test_set_testing.csv','aps_failure_test_set_testing1.csv','aps_failure_test_set_testing3.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import save_yaml\n",
    "\n",
    "key_list = []\n",
    "for i in reverse_update_predicted_files:\n",
    "    for key,value in old_files_predicted.items():\n",
    "        if i == value:\n",
    "            if key.startswith(\"S3\"):\n",
    "                key_list.append(key)\n",
    "                s3_count = s3_counter(old_files_to_predict)\n",
    "                old_files_to_predict[f\"S3_file_{s3_count+1}\"] = i\n",
    "            elif key.startswith('Batch'):\n",
    "                key_list.append(key)\n",
    "                batch_count = batch_counter(old_files_to_predict)\n",
    "                old_files_to_predict[f\"Batch_file_{batch_count+1}\"] = i\n",
    "for i in key_list:\n",
    "    old_files_predicted.pop(i)\n",
    "\n",
    "s3_count = s3_counter(old_files_predicted)\n",
    "batch_count = batch_counter(old_files_predicted)\n",
    "remaining_s3_files = []\n",
    "remaining_batch_files = []\n",
    "for key in old_files_predicted.keys():\n",
    "    if key.startswith(\"S3\"):\n",
    "        remaining_s3_files.append(old_files_predicted[key])\n",
    "    elif key.startswith(\"Batch\"):\n",
    "        remaining_batch_files.append(old_files_predicted[key])\n",
    "old_files_predicted = {}\n",
    "try:\n",
    "    for i in range(s3_count):\n",
    "        old_files_predicted[f\"S3_file_{i+1}\"] = remaining_s3_files[i]\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    for i in range(batch_count):\n",
    "        old_files_predicted[f\"Batch_file{i+1}\"] = remaining_batch_files[i]\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "files_from_s3_dict = {}\n",
    "files_from_s3_dict['files_for_model_training'] = old_files_for_model_training\n",
    "files_from_s3_dict['files_to_predict'] = old_files_to_predict\n",
    "files_from_s3_dict['files_predicted'] = old_files_predicted\n",
    "save_yaml(files_from_s3_dict,\n",
    "          filepath=r'F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\file_lineage.yaml',\n",
    "          mode = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(old_files_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(old_files_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_s3 = s3.list_objects_v2(Bucket='scania_truck')['Contents']\n",
    "old_files = load_yaml(r'F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\file_lineage.yaml')\n",
    "\n",
    "files_from_s3_dict = {}\n",
    "for i in range(len(files_in_s3)):\n",
    "    files_from_s3_dict[f\"file_{i+1}\"] = files_in_s3[i]['Key']\n",
    "files_from_s3_list = list(files_from_s3_dict.values())\n",
    "\n",
    "old_files_for_model_training = old_files['files_for_model_training']\n",
    "pprint(old_files_for_model_training,compact=True)\n",
    "print('\\n')\n",
    "\n",
    "old_files_predicted = old_files['files_predicted']\n",
    "pprint(old_files_predicted,compact=True)\n",
    "print('\\n')\n",
    "\n",
    "old_files_to_predict = old_files['files_to_predict']\n",
    "pprint(old_files_to_predict,compact=True)\n",
    "print('\\n')\n",
    "\n",
    "for key, value in old_files_for_model_training.items():\n",
    "    if value in files_from_s3_list:\n",
    "        files_from_s3_list.remove(value)\n",
    "        print(f'removed_value: {value}')\n",
    "    else:\n",
    "        old_files_for_model_training.pop(key)\n",
    "        raise ValueError(f\"{value} is missing from S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_from_s3_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_from_s3_list_=files_from_s3_list.copy()\n",
    "for file in files_from_s3_list_:\n",
    "    print(file)\n",
    "    if file in list(old_files_predicted.values()):\n",
    "        files_from_s3_list.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_from_s3_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(old_files_to_predict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_from_s3_list_=files_from_s3_list.copy()\n",
    "for file in files_from_s3_list_:\n",
    "    if file in list(old_files_to_predict.values()):\n",
    "        files_from_s3_list.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_from_s3_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_from_s3_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.constants import TEST_DATA,TRAIN_DATA,BUCKET\n",
    "from src.utils import load_yaml,save_yaml\n",
    "files_in_s3 = s3.list_objects_v2(Bucket=BUCKET)['Contents']\n",
    "files_in_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be inserted in the \"else\" block of stage_0_data_DB_upload.py file's \"s3_data_download\" method\n",
    "\n",
    "with open(r'artifacts\\data\\data_loaded_from_s3_so_far.yaml', 'w'):\n",
    "    files_from_s3_dict = {}\n",
    "    files_from_s3_dict['files_for_model_training'] = {}\n",
    "    files_from_s3_dict['files_to_predict'] = {}\n",
    "    files_from_s3_dict['files_predicted'] = {}\n",
    "    file_counter = 1\n",
    "    for i in range(len(files_in_s3)):\n",
    "        if files_in_s3[i]['Key'] == TEST_DATA or files_in_s3[i]['Key'] == TRAIN_DATA:\n",
    "            file_name = \"training_set\" if files_in_s3[i]['Key'] == TRAIN_DATA else \"testing_set\"\n",
    "            files_from_s3_dict['files_for_model_training'][file_name] = files_in_s3[i]['Key']\n",
    "            # s3.download_file(\n",
    "            #     Bucket=BUCKET,\n",
    "            #     Key=files_from_s3_dict['files_for_model_training'][file_name],\n",
    "            #     Filename= f\"artifacts/data/temp\" + f\"/{files_from_s3_dict['files_for_model_training'][file_name]}\"\n",
    "            # )\n",
    "        else:\n",
    "            file_name = f\"file_{file_counter}\"\n",
    "            files_from_s3_dict['files_to_predict'][file_name] = files_in_s3[i]['Key']\n",
    "            # s3.download_file(\n",
    "            #     Bucket=BUCKET,\n",
    "            #     Key=files_from_s3_dict['files_to_predict'][file_name],\n",
    "            #     Filename= \"artifacts/data/temp\" + f\"/{files_from_s3_dict['files_to_predict'][file_name]}\"\n",
    "            # )\n",
    "            file_counter += 1\n",
    "    save_yaml(file=files_from_s3_dict,\n",
    "                filepath=r'artifacts\\data\\data_loaded_from_s3_so_far.yaml',\n",
    "                mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be included at the last of prediction pipeline if user chooses bulk/batch prediction\n",
    "\n",
    "predicted_files = ['aps_failure_test_set_testing2.csv', 'aps_failure_test_set_testing4.csv']#'aps_failure_test_set_testing2.csv',\n",
    "                #    'aps_failure_test_set_testing3.csv']\n",
    "\n",
    "files_from_s3_dict = {}\n",
    "files_from_s3_dict['files_for_model_training'] = {}  # old_files['files_for_model_training']\n",
    "files_from_s3_dict['files_to_predict'] = {}  # old_files['files_to_predict']\n",
    "files_from_s3_dict['files_predicted'] = {}  # old_files['files_predicted']\n",
    "predicted_files_counter = 1\n",
    "files_to_predict_counter = 1\n",
    "for i in range(len(files_in_s3)):\n",
    "    if files_in_s3[i]['Key'] == TEST_DATA or files_in_s3[i]['Key'] == TRAIN_DATA:\n",
    "        file_name = \"training_set\" if files_in_s3[i]['Key'] == TRAIN_DATA else \"testing_set\"\n",
    "        files_from_s3_dict['files_for_model_training'][file_name] = files_in_s3[i]['Key']\n",
    "\n",
    "    elif files_in_s3[i]['Key'] in predicted_files:\n",
    "        file_name = f\"file_{predicted_files_counter}\"\n",
    "        files_from_s3_dict['files_predicted'][file_name] = files_in_s3[i]['Key']\n",
    "        predicted_files_counter += 1\n",
    "\n",
    "    else:\n",
    "        file_name = f\"file_{files_to_predict_counter}\"\n",
    "        files_from_s3_dict['files_to_predict'][file_name] = files_in_s3[i]['Key']\n",
    "        files_to_predict_counter += 1\n",
    "save_yaml(file=files_from_s3_dict,\n",
    "            filepath=r'artifacts\\data\\data_loaded_from_s3_so_far.yaml',\n",
    "            mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(old_files['files_predicted'].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_files = ['aps_failure_test_set_testing3.csv', 'aps_failure_test_set_testing5.csv']\n",
    "# predicted_files = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_files+list(old_files['files_predicted'].values()) if predicted_files else list(old_files['files_predicted'].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be inserted in the \"if\" block of stage_0_data_DB_upload.py file's \"s3_data_download\" method\n",
    "\n",
    "old_files = load_yaml(r'artifacts\\data\\data_loaded_from_s3_so_far.yaml')\n",
    "files_from_s3_dict = {}\n",
    "files_from_s3_dict['files_for_model_training'] = {}  # old_files['files_for_model_training']\n",
    "files_from_s3_dict['files_to_predict'] = {}  # old_files['files_to_predict']\n",
    "files_from_s3_dict['files_predicted'] = {}  # old_files['files_predicted']\n",
    "predicted_files_counter = 1\n",
    "files_to_predict_counter = 1\n",
    "for i in range(len(files_in_s3)):\n",
    "    if files_in_s3[i]['Key'] == TEST_DATA or files_in_s3[i]['Key'] == TRAIN_DATA:\n",
    "        file_name = \"training_set\" if files_in_s3[i]['Key'] == TRAIN_DATA else \"testing_set\"\n",
    "        files_from_s3_dict['files_for_model_training'][file_name] = files_in_s3[i]['Key']\n",
    "\n",
    "    elif files_in_s3[i]['Key'] in list(old_files['files_predicted'].values()):\n",
    "        file_name = f\"file_{predicted_files_counter}\"\n",
    "        files_from_s3_dict['files_predicted'][file_name] = files_in_s3[i]['Key']\n",
    "        predicted_files_counter += 1\n",
    "\n",
    "    else:\n",
    "        file_name = f\"file_{files_to_predict_counter}\"\n",
    "        files_from_s3_dict['files_to_predict'][file_name] = files_in_s3[i]['Key']\n",
    "        files_to_predict_counter += 1\n",
    "save_yaml(file=files_from_s3_dict,\n",
    "            filepath=r'artifacts\\data\\data_loaded_from_s3_so_far.yaml',\n",
    "            mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_files = load_yaml(r\"artifacts\\data\\file_lineage.yaml\")\n",
    "dict(old_files['files_predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_files['files_predicted'] = {'S3_file_11': 'aps_failure_test_set_testing11.csv',\n",
    "                                'S3_file_12': 'aps_failure_test_set_testing12.csv'}\n",
    "old_files['files_predicted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(old_files['files_predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(old_files['files_to_predict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(old_files['files_to_predict']) + dict(old_files['files_predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(files_from_s3_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_s3_new =  files_in_s3.copy()\n",
    "files_in_s3_new.append({\"Key\":\"test_5\"})\n",
    "files_in_s3_new.append({\"Key\":\"test_6\"})\n",
    "files_in_s3_new.append({\"Key\":\"test_7\"})\n",
    "files_in_s3_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_s3_new_list = [files_in_s3_new[i]['Key'] for i in range(len(files_in_s3_new))]\n",
    "files_in_s3_new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_s3_old_list = [files_in_s3[i]['Key'] for i in range(len(files_in_s3))]\n",
    "files_in_s3_old_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncommon_elements = set(files_in_s3_new_list) ^ set(files_in_s3_old_list)\n",
    "list(uncommon_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_files = load_yaml(r'artifacts\\data\\data_loaded_from_s3_so_far.yaml')\n",
    "old_files\n",
    "old_keys = {item['Key'] for item in dict(old_files)}\n",
    "old_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(old_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_keys = {item['Key'] for item in files_in_s3}\n",
    "new_keys = {item['Key'] for item in files_in_s3_new}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newly_added_keys  = new_keys-old_keys\n",
    "newly_added_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newly_added_items = [item for item in files_in_s3_new if item['Key'] in newly_added_keys]\n",
    "newly_added_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_s3[1]['Key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_from_s3_dict = {}\n",
    "for i in range(len(files_in_s3)):\n",
    "    if files_in_s3[i]['Key'] == 'remote_aps_failure_testing_set.csv' or files_in_s3[i]['Key'] == 'remote_aps_failure_training_set.csv':\n",
    "        pass\n",
    "    else:\n",
    "        files_from_s3_dict[f\"file_{i+1}\"] = files_in_s3[i]['Key']\n",
    "files_from_s3_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length  = len(load_yaml(r'config/config.yaml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_from_s3_dict['file_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(files_from_s3_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath,file = os.path.split(\"artifacts/data/temp/test_data_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'F:\\iNeuron\\Projects\\scania_failures_2\\notebooks\\sample_train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_1 = df.iloc[:,:74]\n",
    "train_df_1['ident_id'] = range(1,60001)\n",
    "train_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2 = df.iloc[:,74:148]\n",
    "train_df_2['ident_id'] = range(1,60001)\n",
    "train_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_3 = df.iloc[:,148:]\n",
    "train_df_3['ident_id'] = range(1,60001)\n",
    "train_df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(r'F:\\iNeuron\\Projects\\scania_failures_2\\notebooks\\sample_test.csv')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_1 = df_test.iloc[:,:74]\n",
    "test_df_1['ident_id'] = range(1,16001)\n",
    "test_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_2 = df_test.iloc[:,74:148]\n",
    "test_df_2['ident_id'] = range(1,16001)\n",
    "test_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_3 = df_test.iloc[:,148:]\n",
    "test_df_3['ident_id'] = range(1,16001)\n",
    "test_df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'F:\\iNeuron\\Projects\\scania_failures_2\\Secrets\\Tokens\\sample-token.json') as f:\n",
    "        secrets = json.load(f)\n",
    "        # logger.info(f\"{config[1]} json file is loaded\")\n",
    "\n",
    "cloud_config = {'secure_connect_bundle': r'F:\\iNeuron\\Projects\\scania_failures_2\\Secrets\\Bundles\\secure-connect-sample.zip',\n",
    "                'connect_timeout': None}\n",
    "# print(f\"cloud_config: {cloud_config}\")\n",
    "CLIENT_ID = secrets[\"clientId\"]\n",
    "CLIENT_SECRET = secrets[\"secret\"]\n",
    "auth_provider = PlainTextAuthProvider(CLIENT_ID, CLIENT_SECRET)\n",
    "# profile = ExecutionProfile(request_timeout=None)\n",
    "cluster = Cluster(cloud=cloud_config,\n",
    "                    auth_provider=auth_provider,\n",
    "                    # execution_profiles={EXEC_PROFILE_DEFAULT: profile},\n",
    "                    protocol_version=4)\n",
    "\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = train_df_1\n",
    "# Define keyspace name and table name\n",
    "keyspace = 'sample_keyspace'\n",
    "table_name = 'uploaded_data'\n",
    "colums_types_dict_from_data_df = dict(zip(list(data_df.columns),list(data_df.dtypes.replace({'object':'text','int64':'int'}).values)))\n",
    "# columns = ', '.join(train_df_1.columns)\n",
    "columns = ', '.join([f\"{key} {value}\" for key,value in colums_types_dict_from_data_df.items()])\n",
    "create_table_statement = f\"CREATE TABLE IF NOT EXISTS {keyspace}.{table_name} ({columns}, PRIMARY KEY (ident_id));\"\n",
    "# create_table_statement = f\"CREATE TABLE IF NOT EXISTS {keyspace}.{table_name} ({columns}, PRIMARY KEY (your_primary_key_column));\"\n",
    "session.execute(create_table_statement)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsbulk_command = [\n",
    "    \"dsbulk\",\n",
    "    \"load\",\n",
    "    \"-url\", os.path.join(\"F:/iNeuron/Projects/scania_failures_2/artifacts/data/temp/\",\"train_data1.csv\"),\n",
    "    \"-k\", \"sample_keyspace\",\n",
    "    \"-t\", \"uploaded_data\",\n",
    "    \"-b\", os.path.join(\"F:/iNeuron/Projects/scania_failures_2/Secrets/Bundles/\",\"secure-connect-sample.zip\"),\n",
    "    \"-u\", \"DfYsAxTkqZZWHwNguQHPBJMt\", \n",
    "    \"-p\", \"gl5rR3g-,_vTiPLZ.Zn7pouaWdAbZQ3elybJmlnaa,D+Zr4TLx4hpYNZrQwFHTARO6GXyeByF3BQS87plEhoSDPN50e,mZgbjqUm2IUTg5p3SojZF7r-GvA0gyNpbIdG\",\n",
    "]\n",
    "result = subprocess.run(dsbulk_command, capture_output=True, text=True,shell=True)\n",
    "\n",
    "print(\"Return code:\", result.returncode)\n",
    "print(\"Standard output:\", result.stdout)\n",
    "print(\"Standard error:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_1 = pd.read_csv(r\"F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\raw\\train_data_1.csv\")\n",
    "train_df_1_ = pd.read_csv(r\"F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\temp\\train_data_1.csv\")\n",
    "\n",
    "train_df_2 = pd.read_csv(r\"F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\raw\\train_data_2.csv\")\n",
    "train_df_2_ = pd.read_csv(r\"F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\temp\\train_data_2.csv\")\n",
    "\n",
    "train_df_3 = pd.read_csv(r\"F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\raw\\train_data_3.csv\")\n",
    "train_df_3_ = pd.read_csv(r\"F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\temp\\train_data_3.csv\")\n",
    "\n",
    "##################################################################################################################\n",
    "\n",
    "test_df_1 = pd.read_csv(r\"F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\raw\\test_data_1.csv\")\n",
    "test_df_1_ = pd.read_csv(r\"F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\temp\\test_data_1.csv\")\n",
    "\n",
    "test_df_2 = pd.read_csv(r\"F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\raw\\test_data_2.csv\")\n",
    "test_df_2_ = pd.read_csv(r\"F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\temp\\test_data_2.csv\")\n",
    "\n",
    "test_df_3 = pd.read_csv(r\"F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\raw\\test_data_3.csv\")\n",
    "test_df_3_ = pd.read_csv(r\"F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\temp\\test_data_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checker_2(df_1:pd.DataFrame,df_2_:pd.DataFrame):\n",
    "    df_1.sort_values(by = 'ident_id',inplace = True)\n",
    "    df_1.reset_index(drop=True,inplace=True)\n",
    "    df_1.drop(columns=['ident_id'],inplace=True)\n",
    "    df_2_.drop(columns=['ident_id'],inplace=True)\n",
    "    try:\n",
    "        df_1.drop(columns=['field_74_'],inplace=True)\n",
    "        df_2_.drop(columns=['class'],inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    return (df_2_.compare(df_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checker_2(df_1 = train_df_1, df_2_=train_df_1_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2.compare(train_df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2_.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df:pd.DataFrame):\n",
    "    df.replace('na', np.nan, inplace=True)\n",
    "    df.drop(columns = ['ident_id'], inplace=True)     \n",
    "    df.rename(columns={'field_74_': 'class'}, inplace=True)  \n",
    "    for i in df.columns:\n",
    "        if i != 'class':\n",
    "            df[i] = df[i].astype('float')\n",
    "        else:\n",
    "            df['class'] = df['class'].map({'neg':0,'pos':1})\n",
    "            df.drop(columns = ['class'], inplace=True)   \n",
    "    return(df)\n",
    "\n",
    "def describe_check(df_1:pd.DataFrame, df_2:pd.DataFrame):\n",
    "    checks = {}\n",
    "    stats_cols = ['count','mean','std','min','25%','50%','75%','max']\n",
    "    for i in stats_cols:\n",
    "        checks[i] = all(df_1.describe().T[i] == df_2.describe().T[i])\n",
    "    return checks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_1_transformed = process(train_df_1)\n",
    "train_df_1__transformed = process(train_df_1_)\n",
    "\n",
    "describe_check(train_df_1_transformed,train_df_1__transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_1_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_1__transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2_transformed = process(train_df_2)\n",
    "train_df_2__transformed = process(train_df_2_)\n",
    "\n",
    "describe_check(train_df_2_transformed,train_df_2__transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_3_transformed = process(train_df_3)\n",
    "train_df_3__transformed = process(train_df_3_)\n",
    "\n",
    "describe_check(train_df_3_transformed,train_df_3__transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_1_transformed = process(test_df_1)\n",
    "test_df_1__transformed = process(test_df_1_)\n",
    "\n",
    "describe_check(test_df_1_transformed,test_df_1__transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_2_transformed = process(test_df_2)\n",
    "test_df_2__transformed = process(test_df_2_)\n",
    "\n",
    "describe_check(test_df_2_transformed,test_df_2__transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_3_transformed = process(test_df_3)\n",
    "test_df_3__transformed = process(test_df_3_)\n",
    "\n",
    "describe_check(test_df_3_transformed,test_df_3__transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checker = train_df_1_transformed.describe().T['std'] != train_df_1__transformed.describe().T['std']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_1_transformed.describe().T['std'][checker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_1__transformed.describe().T['std'][checker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.097926e+02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.097926e+02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checker_mean = train_df_2_transformed.describe().T['mean'] != train_df_2__transformed.describe().T['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2_transformed.describe().T['mean'][checker_mean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2__transformed.describe().T['mean'][checker_mean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7.143427e+05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7.143427e+05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checker_std = train_df_2_transformed.describe().T['std'] != train_df_2__transformed.describe().T['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2_transformed.describe().T['std'][checker_std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2__transformed.describe().T['std'][checker_std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8.355997e+06\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8.355997e+06\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_checker_mean = test_df_2_transformed.describe().T['mean'] != test_df_2__transformed.describe().T['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_2_transformed.describe().T['mean'][test_checker_mean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_2__transformed.describe().T['mean'][test_checker_mean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3.656347e+06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3.656347e+06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_checker_std = test_df_2_transformed.describe().T['std'] != test_df_2__transformed.describe().T['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_2_transformed.describe().T['std'][test_checker_std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_2__transformed.describe().T['std'][test_checker_std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8.775294e+06\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8.775294e+06\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"logs\"\n",
    "pattern = os.path.join(directory_path, \"LOAD*\")\n",
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use glob to get a list of file paths matching the pattern\n",
    "files_to_remove = glob.glob(pattern)\n",
    "files_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the files and remove them\n",
    "for file_path in files_to_remove:\n",
    "    shutil.rmtree(file_path)\n",
    "    print(f\"Removed: {file_path}\")\n",
    "\n",
    "print(\"Files removed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\temp\\remote_aps_failure_testing_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GitPython import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-27 13:57:19,878: INFO: utils: pickled_object: F:\\iNeuron\\Projects\\Scania_Truck_Failures\\artifacts\\preprocessor\\preprocessor.joblib loaded]\n"
     ]
    }
   ],
   "source": [
    "from src.utils import load_binary, data_validation,load_yaml\n",
    "from src.constants import SCHEMA_PATH\n",
    "preprocessor = load_binary(r\"F:\\iNeuron\\Projects\\Scania_Truck_Failures\\artifacts\\preprocessor\\preprocessor.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;Knn_imputer&#x27;, KNNImputer()),\n",
       "                (&#x27;Robust_Scaler&#x27;, RobustScaler())],\n",
       "         verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;Knn_imputer&#x27;, KNNImputer()),\n",
       "                (&#x27;Robust_Scaler&#x27;, RobustScaler())],\n",
       "         verbose=True)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;KNNImputer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.impute.KNNImputer.html\">?<span>Documentation for KNNImputer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>KNNImputer()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;RobustScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.RobustScaler.html\">?<span>Documentation for RobustScaler</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>RobustScaler()</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('Knn_imputer', KNNImputer()),\n",
       "                ('Robust_Scaler', RobustScaler())],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000, 171)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "main_test_df = pd.read_csv(r\"F:\\iNeuron\\Projects\\Scania_Truck_Failures\\artifacts\\data\\processed\\stage_1_initial_processing\\preprocessed_test_data.csv\")\n",
    "main_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "0    15625\n",
       "1      375\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_test_df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-27 13:57:22,445: INFO: utils: schema.yaml yaml_file is loaded]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ab_000',\n",
       " 'bm_000',\n",
       " 'bn_000',\n",
       " 'bo_000',\n",
       " 'bp_000',\n",
       " 'bq_000',\n",
       " 'br_000',\n",
       " 'cr_000']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_remove_dict = load_yaml(SCHEMA_PATH)['columns_with_more_than_50%_missing_values']\n",
    "cols_to_remove = list(cols_to_remove_dict.keys())\n",
    "cols_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-27 13:57:24,058: INFO: utils: schema.yaml yaml_file is loaded]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['cd_000']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_with_0_std_dev_dict = load_yaml(SCHEMA_PATH)['columns_with_zero_standard_deviation']\n",
    "columns_with_0_std_dev = list(columns_with_0_std_dev_dict.keys())\n",
    "columns_with_0_std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "0    15625\n",
      "1      375\n",
      "Name: count, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(main_test_df['class'].value_counts(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa_000</th>\n",
       "      <th>ab_000</th>\n",
       "      <th>ac_000</th>\n",
       "      <th>ad_000</th>\n",
       "      <th>ae_000</th>\n",
       "      <th>af_000</th>\n",
       "      <th>ag_000</th>\n",
       "      <th>ag_001</th>\n",
       "      <th>ag_002</th>\n",
       "      <th>ag_003</th>\n",
       "      <th>...</th>\n",
       "      <th>ee_002</th>\n",
       "      <th>ee_003</th>\n",
       "      <th>ee_004</th>\n",
       "      <th>ee_005</th>\n",
       "      <th>ee_006</th>\n",
       "      <th>ee_007</th>\n",
       "      <th>ee_008</th>\n",
       "      <th>ee_009</th>\n",
       "      <th>ef_000</th>\n",
       "      <th>eg_000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2682.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1098.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>412.0</td>\n",
       "      <td>654.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1068.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66002.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199486.0</td>\n",
       "      <td>...</td>\n",
       "      <td>495076.0</td>\n",
       "      <td>380368.0</td>\n",
       "      <td>440134.0</td>\n",
       "      <td>269556.0</td>\n",
       "      <td>1315022.0</td>\n",
       "      <td>153680.0</td>\n",
       "      <td>516.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59816.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1010.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>540820.0</td>\n",
       "      <td>243270.0</td>\n",
       "      <td>483302.0</td>\n",
       "      <td>485332.0</td>\n",
       "      <td>431376.0</td>\n",
       "      <td>210074.0</td>\n",
       "      <td>281662.0</td>\n",
       "      <td>3232.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1814.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>156.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7646.0</td>\n",
       "      <td>4144.0</td>\n",
       "      <td>18466.0</td>\n",
       "      <td>49782.0</td>\n",
       "      <td>3176.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 171 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa_000  ab_000  ac_000  ad_000  ae_000  af_000  ag_000  ag_001  ag_002  \\\n",
       "0     60.0     0.0    20.0    12.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     82.0     0.0    68.0    40.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2  66002.0     2.0   212.0   112.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3  59816.0     NaN  1010.0   936.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4   1814.0     NaN   156.0   140.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "     ag_003  ...    ee_002    ee_003    ee_004    ee_005     ee_006    ee_007  \\\n",
       "0    2682.0  ...    1098.0     138.0     412.0     654.0       78.0      88.0   \n",
       "1       0.0  ...    1068.0     276.0    1620.0     116.0       86.0     462.0   \n",
       "2  199486.0  ...  495076.0  380368.0  440134.0  269556.0  1315022.0  153680.0   \n",
       "3       0.0  ...  540820.0  243270.0  483302.0  485332.0   431376.0  210074.0   \n",
       "4       0.0  ...    7646.0    4144.0   18466.0   49782.0     3176.0     482.0   \n",
       "\n",
       "     ee_008  ee_009  ef_000  eg_000  \n",
       "0       0.0     0.0     0.0     0.0  \n",
       "1       0.0     0.0     0.0     0.0  \n",
       "2     516.0     0.0     0.0     0.0  \n",
       "3  281662.0  3232.0     0.0     0.0  \n",
       "4      76.0     0.0     0.0     0.0  \n",
       "\n",
       "[5 rows x 171 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-27 13:57:40,090: INFO: utils: Dropping same columns in test_data, that had more than 50% missing values in train_data]\n",
      "[2024-02-27 13:57:40,108: INFO: utils: Dropped columns:\n",
      "['ab_000', 'bm_000', 'bn_000', 'bo_000', 'bp_000', 'bq_000', 'br_000', 'cr_000']]\n",
      "[2024-02-27 13:57:40,111: INFO: utils: Dropping same columns in test_data, that had zero standard deviation in train_data]\n",
      "[2024-02-27 13:57:40,126: INFO: utils: Dropped columns:\n",
      "['cd_000']]\n"
     ]
    }
   ],
   "source": [
    "validated_test_data = data_validation(dataframe=main_test_df,\n",
    "                                             cols_to_remove=cols_to_remove,\n",
    "                                             columns_with_0_std_dev=columns_with_0_std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000, 162)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validated_test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "0    15625\n",
       "1      375\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validated_test_data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa_000</th>\n",
       "      <th>ac_000</th>\n",
       "      <th>ad_000</th>\n",
       "      <th>ae_000</th>\n",
       "      <th>af_000</th>\n",
       "      <th>ag_000</th>\n",
       "      <th>ag_001</th>\n",
       "      <th>ag_002</th>\n",
       "      <th>ag_003</th>\n",
       "      <th>ag_004</th>\n",
       "      <th>...</th>\n",
       "      <th>ee_002</th>\n",
       "      <th>ee_003</th>\n",
       "      <th>ee_004</th>\n",
       "      <th>ee_005</th>\n",
       "      <th>ee_006</th>\n",
       "      <th>ee_007</th>\n",
       "      <th>ee_008</th>\n",
       "      <th>ee_009</th>\n",
       "      <th>ef_000</th>\n",
       "      <th>eg_000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60.0</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2682.0</td>\n",
       "      <td>4736.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1098.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>412.0</td>\n",
       "      <td>654.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82.0</td>\n",
       "      <td>6.800000e+01</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>748.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1068.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66002.0</td>\n",
       "      <td>2.120000e+02</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199486.0</td>\n",
       "      <td>1358536.0</td>\n",
       "      <td>...</td>\n",
       "      <td>495076.0</td>\n",
       "      <td>380368.0</td>\n",
       "      <td>440134.0</td>\n",
       "      <td>269556.0</td>\n",
       "      <td>1315022.0</td>\n",
       "      <td>153680.0</td>\n",
       "      <td>516.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59816.0</td>\n",
       "      <td>1.010000e+03</td>\n",
       "      <td>936.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123922.0</td>\n",
       "      <td>...</td>\n",
       "      <td>540820.0</td>\n",
       "      <td>243270.0</td>\n",
       "      <td>483302.0</td>\n",
       "      <td>485332.0</td>\n",
       "      <td>431376.0</td>\n",
       "      <td>210074.0</td>\n",
       "      <td>281662.0</td>\n",
       "      <td>3232.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1814.0</td>\n",
       "      <td>1.560000e+02</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7646.0</td>\n",
       "      <td>4144.0</td>\n",
       "      <td>18466.0</td>\n",
       "      <td>49782.0</td>\n",
       "      <td>3176.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>81852.0</td>\n",
       "      <td>2.130706e+09</td>\n",
       "      <td>892.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5486.0</td>\n",
       "      <td>...</td>\n",
       "      <td>632658.0</td>\n",
       "      <td>273242.0</td>\n",
       "      <td>510354.0</td>\n",
       "      <td>373918.0</td>\n",
       "      <td>349840.0</td>\n",
       "      <td>317840.0</td>\n",
       "      <td>960024.0</td>\n",
       "      <td>25566.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15996</th>\n",
       "      <td>18.0</td>\n",
       "      <td>5.200000e+01</td>\n",
       "      <td>46.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6444.0</td>\n",
       "      <td>...</td>\n",
       "      <td>266.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15997</th>\n",
       "      <td>79636.0</td>\n",
       "      <td>1.670000e+03</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15062.0</td>\n",
       "      <td>...</td>\n",
       "      <td>806832.0</td>\n",
       "      <td>449962.0</td>\n",
       "      <td>778826.0</td>\n",
       "      <td>581558.0</td>\n",
       "      <td>375498.0</td>\n",
       "      <td>222866.0</td>\n",
       "      <td>358934.0</td>\n",
       "      <td>19548.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15998</th>\n",
       "      <td>110.0</td>\n",
       "      <td>3.600000e+01</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>...</td>\n",
       "      <td>588.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>544.0</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>1338.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15999</th>\n",
       "      <td>8.0</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1350.0</td>\n",
       "      <td>...</td>\n",
       "      <td>46.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16000 rows × 162 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        aa_000        ac_000  ad_000  ae_000  af_000  ag_000  ag_001  ag_002  \\\n",
       "0         60.0  2.000000e+01    12.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1         82.0  6.800000e+01    40.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2      66002.0  2.120000e+02   112.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3      59816.0  1.010000e+03   936.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4       1814.0  1.560000e+02   140.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...        ...           ...     ...     ...     ...     ...     ...     ...   \n",
       "15995  81852.0  2.130706e+09   892.0     0.0     0.0     0.0     0.0     0.0   \n",
       "15996     18.0  5.200000e+01    46.0     8.0    26.0     0.0     0.0     0.0   \n",
       "15997  79636.0  1.670000e+03  1518.0     0.0     0.0     0.0     0.0     0.0   \n",
       "15998    110.0  3.600000e+01    32.0     0.0     0.0     0.0     0.0     0.0   \n",
       "15999      8.0  6.000000e+00     4.0     2.0     2.0     0.0     0.0     0.0   \n",
       "\n",
       "         ag_003     ag_004  ...    ee_002    ee_003    ee_004    ee_005  \\\n",
       "0        2682.0     4736.0  ...    1098.0     138.0     412.0     654.0   \n",
       "1           0.0      748.0  ...    1068.0     276.0    1620.0     116.0   \n",
       "2      199486.0  1358536.0  ...  495076.0  380368.0  440134.0  269556.0   \n",
       "3           0.0   123922.0  ...  540820.0  243270.0  483302.0  485332.0   \n",
       "4           0.0       72.0  ...    7646.0    4144.0   18466.0   49782.0   \n",
       "...         ...        ...  ...       ...       ...       ...       ...   \n",
       "15995       0.0     5486.0  ...  632658.0  273242.0  510354.0  373918.0   \n",
       "15996       0.0     6444.0  ...     266.0      44.0      46.0      14.0   \n",
       "15997       0.0    15062.0  ...  806832.0  449962.0  778826.0  581558.0   \n",
       "15998       0.0      198.0  ...     588.0     210.0     180.0     544.0   \n",
       "15999       0.0     1350.0  ...      46.0      10.0      48.0      14.0   \n",
       "\n",
       "          ee_006    ee_007    ee_008   ee_009  ef_000  eg_000  \n",
       "0           78.0      88.0       0.0      0.0     0.0     0.0  \n",
       "1           86.0     462.0       0.0      0.0     0.0     0.0  \n",
       "2      1315022.0  153680.0     516.0      0.0     0.0     0.0  \n",
       "3       431376.0  210074.0  281662.0   3232.0     0.0     0.0  \n",
       "4         3176.0     482.0      76.0      0.0     0.0     0.0  \n",
       "...          ...       ...       ...      ...     ...     ...  \n",
       "15995   349840.0  317840.0  960024.0  25566.0     0.0     0.0  \n",
       "15996        2.0       0.0       0.0      0.0     0.0     0.0  \n",
       "15997   375498.0  222866.0  358934.0  19548.0     0.0     0.0  \n",
       "15998     1004.0    1338.0      74.0      0.0     0.0     0.0  \n",
       "15999       42.0      46.0       0.0      0.0     0.0     0.0  \n",
       "\n",
       "[16000 rows x 162 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validated_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_x = validated_test_data.drop(columns=['class'])\n",
    "test_data_y = validated_test_data['class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "smote = SMOTETomek(sampling_strategy='minority', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_x_transformed = preprocessor.transform(test_data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(pd.DataFrame(test_data_x_transformed).isna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_x_transformed_smote, test_data_y_transformed_smote = smote.fit_resample(X=test_data_x_transformed,\n",
    "                                                                                          y=test_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-27 15:46:32,580: INFO: utils: file_lineage.yaml yaml_file is loaded]\n"
     ]
    }
   ],
   "source": [
    "file_lineage = load_yaml(r\"F:\\iNeuron\\Projects\\Scania_Truck_Failures\\artifacts\\data\\file_lineage.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Batch_file_1': 'aps_failure_test_set_testing4.csv',\n",
       " 'S3_file_1': 'aps_failure_test_set_testing1.csv',\n",
       " 'S3_file_2': 'aps_failure_test_set_testing2.csv',\n",
       " 'S3_file_3': 'aps_failure_test_set_testing6.csv',\n",
       " 'S3_file_4': 'aps_failure_test_set_testing10.csv',\n",
       " 'S3_file_5': 'aps_failure_test_set_testing3.csv',\n",
       " 'S3_file_6': 'aps_failure_test_set_testing4.csv'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_lineage['files_predicted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Key': 'aps_failure_test_set_testing2.csv',\n",
       " 'LastModified': datetime.datetime(2024, 2, 27, 9, 39, 18, 812000, tzinfo=tzutc()),\n",
       " 'ETag': '\"b678a6d01e1b1ae50d8d131c71979adf\"',\n",
       " 'Size': 29536}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_in_s3_[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_1': 'Prediction_aps_failure_test_set_testing5.csv',\n",
       " 'file_2': 'aps_failure_test_set_testing1.csv',\n",
       " 'file_3': 'aps_failure_test_set_testing10.csv',\n",
       " 'file_4': 'aps_failure_test_set_testing2.csv',\n",
       " 'file_5': 'aps_failure_test_set_testing3.csv',\n",
       " 'file_6': 'aps_failure_test_set_testing4.csv',\n",
       " 'file_7': 'aps_failure_test_set_testing5.csv',\n",
       " 'file_8': 'aps_failure_test_set_testing6.csv',\n",
       " 'file_9': 'aps_failure_test_set_testing7.csv',\n",
       " 'file_10': 'aps_failure_test_set_testing8.csv',\n",
       " 'file_11': 'aps_failure_test_set_testing9.csv',\n",
       " 'file_12': 'remote_aps_failure_testing_set.csv',\n",
       " 'file_13': 'remote_aps_failure_training_set.csv'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.constants import BUCKET\n",
    "files_from_s3_dict_temp = {}\n",
    "files_in_s3_ = s3.list_objects_v2(Bucket=BUCKET)['Contents']\n",
    "for i in range(len(files_in_s3_)):\n",
    "        files_from_s3_dict_temp[f\"file_{i+1}\"] = files_in_s3_[i]['Key']\n",
    "files_from_s3_dict_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_from_s3_dict_temp_copy = files_from_s3_dict_temp.copy()\n",
    "for key,value in files_from_s3_dict_temp.items():\n",
    "        if value.startswith(\"Prediction\"):\n",
    "            files_from_s3_dict_temp_copy.pop(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_2': 'aps_failure_test_set_testing1.csv',\n",
       " 'file_3': 'aps_failure_test_set_testing10.csv',\n",
       " 'file_4': 'aps_failure_test_set_testing2.csv',\n",
       " 'file_5': 'aps_failure_test_set_testing3.csv',\n",
       " 'file_6': 'aps_failure_test_set_testing4.csv',\n",
       " 'file_7': 'aps_failure_test_set_testing5.csv',\n",
       " 'file_8': 'aps_failure_test_set_testing6.csv',\n",
       " 'file_9': 'aps_failure_test_set_testing7.csv',\n",
       " 'file_10': 'aps_failure_test_set_testing8.csv',\n",
       " 'file_11': 'aps_failure_test_set_testing9.csv',\n",
       " 'file_12': 'remote_aps_failure_testing_set.csv',\n",
       " 'file_13': 'remote_aps_failure_training_set.csv'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_from_s3_dict_temp_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
