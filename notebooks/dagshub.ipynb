{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\iNeuron\\Projects\\Scania_Truck_Failures\\scania_truck_failures\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Client created. Use the name of the repo <span style=\"font-weight: bold\">(</span>scania_truck_failures<span style=\"font-weight: bold\">)</span> as the name of the bucket\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Client created. Use the name of the repo \u001b[1m(\u001b[0mscania_truck_failures\u001b[1m)\u001b[0m as the name of the bucket\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-27 11:35:10,611: INFO: helpers: Client created. Use the name of the repo (scania_truck_failures) as the name of the bucket]\n",
      "[2024-02-27 11:35:10,639: INFO: utils: config.yaml yaml_file is loaded]\n",
      "[2024-02-27 11:35:10,644: INFO: utils: params.yaml yaml_file is loaded]\n",
      "[2024-02-27 11:35:10,654: INFO: utils: schema.yaml yaml_file is loaded]\n",
      "[2024-02-27 11:35:10,657: INFO: utils: mlflow_config.yaml yaml_file is loaded]\n",
      "[2024-02-27 11:35:10,695: INFO: utils: mlflow_config.yaml yaml_file is loaded]\n",
      "[2024-02-27 11:35:10,699: INFO: utils: mlflow_config.yaml yaml_file is loaded]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Client created. Use the name of the repo <span style=\"font-weight: bold\">(</span>scania_truck_failures<span style=\"font-weight: bold\">)</span> as the name of the bucket\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Client created. Use the name of the repo \u001b[1m(\u001b[0mscania_truck_failures\u001b[1m)\u001b[0m as the name of the bucket\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-27 11:35:10,721: INFO: helpers: Client created. Use the name of the repo (scania_truck_failures) as the name of the bucket]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import json\n",
    "from src.utils import load_yaml\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "from src.config.configuration_manager import ConfigurationManager\n",
    "from dagshub import get_repo_bucket_client\n",
    "s3 = get_repo_bucket_client(\"Raj-Narayanan-B/scania_truck_failures\")\n",
    "import os\n",
    "os.chdir(r\"f:\\\\iNeuron\\\\Projects\\\\Scania_Truck_Failures\")\n",
    "import shutil\n",
    "import glob\n",
    "from src.components.stage_0_data_DB_upload import file_tracker_component\n",
    "file_tracker_obj = file_tracker_component()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-27 11:46:37,102: INFO: utils: file_lineage.yaml yaml_file is loaded]\n",
      "I'm in ELSE BLOCK\n",
      "s3_files_list: ['aps_failure_test_set_testing1.csv', 'aps_failure_test_set_testing10.csv', 'aps_failure_test_set_testing4.csv', 'aps_failure_test_set_testing5.csv', 'aps_failure_test_set_testing6.csv', 'aps_failure_test_set_testing7.csv', 'aps_failure_test_set_testing8.csv', 'aps_failure_test_set_testing9.csv']\n",
      "batch_files_list: []\n",
      "[2024-02-27 11:46:37,107: INFO: utils: yaml file is saved]\n"
     ]
    }
   ],
   "source": [
    "file_tracker_obj.file_lineage_tracker(#add_list = ['test_file10.csv'],\n",
    "                                       update_list = [#'aps_failure_test_set_testing3.csv',#'aps_failure_test_set_testing3.csv',\n",
    "                                                       #'aps_failure_test_set_testing2.csv','aps_failure_test_set_testing4.csv',\n",
    "                                                       'test_file2.csv','test_file4.csv','test_file6.csv','test_file8.csv'],\n",
    "                                      reverse_update_list = [#'test_file2.csv','test_file4.csv', 'test_file6.csv','test_file8.csv', 'test_file10.csv',\n",
    "                                                              'aps_failure_test_set_testing1.csv','aps_failure_test_set_testing2.csv',\n",
    "                                                              'aps_failure_test_set_testing3.csv','aps_failure_test_set_testing4.csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint({'S3_file_10': 'aps_failure_test_set_testing8.csv', 'S3_file_11': 'aps_failure_test_set_testing9.csv', 'S3_file_3': 'aps_failure_test_set_testing10.csv', 'S3_file_4': 'aps_failure_test_set_testing2.csv', 'S3_file_5': 'aps_failure_test_set_testing3.csv', 'S3_file_6': 'aps_failure_test_set_testing4.csv', 'S3_file_7': 'aps_failure_test_set_testing5.csv', 'S3_file_8': 'aps_failure_test_set_testing6.csv', 'S3_file_9': 'aps_failure_test_set_testing7.csv'},compact=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['1'] not in ['test'] + ['aps_failure_test_set_testing2.csv', \n",
    "            'aps_failure_test_set_testing4.csv',\n",
    "            'aps_failure_test_set_testing5.csv',\n",
    "            'aps_failure_test_set_testing6.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = ['test', 'aps_failure_test_set_testing2.csv', 'aps_failure_test_set_testing4.csv', 'aps_failure_test_set_testing5.csv', 'aps_failure_test_set_testing6.csv']\n",
    "list2 = ['aps_failure_test_set_testing4.csv', 'aps_failure_test_set_testing5.csv', 'aps_failure_test_set_testing6.csv']\n",
    "\n",
    "result = list(set(list1) - set(list2))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(list1) - set(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file(\n",
    "    Bucket=\"scania_truck_failures\",  # name of the repo\n",
    "    Filename=r\"F:\\iNeuron\\Projects\\Data\\Scania Truck Failures\\aps_failure_test_set.csv\",  # local path of file to upload\n",
    "    Key=\"remote_aps_failure_testing_set.csv\",  # remote path where to upload the file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file(\n",
    "    Bucket=\"scania_truck_failures\",  # name of the repo\n",
    "    Filename=r\"F:\\iNeuron\\Projects\\Data\\Scania Truck Failures\\aps_failure_training_set.csv\",  # local path of file to upload\n",
    "    Key=\"remote_aps_failure_training_set.csv\",  # remote path where to upload the file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [6,9,10]:\n",
    "    s3.upload_file(\n",
    "        Bucket=\"scania_truck_failures\",  # name of the repo\n",
    "        Filename=f\"F:\\iNeuron\\Projects\\Data\\Scania Truck Failures\\\\aps_failure_test_set_testing{i}.csv\",  # local path of file to upload\n",
    "        Key=f\"aps_failure_test_set_testing{i}.csv\",  # remote path where to upload the file\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [6,9,10]:\n",
    "    s3.delete_object(Bucket = \"scania_truck_failures\",\n",
    "                    Key=f\"aps_failure_test_set_testing{i}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file(\n",
    "    Bucket=\"scania_truck\",  # name of the repo\n",
    "    Filename=r\"F:\\iNeuron\\Projects\\Data\\Scania Truck Failures\\aps_failure_training_set.csv\",  # local path of file to upload\n",
    "    Key=\"remote_aps_failure_training_set.csv\",  # remote path where to upload the file\n",
    ")\n",
    "\n",
    "s3.upload_file(\n",
    "    Bucket=\"scania_truck\",  # name of the repo\n",
    "    Filename=r\"F:\\iNeuron\\Projects\\Data\\Scania Truck Failures\\aps_failure_test_set.csv\",  # local path of file to upload\n",
    "    Key=\"remote_aps_failure_testing_set.csv\",  # remote path where to upload the file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.download_file(\n",
    "    Bucket=\"scania_truck\",  # name of the repo\n",
    "    Key=\"remote_aps_failure_training_set.csv\",  #  remote path from where to download the file\n",
    "    Filename=r\"F:\\iNeuron\\Projects\\scania_failures_2\\notebooks\\sample_train.csv\",  # local path where to download the file\n",
    ")\n",
    "\n",
    "s3.download_file(\n",
    "    Bucket=\"scania_truck\",  # name of the repo\n",
    "    Key=\"remote_aps_failure_testing_set.csv\",  #  remote path from where to download the file\n",
    "    Filename=r\"F:\\iNeuron\\Projects\\scania_failures_2\\notebooks\\sample_test.csv\",  # local path where to download the file\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dagshub import get_repo_bucket_client\n",
    "s3 = get_repo_bucket_client(\"Raj-Narayanan-B/scania_truck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_files = load_yaml(r\"artifacts\\data\\file_lineage.yaml\")\n",
    "pprint(dict(old_files),compact = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_s3 = s3.list_objects_v2(Bucket='scania_truck')['Contents']\n",
    "files_in_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_from_s3_dict = {}\n",
    "for i in range(len(files_in_s3)):\n",
    "    files_from_s3_dict[f\"file_{i+1}\"] = files_in_s3[i]['Key']\n",
    "files_from_s3_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_from_s3_list = list(files_from_s3_dict.values())\n",
    "files_from_s3_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_files_for_model_training = old_files['files_for_model_training']\n",
    "old_files_predicted = old_files['files_predicted']\n",
    "old_files_to_predict = old_files['files_to_predict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_files_for_model_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import s3_counter\n",
    "\n",
    "\n",
    "for key,value in old_files_for_model_training.items():\n",
    "    if value in files_from_s3_list:\n",
    "        files_from_s3_list.remove(value)\n",
    "    else:\n",
    "        old_files_for_model_training.pop(key)\n",
    "        raise ValueError(f\"{value} is missing from S3\")\n",
    "\n",
    "for key,value in old_files_predicted.items():\n",
    "    if value in files_from_s3_list:\n",
    "        files_from_s3_list.remove(value)\n",
    "    else:\n",
    "        old_files_predicted.pop(key)\n",
    "\n",
    "for key,value in old_files_to_predict.items():\n",
    "    if value in files_from_s3_list:\n",
    "        files_from_s3_list.remove(value)\n",
    "    else:\n",
    "        old_files_to_predict.pop(key)\n",
    "\n",
    "if len(files_from_s3_list) > 0:\n",
    "    s3_count = s3_counter(old_files_to_predict)\n",
    "    for i in files_from_s3_list:\n",
    "        old_files_to_predict[f'S3_file_{s3_count+1}'] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_from_s3_list.remove('remote_aps_failure_testing_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_from_s3_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_s3 = files_in_s3_.copy()\n",
    "# files_in_s3.append({'Key':\"test_1\"})\n",
    "# files_in_s3.append({\"Key\":\"test_2\"})\n",
    "# files_in_s3.append({\"Key\":\"test_3\"})\n",
    "# files_in_s3.append({\"Key\":\"test_4\"})\n",
    "# files_in_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_from_s3_dict = {}\n",
    "for i in range(len(files_in_s3)):\n",
    "    files_from_s3_dict[f\"file_{i+1}\"] = files_in_s3[i]['Key']\n",
    "files_from_s3_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in files_from_s3_dict.items():\n",
    "    try:\n",
    "        if files_from_s3_dict[key] == dict(config)[key]:\n",
    "            pass\n",
    "    except:\n",
    "        print(f\"{key}: {files_from_s3_dict[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_files = load_yaml(r'artifacts\\data\\file_lineage.yaml')\n",
    "files_from_s3_dict = {}\n",
    "files_from_s3_dict['files_for_model_training'] = {}  # old_files['files_for_model_training']\n",
    "files_from_s3_dict['files_to_predict'] = {}  # old_files['files_to_predict']\n",
    "files_from_s3_dict['files_predicted'] = {}  # old_files['files_predicted']\n",
    "predicted_files_counter = 1\n",
    "files_to_predict_counter = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_files.files_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_files_predicted = {}\n",
    "for key,value in old_files.files_predicted.items():\n",
    "    old_files_predicted[key] = old_files.files_predicted[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_files_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_files_to_predict = {}\n",
    "for key,value in old_files.files_to_predict.items():\n",
    "    old_files_to_predict[key] = old_files.files_to_predict[key]\n",
    "old_files_to_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_old_files = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for main_key in ['files_to_predict','files_predicted']:\n",
    "    for key,value in old_files[main_key].items():\n",
    "        all_old_files[key] = old_files[main_key][key]\n",
    "all_old_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Batch_file_3'.startswith('Batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_s3 = s3.list_objects_v2(Bucket='scania_truck')['Contents']\n",
    "files_in_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_ = {}\n",
    "# files_['S3_file_'] = {}\n",
    "for i in range(len(files_in_s3)):\n",
    "    files_[f'S3_file_{i+1}'] = files_in_s3[i]['Key']\n",
    "files_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previously_added_files = dict(set(all_old_files.items()) - set(files_.items()))\n",
    "previously_added_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_old_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_file_counter = 0\n",
    "for key in all_old_files.keys():\n",
    "    if key.startswith('Batch'):\n",
    "        batch_file_counter+=1\n",
    "batch_file_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_predicted_files = ['test_4','test_5','test_6']\n",
    "for i, file_name in enumerate(add_predicted_files):\n",
    "    if file_name in list(old_files_to_predict.values()) or file_name in list(all_old_files.values()):\n",
    "        raise ValueError(f\"Duplicate file entry: {file_name}\")\n",
    "    else:\n",
    "        old_files_to_predict[f'Batch_file_{batch_file_counter+(i+1)}'] = file_name\n",
    "        files_[f'Batch_file_{batch_file_counter+(i+1)}'] = file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_files_to_predict.pop('Batch_file_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_files_to_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_tracker_obj.file_lineage_tracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_files = load_yaml(r'artifacts\\data\\file_lineage.yaml')\n",
    "old_files_for_model_training = old_files['files_for_model_training']\n",
    "old_files_predicted = old_files['files_predicted']\n",
    "old_files_to_predict = old_files['files_to_predict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_counter(dict_:dict):\n",
    "    counter = 0\n",
    "    for key in dict_.keys():\n",
    "        if key.startswith('Batch'):\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "def s3_counter(dict_:dict):\n",
    "    counter = 0\n",
    "    for key in dict_.keys():\n",
    "        if key.startswith('S3'):\n",
    "            counter += 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_counter(old_files_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_counter(old_files_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_counter(old_files_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch_file_1: test_1.csv\n",
    "# Batch_file_2: test_2.csv\n",
    "# Batch_file_3: test_3.csv\n",
    "# Batch_file_4: test_7\n",
    "# Batch_file_5: test_4.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import save_yaml\n",
    "add_predicted_files = ['test_7']\n",
    "batch_count = batch_counter(old_files_predicted)\n",
    "for i,file in enumerate(add_predicted_files,start=1):\n",
    "    if file in list(old_files_predicted.values()):\n",
    "        raise ValueError(f\"Duplicate file entry: {file}\")\n",
    "    else:\n",
    "        old_files_predicted[f\"Batch_file_{batch_count+(i)}\"] = file\n",
    "\n",
    "files_from_s3_dict = {}\n",
    "files_from_s3_dict['files_for_model_training'] = old_files_for_model_training\n",
    "files_from_s3_dict['files_to_predict'] = old_files_to_predict\n",
    "files_from_s3_dict['files_predicted'] = old_files_predicted\n",
    "save_yaml(files_from_s3_dict,\n",
    "          filepath=r'F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\file_lineage.yaml',\n",
    "          mode = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_predicted_files = ['test_4.csv','aps_failure_test_set_testing.csv','aps_failure_test_set_testing1.csv','aps_failure_test_set_testing3.csv']\n",
    "update_predicted_files = ['aps_failure_test_set_testing8.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import save_yaml\n",
    "key_list = []\n",
    "for i in update_predicted_files:\n",
    "    for key,value in old_files_to_predict.items():\n",
    "        if i == value:\n",
    "            if key.startswith(\"S3\"):\n",
    "                key_list.append(key)\n",
    "                s3_count = s3_counter(old_files_predicted)\n",
    "                old_files_predicted[f\"S3_file_{s3_count+1}\"] = i\n",
    "            elif key.startswith('Batch'):\n",
    "                key_list.append(key)\n",
    "                batch_count = batch_counter(old_files_predicted)\n",
    "                old_files_predicted[f\"Batch_file_{batch_count+1}\"] = i\n",
    "for i in key_list:\n",
    "    old_files_to_predict.pop(i)\n",
    "\n",
    "s3_count = s3_counter(old_files_to_predict)\n",
    "batch_count = batch_counter(old_files_to_predict)\n",
    "remaining_s3_files_to_predict = []\n",
    "remaining_batch_files_to_predict = []\n",
    "for key in old_files_to_predict.keys():\n",
    "    if key.startswith(\"S3\"):\n",
    "        remaining_s3_files_to_predict.append(old_files_to_predict[key])\n",
    "    elif key.startswith(\"Batch\"):\n",
    "        remaining_batch_files_to_predict.append(old_files_to_predict[key])\n",
    "old_files_to_predict = {}\n",
    "try:\n",
    "    for i in range(s3_count):\n",
    "        old_files_to_predict[f\"S3_file_{i+1}\"] = remaining_s3_files_to_predict[i]\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    for i in range(batch_count):\n",
    "        old_files_to_predict[f\"Batch_file{i+1}\"] = remaining_batch_files_to_predict[i]\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "files_from_s3_dict = {}\n",
    "files_from_s3_dict['files_for_model_training'] = old_files_for_model_training\n",
    "files_from_s3_dict['files_to_predict'] = old_files_to_predict\n",
    "files_from_s3_dict['files_predicted'] = old_files_predicted\n",
    "save_yaml(files_from_s3_dict,\n",
    "          filepath=r'F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\file_lineage.yaml',\n",
    "          mode = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_update_predicted_files = ['test_7','aps_failure_test_set_testing.csv','aps_failure_test_set_testing1.csv','aps_failure_test_set_testing3.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import save_yaml\n",
    "\n",
    "key_list = []\n",
    "for i in reverse_update_predicted_files:\n",
    "    for key,value in old_files_predicted.items():\n",
    "        if i == value:\n",
    "            if key.startswith(\"S3\"):\n",
    "                key_list.append(key)\n",
    "                s3_count = s3_counter(old_files_to_predict)\n",
    "                old_files_to_predict[f\"S3_file_{s3_count+1}\"] = i\n",
    "            elif key.startswith('Batch'):\n",
    "                key_list.append(key)\n",
    "                batch_count = batch_counter(old_files_to_predict)\n",
    "                old_files_to_predict[f\"Batch_file_{batch_count+1}\"] = i\n",
    "for i in key_list:\n",
    "    old_files_predicted.pop(i)\n",
    "\n",
    "s3_count = s3_counter(old_files_predicted)\n",
    "batch_count = batch_counter(old_files_predicted)\n",
    "remaining_s3_files = []\n",
    "remaining_batch_files = []\n",
    "for key in old_files_predicted.keys():\n",
    "    if key.startswith(\"S3\"):\n",
    "        remaining_s3_files.append(old_files_predicted[key])\n",
    "    elif key.startswith(\"Batch\"):\n",
    "        remaining_batch_files.append(old_files_predicted[key])\n",
    "old_files_predicted = {}\n",
    "try:\n",
    "    for i in range(s3_count):\n",
    "        old_files_predicted[f\"S3_file_{i+1}\"] = remaining_s3_files[i]\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    for i in range(batch_count):\n",
    "        old_files_predicted[f\"Batch_file{i+1}\"] = remaining_batch_files[i]\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "files_from_s3_dict = {}\n",
    "files_from_s3_dict['files_for_model_training'] = old_files_for_model_training\n",
    "files_from_s3_dict['files_to_predict'] = old_files_to_predict\n",
    "files_from_s3_dict['files_predicted'] = old_files_predicted\n",
    "save_yaml(files_from_s3_dict,\n",
    "          filepath=r'F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\file_lineage.yaml',\n",
    "          mode = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(old_files_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(old_files_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_s3 = s3.list_objects_v2(Bucket='scania_truck')['Contents']\n",
    "old_files = load_yaml(r'F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\file_lineage.yaml')\n",
    "\n",
    "files_from_s3_dict = {}\n",
    "for i in range(len(files_in_s3)):\n",
    "    files_from_s3_dict[f\"file_{i+1}\"] = files_in_s3[i]['Key']\n",
    "files_from_s3_list = list(files_from_s3_dict.values())\n",
    "\n",
    "old_files_for_model_training = old_files['files_for_model_training']\n",
    "pprint(old_files_for_model_training,compact=True)\n",
    "print('\\n')\n",
    "\n",
    "old_files_predicted = old_files['files_predicted']\n",
    "pprint(old_files_predicted,compact=True)\n",
    "print('\\n')\n",
    "\n",
    "old_files_to_predict = old_files['files_to_predict']\n",
    "pprint(old_files_to_predict,compact=True)\n",
    "print('\\n')\n",
    "\n",
    "for key, value in old_files_for_model_training.items():\n",
    "    if value in files_from_s3_list:\n",
    "        files_from_s3_list.remove(value)\n",
    "        print(f'removed_value: {value}')\n",
    "    else:\n",
    "        old_files_for_model_training.pop(key)\n",
    "        raise ValueError(f\"{value} is missing from S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_from_s3_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_from_s3_list_=files_from_s3_list.copy()\n",
    "for file in files_from_s3_list_:\n",
    "    print(file)\n",
    "    if file in list(old_files_predicted.values()):\n",
    "        files_from_s3_list.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_from_s3_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(old_files_to_predict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_from_s3_list_=files_from_s3_list.copy()\n",
    "for file in files_from_s3_list_:\n",
    "    if file in list(old_files_to_predict.values()):\n",
    "        files_from_s3_list.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_from_s3_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_from_s3_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.constants import TEST_DATA,TRAIN_DATA,BUCKET\n",
    "from src.utils import load_yaml,save_yaml\n",
    "files_in_s3 = s3.list_objects_v2(Bucket=BUCKET)['Contents']\n",
    "files_in_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be inserted in the \"else\" block of stage_0_data_DB_upload.py file's \"s3_data_download\" method\n",
    "\n",
    "with open(r'artifacts\\data\\data_loaded_from_s3_so_far.yaml', 'w'):\n",
    "    files_from_s3_dict = {}\n",
    "    files_from_s3_dict['files_for_model_training'] = {}\n",
    "    files_from_s3_dict['files_to_predict'] = {}\n",
    "    files_from_s3_dict['files_predicted'] = {}\n",
    "    file_counter = 1\n",
    "    for i in range(len(files_in_s3)):\n",
    "        if files_in_s3[i]['Key'] == TEST_DATA or files_in_s3[i]['Key'] == TRAIN_DATA:\n",
    "            file_name = \"training_set\" if files_in_s3[i]['Key'] == TRAIN_DATA else \"testing_set\"\n",
    "            files_from_s3_dict['files_for_model_training'][file_name] = files_in_s3[i]['Key']\n",
    "            # s3.download_file(\n",
    "            #     Bucket=BUCKET,\n",
    "            #     Key=files_from_s3_dict['files_for_model_training'][file_name],\n",
    "            #     Filename= f\"artifacts/data/temp\" + f\"/{files_from_s3_dict['files_for_model_training'][file_name]}\"\n",
    "            # )\n",
    "        else:\n",
    "            file_name = f\"file_{file_counter}\"\n",
    "            files_from_s3_dict['files_to_predict'][file_name] = files_in_s3[i]['Key']\n",
    "            # s3.download_file(\n",
    "            #     Bucket=BUCKET,\n",
    "            #     Key=files_from_s3_dict['files_to_predict'][file_name],\n",
    "            #     Filename= \"artifacts/data/temp\" + f\"/{files_from_s3_dict['files_to_predict'][file_name]}\"\n",
    "            # )\n",
    "            file_counter += 1\n",
    "    save_yaml(file=files_from_s3_dict,\n",
    "                filepath=r'artifacts\\data\\data_loaded_from_s3_so_far.yaml',\n",
    "                mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be included at the last of prediction pipeline if user chooses bulk/batch prediction\n",
    "\n",
    "predicted_files = ['aps_failure_test_set_testing2.csv', 'aps_failure_test_set_testing4.csv']#'aps_failure_test_set_testing2.csv',\n",
    "                #    'aps_failure_test_set_testing3.csv']\n",
    "\n",
    "files_from_s3_dict = {}\n",
    "files_from_s3_dict['files_for_model_training'] = {}  # old_files['files_for_model_training']\n",
    "files_from_s3_dict['files_to_predict'] = {}  # old_files['files_to_predict']\n",
    "files_from_s3_dict['files_predicted'] = {}  # old_files['files_predicted']\n",
    "predicted_files_counter = 1\n",
    "files_to_predict_counter = 1\n",
    "for i in range(len(files_in_s3)):\n",
    "    if files_in_s3[i]['Key'] == TEST_DATA or files_in_s3[i]['Key'] == TRAIN_DATA:\n",
    "        file_name = \"training_set\" if files_in_s3[i]['Key'] == TRAIN_DATA else \"testing_set\"\n",
    "        files_from_s3_dict['files_for_model_training'][file_name] = files_in_s3[i]['Key']\n",
    "\n",
    "    elif files_in_s3[i]['Key'] in predicted_files:\n",
    "        file_name = f\"file_{predicted_files_counter}\"\n",
    "        files_from_s3_dict['files_predicted'][file_name] = files_in_s3[i]['Key']\n",
    "        predicted_files_counter += 1\n",
    "\n",
    "    else:\n",
    "        file_name = f\"file_{files_to_predict_counter}\"\n",
    "        files_from_s3_dict['files_to_predict'][file_name] = files_in_s3[i]['Key']\n",
    "        files_to_predict_counter += 1\n",
    "save_yaml(file=files_from_s3_dict,\n",
    "            filepath=r'artifacts\\data\\data_loaded_from_s3_so_far.yaml',\n",
    "            mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(old_files['files_predicted'].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_files = ['aps_failure_test_set_testing3.csv', 'aps_failure_test_set_testing5.csv']\n",
    "# predicted_files = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_files+list(old_files['files_predicted'].values()) if predicted_files else list(old_files['files_predicted'].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be inserted in the \"if\" block of stage_0_data_DB_upload.py file's \"s3_data_download\" method\n",
    "\n",
    "old_files = load_yaml(r'artifacts\\data\\data_loaded_from_s3_so_far.yaml')\n",
    "files_from_s3_dict = {}\n",
    "files_from_s3_dict['files_for_model_training'] = {}  # old_files['files_for_model_training']\n",
    "files_from_s3_dict['files_to_predict'] = {}  # old_files['files_to_predict']\n",
    "files_from_s3_dict['files_predicted'] = {}  # old_files['files_predicted']\n",
    "predicted_files_counter = 1\n",
    "files_to_predict_counter = 1\n",
    "for i in range(len(files_in_s3)):\n",
    "    if files_in_s3[i]['Key'] == TEST_DATA or files_in_s3[i]['Key'] == TRAIN_DATA:\n",
    "        file_name = \"training_set\" if files_in_s3[i]['Key'] == TRAIN_DATA else \"testing_set\"\n",
    "        files_from_s3_dict['files_for_model_training'][file_name] = files_in_s3[i]['Key']\n",
    "\n",
    "    elif files_in_s3[i]['Key'] in list(old_files['files_predicted'].values()):\n",
    "        file_name = f\"file_{predicted_files_counter}\"\n",
    "        files_from_s3_dict['files_predicted'][file_name] = files_in_s3[i]['Key']\n",
    "        predicted_files_counter += 1\n",
    "\n",
    "    else:\n",
    "        file_name = f\"file_{files_to_predict_counter}\"\n",
    "        files_from_s3_dict['files_to_predict'][file_name] = files_in_s3[i]['Key']\n",
    "        files_to_predict_counter += 1\n",
    "save_yaml(file=files_from_s3_dict,\n",
    "            filepath=r'artifacts\\data\\data_loaded_from_s3_so_far.yaml',\n",
    "            mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_files = load_yaml(r\"artifacts\\data\\file_lineage.yaml\")\n",
    "dict(old_files['files_predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_files['files_predicted'] = {'S3_file_11': 'aps_failure_test_set_testing11.csv',\n",
    "                                'S3_file_12': 'aps_failure_test_set_testing12.csv'}\n",
    "old_files['files_predicted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(old_files['files_predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(old_files['files_to_predict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(old_files['files_to_predict']) + dict(old_files['files_predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(files_from_s3_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_s3_new =  files_in_s3.copy()\n",
    "files_in_s3_new.append({\"Key\":\"test_5\"})\n",
    "files_in_s3_new.append({\"Key\":\"test_6\"})\n",
    "files_in_s3_new.append({\"Key\":\"test_7\"})\n",
    "files_in_s3_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_s3_new_list = [files_in_s3_new[i]['Key'] for i in range(len(files_in_s3_new))]\n",
    "files_in_s3_new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_s3_old_list = [files_in_s3[i]['Key'] for i in range(len(files_in_s3))]\n",
    "files_in_s3_old_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncommon_elements = set(files_in_s3_new_list) ^ set(files_in_s3_old_list)\n",
    "list(uncommon_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_files = load_yaml(r'artifacts\\data\\data_loaded_from_s3_so_far.yaml')\n",
    "old_files\n",
    "old_keys = {item['Key'] for item in dict(old_files)}\n",
    "old_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(old_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_keys = {item['Key'] for item in files_in_s3}\n",
    "new_keys = {item['Key'] for item in files_in_s3_new}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newly_added_keys  = new_keys-old_keys\n",
    "newly_added_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newly_added_items = [item for item in files_in_s3_new if item['Key'] in newly_added_keys]\n",
    "newly_added_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_s3[1]['Key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_from_s3_dict = {}\n",
    "for i in range(len(files_in_s3)):\n",
    "    if files_in_s3[i]['Key'] == 'remote_aps_failure_testing_set.csv' or files_in_s3[i]['Key'] == 'remote_aps_failure_training_set.csv':\n",
    "        pass\n",
    "    else:\n",
    "        files_from_s3_dict[f\"file_{i+1}\"] = files_in_s3[i]['Key']\n",
    "files_from_s3_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length  = len(load_yaml(r'config/config.yaml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_from_s3_dict['file_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(files_from_s3_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath,file = os.path.split(\"artifacts/data/temp/test_data_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'F:\\iNeuron\\Projects\\scania_failures_2\\notebooks\\sample_train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_1 = df.iloc[:,:74]\n",
    "train_df_1['ident_id'] = range(1,60001)\n",
    "train_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2 = df.iloc[:,74:148]\n",
    "train_df_2['ident_id'] = range(1,60001)\n",
    "train_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_3 = df.iloc[:,148:]\n",
    "train_df_3['ident_id'] = range(1,60001)\n",
    "train_df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(r'F:\\iNeuron\\Projects\\scania_failures_2\\notebooks\\sample_test.csv')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_1 = df_test.iloc[:,:74]\n",
    "test_df_1['ident_id'] = range(1,16001)\n",
    "test_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_2 = df_test.iloc[:,74:148]\n",
    "test_df_2['ident_id'] = range(1,16001)\n",
    "test_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_3 = df_test.iloc[:,148:]\n",
    "test_df_3['ident_id'] = range(1,16001)\n",
    "test_df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'F:\\iNeuron\\Projects\\scania_failures_2\\Secrets\\Tokens\\sample-token.json') as f:\n",
    "        secrets = json.load(f)\n",
    "        # logger.info(f\"{config[1]} json file is loaded\")\n",
    "\n",
    "cloud_config = {'secure_connect_bundle': r'F:\\iNeuron\\Projects\\scania_failures_2\\Secrets\\Bundles\\secure-connect-sample.zip',\n",
    "                'connect_timeout': None}\n",
    "# print(f\"cloud_config: {cloud_config}\")\n",
    "CLIENT_ID = secrets[\"clientId\"]\n",
    "CLIENT_SECRET = secrets[\"secret\"]\n",
    "auth_provider = PlainTextAuthProvider(CLIENT_ID, CLIENT_SECRET)\n",
    "# profile = ExecutionProfile(request_timeout=None)\n",
    "cluster = Cluster(cloud=cloud_config,\n",
    "                    auth_provider=auth_provider,\n",
    "                    # execution_profiles={EXEC_PROFILE_DEFAULT: profile},\n",
    "                    protocol_version=4)\n",
    "\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = train_df_1\n",
    "# Define keyspace name and table name\n",
    "keyspace = 'sample_keyspace'\n",
    "table_name = 'uploaded_data'\n",
    "colums_types_dict_from_data_df = dict(zip(list(data_df.columns),list(data_df.dtypes.replace({'object':'text','int64':'int'}).values)))\n",
    "# columns = ', '.join(train_df_1.columns)\n",
    "columns = ', '.join([f\"{key} {value}\" for key,value in colums_types_dict_from_data_df.items()])\n",
    "create_table_statement = f\"CREATE TABLE IF NOT EXISTS {keyspace}.{table_name} ({columns}, PRIMARY KEY (ident_id));\"\n",
    "# create_table_statement = f\"CREATE TABLE IF NOT EXISTS {keyspace}.{table_name} ({columns}, PRIMARY KEY (your_primary_key_column));\"\n",
    "session.execute(create_table_statement)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsbulk_command = [\n",
    "    \"dsbulk\",\n",
    "    \"load\",\n",
    "    \"-url\", os.path.join(\"F:/iNeuron/Projects/scania_failures_2/artifacts/data/temp/\",\"train_data1.csv\"),\n",
    "    \"-k\", \"sample_keyspace\",\n",
    "    \"-t\", \"uploaded_data\",\n",
    "    \"-b\", os.path.join(\"F:/iNeuron/Projects/scania_failures_2/Secrets/Bundles/\",\"secure-connect-sample.zip\"),\n",
    "    \"-u\", \"DfYsAxTkqZZWHwNguQHPBJMt\", \n",
    "    \"-p\", \"gl5rR3g-,_vTiPLZ.Zn7pouaWdAbZQ3elybJmlnaa,D+Zr4TLx4hpYNZrQwFHTARO6GXyeByF3BQS87plEhoSDPN50e,mZgbjqUm2IUTg5p3SojZF7r-GvA0gyNpbIdG\",\n",
    "]\n",
    "result = subprocess.run(dsbulk_command, capture_output=True, text=True,shell=True)\n",
    "\n",
    "print(\"Return code:\", result.returncode)\n",
    "print(\"Standard output:\", result.stdout)\n",
    "print(\"Standard error:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_1 = pd.read_csv(r\"F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\raw\\train_data_1.csv\")\n",
    "train_df_1_ = pd.read_csv(r\"F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\temp\\train_data_1.csv\")\n",
    "\n",
    "train_df_2 = pd.read_csv(r\"F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\raw\\train_data_2.csv\")\n",
    "train_df_2_ = pd.read_csv(r\"F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\temp\\train_data_2.csv\")\n",
    "\n",
    "train_df_3 = pd.read_csv(r\"F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\raw\\train_data_3.csv\")\n",
    "train_df_3_ = pd.read_csv(r\"F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\temp\\train_data_3.csv\")\n",
    "\n",
    "##################################################################################################################\n",
    "\n",
    "test_df_1 = pd.read_csv(r\"F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\raw\\test_data_1.csv\")\n",
    "test_df_1_ = pd.read_csv(r\"F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\temp\\test_data_1.csv\")\n",
    "\n",
    "test_df_2 = pd.read_csv(r\"F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\raw\\test_data_2.csv\")\n",
    "test_df_2_ = pd.read_csv(r\"F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\temp\\test_data_2.csv\")\n",
    "\n",
    "test_df_3 = pd.read_csv(r\"F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\raw\\test_data_3.csv\")\n",
    "test_df_3_ = pd.read_csv(r\"F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\temp\\test_data_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checker_2(df_1:pd.DataFrame,df_2_:pd.DataFrame):\n",
    "    df_1.sort_values(by = 'ident_id',inplace = True)\n",
    "    df_1.reset_index(drop=True,inplace=True)\n",
    "    df_1.drop(columns=['ident_id'],inplace=True)\n",
    "    df_2_.drop(columns=['ident_id'],inplace=True)\n",
    "    try:\n",
    "        df_1.drop(columns=['field_74_'],inplace=True)\n",
    "        df_2_.drop(columns=['class'],inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    return (df_2_.compare(df_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checker_2(df_1 = train_df_1, df_2_=train_df_1_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2.compare(train_df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2_.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df:pd.DataFrame):\n",
    "    df.replace('na', np.nan, inplace=True)\n",
    "    df.drop(columns = ['ident_id'], inplace=True)     \n",
    "    df.rename(columns={'field_74_': 'class'}, inplace=True)  \n",
    "    for i in df.columns:\n",
    "        if i != 'class':\n",
    "            df[i] = df[i].astype('float')\n",
    "        else:\n",
    "            df['class'] = df['class'].map({'neg':0,'pos':1})\n",
    "            df.drop(columns = ['class'], inplace=True)   \n",
    "    return(df)\n",
    "\n",
    "def describe_check(df_1:pd.DataFrame, df_2:pd.DataFrame):\n",
    "    checks = {}\n",
    "    stats_cols = ['count','mean','std','min','25%','50%','75%','max']\n",
    "    for i in stats_cols:\n",
    "        checks[i] = all(df_1.describe().T[i] == df_2.describe().T[i])\n",
    "    return checks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_1_transformed = process(train_df_1)\n",
    "train_df_1__transformed = process(train_df_1_)\n",
    "\n",
    "describe_check(train_df_1_transformed,train_df_1__transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_1_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_1__transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2_transformed = process(train_df_2)\n",
    "train_df_2__transformed = process(train_df_2_)\n",
    "\n",
    "describe_check(train_df_2_transformed,train_df_2__transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_3_transformed = process(train_df_3)\n",
    "train_df_3__transformed = process(train_df_3_)\n",
    "\n",
    "describe_check(train_df_3_transformed,train_df_3__transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_1_transformed = process(test_df_1)\n",
    "test_df_1__transformed = process(test_df_1_)\n",
    "\n",
    "describe_check(test_df_1_transformed,test_df_1__transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_2_transformed = process(test_df_2)\n",
    "test_df_2__transformed = process(test_df_2_)\n",
    "\n",
    "describe_check(test_df_2_transformed,test_df_2__transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_3_transformed = process(test_df_3)\n",
    "test_df_3__transformed = process(test_df_3_)\n",
    "\n",
    "describe_check(test_df_3_transformed,test_df_3__transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checker = train_df_1_transformed.describe().T['std'] != train_df_1__transformed.describe().T['std']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_1_transformed.describe().T['std'][checker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_1__transformed.describe().T['std'][checker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.097926e+02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.097926e+02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checker_mean = train_df_2_transformed.describe().T['mean'] != train_df_2__transformed.describe().T['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2_transformed.describe().T['mean'][checker_mean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2__transformed.describe().T['mean'][checker_mean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7.143427e+05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7.143427e+05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checker_std = train_df_2_transformed.describe().T['std'] != train_df_2__transformed.describe().T['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2_transformed.describe().T['std'][checker_std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2__transformed.describe().T['std'][checker_std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8.355997e+06\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8.355997e+06\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_checker_mean = test_df_2_transformed.describe().T['mean'] != test_df_2__transformed.describe().T['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_2_transformed.describe().T['mean'][test_checker_mean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_2__transformed.describe().T['mean'][test_checker_mean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3.656347e+06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3.656347e+06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_checker_std = test_df_2_transformed.describe().T['std'] != test_df_2__transformed.describe().T['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_2_transformed.describe().T['std'][test_checker_std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_2__transformed.describe().T['std'][test_checker_std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8.775294e+06\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8.775294e+06\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"logs\"\n",
    "pattern = os.path.join(directory_path, \"LOAD*\")\n",
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use glob to get a list of file paths matching the pattern\n",
    "files_to_remove = glob.glob(pattern)\n",
    "files_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the files and remove them\n",
    "for file_path in files_to_remove:\n",
    "    shutil.rmtree(file_path)\n",
    "    print(f\"Removed: {file_path}\")\n",
    "\n",
    "print(\"Files removed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'F:\\iNeuron\\Projects\\scania_failures_2\\artifacts\\data\\temp\\remote_aps_failure_testing_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GitPython import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
